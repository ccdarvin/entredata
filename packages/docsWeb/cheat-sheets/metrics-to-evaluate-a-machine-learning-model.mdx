---
title: Metricas para evaluar un modelo en machine learning
description: Cheat Sheet de Metricas para evaluar un modelo en machine learning
authors: ccdarvin
date: 2023-07-03
---

import CheatSheet from '@site/src/components/CheatSheet';


# Metricas para evaluar un modelo en machine learning

## Classification Metrics

<CheatSheet header="Confusion Matrix">
<table width="100%">
    <thead>
        <tr>
            <th></th>
            <th colspan="2">Predicted</th>
        </tr>
        <tr>
            <th>Actual class</th>
            <th>Positive</th>
            <th>Negative</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <th>Positive</th>
            <td>True Positive (TP)</td>
            <td>False Negative (FN)</td>
        </tr>
        <tr>
            <th>Negative</th>
            <td>False Positive (FP)</td>
            <td>True Negative (TN)</td>
        </tr>
    </tbody>
</table>

```python	
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred)
```
</CheatSheet>

<CheatSheet header="Accuaracy">

Usar Accuaracy cuando quieres medir la performance de un modelo de clasificacion. Es la proporcion de predicciones correctas sobre el total de predicciones realizadas.

* Accuracy = $\frac{TP + TN}{TP + TN + FP + FN}$

```python	
from sklearn.metrics import accuracy_score
accuracy_score(y_true, y_pred)
```

</CheatSheet>
<CheatSheet header="Precision">

Usar Precision cuanto quieres minimizar los falsos positivos (Errores de tipo I). Es la proporcion de predicciones correctas sobre el total de predicciones realizadas.
* Precision = $\frac{TP}{TP + FP}$

```python
from sklearn.metrics import precision_score
precision_score(y_true, y_pred)
```

</CheatSheet>

<CheatSheet header="Recall">

Usar Recall cuando quieres minimizar los falsos negativos (Errores de tipo II). Es la proporcion de predicciones correctas sobre el total de predicciones realizadas.

* Recall = $\frac{TP}{TP + FN}$

```python
from sklearn.metrics import recall_score
recall_score(y_true, y_pred)
```

</CheatSheet>

<CheatSheet header="F1 Score">

Usar F1 Score cuando quieres minimizar los falsos negativos y falsos positivos. Es la media armonica entre Precision y Recall.

* F1 Score = $\frac{2 * Precision * Recall}{Precision + Recall}$

```python
from sklearn.metrics import f1_score
f1_score(y_true, y_pred)
```

</CheatSheet>

<CheatSheet header="ROC Curve">

Usar ROC Curve cuando quieres evaluar el rendimiento de un modelo de clasificacion binaria. Es una grafica de la tasa de verdaderos positivos (TPR) frente a la tasa de falsos positivos (FPR) para diferentes umbrales de probabilidad de clasificacion.

* TPR = $\frac{TP}{TP + FN}$
* FPR = $\frac{FP}{FP + TN}$

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_true, y_pred)
auc = roc_auc_score(y_true, y_pred)

plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

</CheatSheet>

<CheatSheet header="Classification Report">

Usar Classification Report cuando quieres evaluar el rendimiento de un modelo de clasificacion. Es un resumen de las metricas de clasificacion para cada clase del problema.

```python
from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred))
```

</CheatSheet>

## Regression Metrics

<CheatSheet header="Mean Absolute Error (MAE)">

Usar MAE cuando quieres medir el error medio de un modelo de regresion. Es la media de la diferencia absoluta entre las predicciones y los valores reales.

* MAE = $\frac{1}{n} \sum_{i=1}^{n} |y_{i} - \hat{y}_{i}|$

```python
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_true, y_pred)
```

</CheatSheet>

<CheatSheet header="Mean Squared Error (MSE)">

Usar MSE cuando quieres penalizar los errores mas grandes. Es la mejor metrica cuando le preocupa las grandes desviaciones en los errores.

* MSE = $\frac{1}{n} \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2}$

```python
from sklearn.metrics import mean_squared_error
mean_squared_error(y_true, y_pred)
```

</CheatSheet>

<CheatSheet header="Root Mean Squared Error (RMSE)">

Usar RMSE cuando quieres penalizar los errores mas grandes. Es la mejor metrica cuando desea una medida que sea menos sensible a los valores atipicos.

* RMSE = $\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2}}$

```python
from sklearn.metrics import mean_squared_error
mean_squared_error(y_true, y_pred, squared=False)
# or
import numpy as np
np.sqrt(mean_squared_error(y_true, y_pred))
```

</CheatSheet>

<CheatSheet header="R-Squared (R2)">
Usar R2 cuando quieres medir la varianza de los errores. Es la proporcion de la varianza de los errores y la varianza de los valores reales

* R2 = $1 - \frac{\sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2}}{\sum_{i=1}^{n} (y_{i} - \bar{y}_{i})^{2}}$

```python
from sklearn.metrics import r2_score
r2_score(y_true, y_pred)
```

</CheatSheet>


