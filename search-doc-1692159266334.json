[{"title":"Implementación de Siamese Network","type":0,"sectionRef":"#","url":"/article/2023/08/11/implementacion-de-siamese-network","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"¿Que es una Siamese Network?​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#que-es-una-siamese-network","content":"Es un tipo especial de arquitecura de red neuronal que permite comparar dos imagenes y determinar si son similares o no, la caracteristica principal de este tipo de red es que comparten los mismos pesos y arquitectura, es decir, son dos redes neuronales que comparten los mismos pesos y arquitectura, esto permite que la red pueda aprender a comparar dos imagenes y determinar si son similares o no. "},{"title":"Importar modulos​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#importar-modulos","content":"import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import random  "},{"title":"Preparar los datos​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#preparar-los-datos","content":"(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() # cast to float32 X_train = X_train.astype('float32') X_test = X_test.astype('float32') print('fashion_mnist train shape:', X_train.shape) print('fashion_mnist test shape:', X_test.shape) # normalize data X_train /= 255. X_test /= 255.  fashion_mnist train shape: (60000, 28, 28) fashion_mnist test shape: (10000, 28, 28)  "},{"title":"Crear nuestros pares de datos positivos y negativos​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#crear-nuestros-pares-de-datos-positivos-y-negativos","content":"Pares positivos: Dos imágenes que contienen las mismas caracteristicasPares negativos: Dos imágenes que contienen diferentes caracteristicas def create_pairs(X: np.ndarray, y: np.ndarray): &quot;&quot;&quot; Args: X (np.ndarray): Array de imagenes y (np.ndarray): Array de etiquetas return: pairs (np.ndarray): Array de pares de imagenes labels: Etiquetas de las imagenes, 1 si son pares positivos y 0 si son pares negativos &quot;&quot;&quot; # image indices by class labels: y num_classes = max(y) + 1 indices = [np.where(y == i)[0] for i in range(num_classes)] pairs, labels = [], [] # max number of pairs using the min number of images by class n = min([len(i) for i in indices]) - 1 for c in range(num_classes): for i in range(n): # positive pair img1 = X[indices[c][i]] # next image of the same class img2 = X[indices[c][i+1]] pairs.append((img1, img2)) labels.append(1.) # negative pair # select a random class neg = np.random.choice([_c for _c in range(num_classes) if _c != c]) # select a random image img1 = X[indices[c][i]] img2 = X[indices[neg][i]] pairs.append((img1, img2)) labels.append(0.) return np.array(pairs), np.array(labels)  X_train_pair, y_train_bin = create_pairs(X_train, y_train) X_test_pair, y_test_bin = create_pairs(X_test, y_test) print(f'train pair: {X_train_pair.shape} and type {X_train_pair.dtype} label: {y_train_bin.shape} type {y_train_bin.dtype}') print(f'test pair: {X_test_pair.shape} and type {X_test_pair.dtype} labe: {y_test_bin.shape} type {y_test_bin.dtype}')  train pair: (119980, 2, 28, 28) and type float32 label: (119980,) type float64 test pair: (19980, 2, 28, 28) and type float32 labe: (19980,) type float64  Ya preparamos los pares de datos para entrenar la red neuronal ahora por cada ejemplo en nuestra input tenemos dos images y nuestro output es un valor 0 o 1 dependiendo si son negativos o positivos respectivamente. Para poder tener un contexto visual graficaremos las imagenes de ejemplo, una a lado de la otra. En el código usamos un random para seleccionar el par de imagenes a mostrar esto se hace para que se pueda jugar por los datos y ver distintos ejemplos. def plot_pair(pair: np.ndarray, label: float, pred: float = None): fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(pair[0]) ax[1].imshow(pair[1]) if label: title = &quot;Positivo&quot; else: title = &quot;Negativo&quot; if pred is not None: if pred &lt; 0.5: title += f&quot; - pred: {pred:.2f} - Positivo&quot; else: title += f&quot; - pred: {pred:.2f} - Negativo&quot; fig.suptitle(title) plt.show()  num_exam = random.randrange(len(X_train_pair)) plot_pair(X_train_pair[num_exam], y_train_bin[num_exam])   "},{"title":"Modelo base​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#modelo-base","content":"input_shape = (28, 28, ) def build_model_base(): input = tf.keras.Input(shape=input_shape, name=&quot;base_input&quot;) x = tf.keras.layers.Flatten(name=&quot;flatten_input&quot;)(input) x = tf.keras.layers.Dense(128, activation='relu', name=&quot;first_base_dense&quot;)(x) x = tf.keras.layers.Dropout(0.1, name=&quot;first_dropout&quot;)(x) x = tf.keras.layers.Dense(128, activation='relu', name=&quot;second_base_dense&quot;)(x) x = tf.keras.layers.Dropout(0.1, name=&quot;second_dropout&quot;)(x) x = tf.keras.layers.Dense(128, activation='relu', name=&quot;third_base_dense&quot;)(x) return tf.keras.Model(inputs=input, outputs=x, name=&quot;base_model&quot;)  model_base = build_model_base() tf.keras.utils.plot_model(model_base, show_shapes=True)   Vamos ahora a construir la Siamense Network, donde podremos ver que tenemos dos entradas que se enviaran a al modelo base y esta pasara por una capa personalizada que calculara la distancia entre los dos vectores de salida de la red base, distancia euclidiana "},{"title":"Distancia Euclidiana​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#distancia-euclidiana","content":"la distancia euclidiana es una medida de distancia entre dos puntos que se puede generalizar a cualquier dimensión. En otras palabras, es la distancia entre dos puntos que se puede medir con una regla. En dos dimensiones, la distancia euclidiana entre los puntos def euclidean_distance(vects): import keras.backend as K vect_a, vect_b = vects sum_square = K.sum(K.square(vect_a - vect_b), axis=1, keepdims=True) return K.sqrt(K.maximum(sum_square, K.epsilon())) def ouput_shape(shapes): shape1, shape2 = shapes return (shape1[0], 1)  # create input a input_a = tf.keras.Input(shape=input_shape, name='left_input') vect_output_a = model_base(input_a) # create input b input_b = tf.keras.Input(shape=input_shape, name='right_input') vect_output_b = model_base(input_b) # measure the similarity of the two vectorized outputs output = tf.keras.layers.Lambda(euclidean_distance, name='output_layer', output_shape=ouput_shape)([vect_output_a, vect_output_b]) # create the model model = tf.keras.Model([input_a, input_b], output) tf.keras.utils.plot_model(model, show_shapes=True)   "},{"title":"Configurar modelo​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#configurar-modelo","content":" def contrastive_loss_with_margin(margin): def contrastive_loss(y_true, y_pred): '''Contrastive loss from Hadsell-et-al.'06 http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf ''' import tensorflow.keras.backend as K square_pred = K.square(y_pred) margin_square = K.square(K.maximum(margin - y_pred, 0)) return (y_true * square_pred + (1 - y_true) * margin_square) return contrastive_loss  rms = tf.keras.optimizers.RMSprop(learning_rate=0.0001) model.compile(optimizer=rms, loss=contrastive_loss_with_margin(1), metrics=['accuracy'])  "},{"title":"Entrenamos el modelo​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#entrenamos-el-modelo","content":"history = model.fit([X_train_pair[:, 0], X_train_pair[:, 1]], y_train_bin, epochs=30, batch_size=180, validation_data=([X_test_pair[:, 0], X_test_pair[:, 1]], y_test_bin))  Epoch 1/30 667/667 [==============================] - 6s 7ms/step - loss: 0.1847 - accuracy: 0.2415 - val_loss: 0.1192 - val_accuracy: 0.1572 Epoch 2/30 667/667 [==============================] - 4s 6ms/step - loss: 0.1169 - accuracy: 0.1469 - val_loss: 0.1023 - val_accuracy: 0.1336 Epoch 3/30 667/667 [==============================] - 5s 7ms/step - loss: 0.1039 - accuracy: 0.1306 - val_loss: 0.0955 - val_accuracy: 0.1266 Epoch 4/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0968 - accuracy: 0.1215 - val_loss: 0.0910 - val_accuracy: 0.1217 Epoch 5/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0920 - accuracy: 0.1152 - val_loss: 0.0883 - val_accuracy: 0.1200 Epoch 6/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0880 - accuracy: 0.1096 - val_loss: 0.0851 - val_accuracy: 0.1137 Epoch 7/30 667/667 [==============================] - 4s 5ms/step - loss: 0.0846 - accuracy: 0.1046 - val_loss: 0.0825 - val_accuracy: 0.1064 Epoch 8/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0818 - accuracy: 0.1006 - val_loss: 0.0804 - val_accuracy: 0.1040 Epoch 9/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0794 - accuracy: 0.0970 - val_loss: 0.0786 - val_accuracy: 0.1025 Epoch 10/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0771 - accuracy: 0.0931 - val_loss: 0.0777 - val_accuracy: 0.0988 Epoch 11/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0750 - accuracy: 0.0906 - val_loss: 0.0757 - val_accuracy: 0.0969 Epoch 12/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0733 - accuracy: 0.0877 - val_loss: 0.0746 - val_accuracy: 0.0946 Epoch 13/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0716 - accuracy: 0.0852 - val_loss: 0.0739 - val_accuracy: 0.0930 Epoch 14/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0699 - accuracy: 0.0832 - val_loss: 0.0721 - val_accuracy: 0.0910 Epoch 15/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0684 - accuracy: 0.0813 - val_loss: 0.0720 - val_accuracy: 0.0911 Epoch 16/30 667/667 [==============================] - 4s 5ms/step - loss: 0.0671 - accuracy: 0.0798 - val_loss: 0.0705 - val_accuracy: 0.0858 Epoch 17/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0661 - accuracy: 0.0779 - val_loss: 0.0704 - val_accuracy: 0.0880 Epoch 18/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0648 - accuracy: 0.0764 - val_loss: 0.0699 - val_accuracy: 0.0890 Epoch 19/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0637 - accuracy: 0.0752 - val_loss: 0.0693 - val_accuracy: 0.0877 Epoch 20/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0626 - accuracy: 0.0729 - val_loss: 0.0683 - val_accuracy: 0.0863 Epoch 21/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0616 - accuracy: 0.0724 - val_loss: 0.0683 - val_accuracy: 0.0880 Epoch 22/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0608 - accuracy: 0.0718 - val_loss: 0.0671 - val_accuracy: 0.0845 Epoch 23/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0599 - accuracy: 0.0700 - val_loss: 0.0671 - val_accuracy: 0.0852 Epoch 24/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0591 - accuracy: 0.0698 - val_loss: 0.0670 - val_accuracy: 0.0849 Epoch 25/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0585 - accuracy: 0.0693 - val_loss: 0.0663 - val_accuracy: 0.0843 Epoch 26/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0578 - accuracy: 0.0683 - val_loss: 0.0660 - val_accuracy: 0.0848 Epoch 27/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0568 - accuracy: 0.0671 - val_loss: 0.0659 - val_accuracy: 0.0848 Epoch 28/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0563 - accuracy: 0.0671 - val_loss: 0.0658 - val_accuracy: 0.0852 Epoch 29/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0559 - accuracy: 0.0663 - val_loss: 0.0650 - val_accuracy: 0.0834 Epoch 30/30 667/667 [==============================] - 4s 6ms/step - loss: 0.0553 - accuracy: 0.0663 - val_loss: 0.0655 - val_accuracy: 0.0842  "},{"title":"Evaluamos el modelo​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#evaluamos-el-modelo","content":"pred = model.predict([X_test_pair[:, 0], X_test_pair[:, 1]])  625/625 [==============================] - 1s 2ms/step  examples = np.random.choice(range(len(pred)), size=5, replace=False) for i in examples: plot_pair(X_test_pair[i], y_test_bin[i], pred[i][0])       "},{"title":"graficar funcion de perdida​","type":1,"pageTitle":"Implementación de Siamese Network","url":"/article/2023/08/11/implementacion-de-siamese-network#graficar-funcion-de-perdida","content":"plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('Modelo de perdida') plt.ylabel('Perdida') plt.show()   "},{"title":"Crear una funcion de perdida en tensorflow","type":0,"sectionRef":"#","url":"/article/2023/08/15/crear-una-funcion-de-perdida-en-tensorflow","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"¿Como crear una función de perdida en tensorflow?​","type":1,"pageTitle":"Crear una funcion de perdida en tensorflow","url":"/article/2023/08/15/crear-una-funcion-de-perdida-en-tensorflow#como-crear-una-función-de-perdida-en-tensorflow","content":"En este post vamos a ver como crear una función de perdida en tensorflow. Para ello vamos a crear una función de perdida de tipo Huber. "},{"title":"Funcion de perdida Huber​","type":1,"pageTitle":"Crear una funcion de perdida en tensorflow","url":"/article/2023/08/15/crear-una-funcion-de-perdida-en-tensorflow#funcion-de-perdida-huber","content":"La funcion de perdida Huber es una funcion de perdida robusta que es menos sensible a los valores atipicos en los datos que la funcion de perdida de error cuadratico medio (MSE) y la funcion de perdida de error absoluto medio (MAE). La funcion de perdida Huber es una combinacion de ambas funciones de perdida. En esta publicacion, aprendera como funciona la funcion de perdida Huber y como implementarla en Python. "},{"title":"Equacion de la funcion de perdida Huber​","type":1,"pageTitle":"Crear una funcion de perdida en tensorflow","url":"/article/2023/08/15/crear-una-funcion-de-perdida-en-tensorflow#equacion-de-la-funcion-de-perdida-huber","content":"La funcion de perdida Huber se define como: l={12(y−y^)2para ∣y−y^∣≤δδ(∣y−y^∣−12δ)en otro casol = \\begin{cases} \\frac{1}{2} (y - \\hat{y})^2 &amp; \\text{para } |y - \\hat{y}| \\leq \\delta \\\\ \\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta) &amp; \\text{en otro caso} \\end{cases}l={21​(y−y^​)2δ(∣y−y^​∣−21​δ)​para ∣y−y^​∣≤δen otro caso​ Donde yyy es el valor verdadero, y^\\hat{y}y^​ es el valor predicho yδ\\deltaδ es un valor constante que define el umbral entre las dos funciones de perdida. La funcion de perdida Huber es convexa y diferenciable en todas partes, excepto en y=y^y = \\hat{y}y=y^​ "},{"title":"Importar librerías​","type":1,"pageTitle":"Crear una funcion de perdida en tensorflow","url":"/article/2023/08/15/crear-una-funcion-de-perdida-en-tensorflow#importar-librerías","content":"import tensorflow as tf import numpy as np import matplotlib.pyplot as plt  "},{"title":"Preparar datos​","type":1,"pageTitle":"Crear una funcion de perdida en tensorflow","url":"/article/2023/08/15/crear-una-funcion-de-perdida-en-tensorflow#preparar-datos","content":"Para nuestro ejemplo crearemos un conjunto de datos de prueba con una distribución normal con media 0 y desviación estándar 1. Para ello usaremos la función normal de la librería numpy. x = np.random.uniform(-1, 1, 100) y = 2 * x + np.random.normal(0, 0.1, 100) print(f'x: {x[:5]}') print(f'y: {y[:5]}')  x: [ 0.09556661 0.4654635 0.83118325 -0.85447542 -0.31583249] y: [ 0.2331942 0.786064 1.47523312 -1.85194843 -0.68147018]  "},{"title":"Crear la función de pérdida Huber​","type":1,"pageTitle":"Crear una funcion de perdida en tensorflow","url":"/article/2023/08/15/crear-una-funcion-de-perdida-en-tensorflow#crear-la-función-de-pérdida-huber","content":"Ahora vamos a crear la funciones de perdida huber, que resive como parámetros el valor de la predicción y el valor real, y el delta que es el valor que define el punto de inflexión de la función. debemos tener en cuenta que delta hiperparámetro que se debe ajustar para cada problema. def huber_loss(y_true, y_pred, delta = 1.0): error = tf.abs(y_true - y_pred) return tf.where(error &lt;= delta, 0.5 * error ** 2, delta * (error - 0.5 * delta))  "},{"title":"Crear un modelo en tensorflow​","type":1,"pageTitle":"Crear una funcion de perdida en tensorflow","url":"/article/2023/08/15/crear-una-funcion-de-perdida-en-tensorflow#crear-un-modelo-en-tensorflow","content":"Ahora crearemos un modelo en tf para poder probar nuestra función de perdida. model = tf.keras.models.Sequential([ tf.keras.layers.Dense(1, input_shape=(1,)) ]) model.compile(optimizer='sgd', loss=lambda y_true, y_pred: tf.keras.losses.Huber()(y_true, y_pred, 2)) history = model.fit(x, y, epochs=100, verbose=0) model.predict([10])  1/1 [==============================] - 0s 66ms/step  array([[19.569162]], dtype=float32)  plt.plot(history.history['loss']) plt.title('Model loss') plt.ylabel('Loss') plt.xlabel('Epochs') plt.legend(['Train', 'Val'], loc='upper right') plt.show()   Como hemos visto crear una funcion de perdida es muy simple y usarla en tensorflow es aun mas simple. En este caso hemos creado una funcion de perdida que es la funcion de perdida de Huber. En el caso que queramos usar la funcion de perdida Huber recomiendo usar la funcion de perdida de Huber de tensorflow que es mas eficiente que la que hemos creado "},{"title":"Arboles de decisión y métodos de ensamble","type":0,"sectionRef":"#","url":"/article/arboles-decision-ensamble","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"¿Que es un árbol de decisión?​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#que-es-un-árbol-de-decisión","content":"Un árbol de decisión es un modelo de predicción utilizado en el ámbito de la inteligencia artificial. Dada una base de datos se construye un árbol de decisión para poder llegar a la conclusión deseada. Es una herramienta de apoyo para la toma de decisiones. "},{"title":"¿Que es un método de ensamblaje?​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#que-es-un-método-de-ensamblaje","content":"Los métodos de ensamblaje son métodos que combinan varios algoritmos de aprendizaje automático para obtener un mejor rendimiento predictivo que un solo algoritmo de aprendizaje automático. Los métodos de ensamblaje funcionan mejor cuando los predictores individuales están correlacionados entre sí. "},{"title":"Muestras con reemplazo​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#muestras-con-reemplazo","content":"En estadística, el muestreo con reemplazo es un método de muestreo en el que, para cada extracción, el elemento elegido se devuelve a la población y se mezcla con el resto de elementos. El muestreo con reemplazo es un método de muestreo no exhaustivo. P(xi)=1NP(x_i) = \\frac{1}{N}P(xi​)=N1​ En arboles de decisión se utiliza el muestreo con reemplazo para generar los árboles de decisión que se utilizaran para el ensamblaje, es decir, se generan varios árboles de decisión con muestras de la base de datos original, y se combinan para generar un modelo más robusto. "},{"title":"Random Forest​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#random-forest","content":"Random Forest es un método de ensamblaje que combina varios árboles de decisión, cada uno de los cuales se genera con una muestra de la base de datos original, y se combinan para generar un modelo más robusto. esteme metodo usa el muestreo con reemplazo para generar los árboles de decisión. Tenemos un datos de entrenamiento de tamaño mmm para b = 1 hasta B: Utilizamos el muestreo con reemplazo para generar una muestra de tamaño mmm de la base de datos original. Entrenamos un árbol de decisión TbT_bTb​ con la muestra generada.Se obtiene el modelo final combinando los BBB árboles de decisión generados. Cuando usamos este algorithmo, muchas veces tenemos la misma división en el nodo raíz, por lo que podemos modificar un poco el algorithmo para que esto no suceda, y así obtener un mejor modelo. "},{"title":"Elección de características aleatorias​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#elección-de-características-aleatorias","content":"En cada nodo, se elige un subconjunto aleatorio de kkk características de todo el conjunto de características. si nnn es el número total de características, se recomienda k=nk = \\sqrt{n}k=n​ para la regresión y k=n3k = \\frac{n}{3}k=3n​ para la clasificación, debe de tener en cuenta que esto es recomendado para un gran número de características "},{"title":"XGBoost ( ExTreme Gradient Boosting)​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#xgboost--extreme-gradient-boosting","content":"XGBoost es un método de ensamblaje que combina varios árboles de decisión, cada uno de los cuales se genera con una muestra de la base de datos original, y se combinan para generar un modelo más robusto. este metodo usa el muestreo con reemplazo para generar los árboles de decisión. Pero a diferencia de Random Forest, XGBoost utiliza un algorithmo de optimización para generar los árboles de decisión, En vez de utilizar el muestreo con reemplazo para generar los árboles de decisión con una probabilidad uniforme 1/m1/m1/m, XGBoost utiliza un algorithmo de optimización para generar los árboles de decisión con una probabilidad pip_ipi​ que depende de la pérdida de la iteración anterior. pi=e−ΔLiλ∑i=1me−ΔLiλp_i = \\frac{e^{\\frac{-\\Delta L_i}{\\lambda}}}{\\sum_{i=1}^{m} e^{\\frac{-\\Delta L_i}{\\lambda}}}pi​=∑i=1m​eλ−ΔLi​​eλ−ΔLi​​​ Donde: ΔLi\\Delta L_iΔLi​ es la pérdida de la iteración iiiλ\\lambdaλ es un parámetro de regularización La idea de esto es que el algorithmo de optimización se enfoque en las muestras que tienen una pérdida mayor, y así generar un mejor modelo. "},{"title":"Ventajas de XGBoost​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#ventajas-de-xgboost","content":"Implementaciónes open source en varios lenguajes de programaciónRapidez en el entrenamientoBuena elección de divisiónes criticas por defecto y criterio para cuando parar de dividirRegularización para evitar el sobreajuste "},{"title":"Implementación en Python​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#implementación-en-python","content":"Para la implementación en Python, se utilizara la librería XGBoost, la cual se puede instalar con el comando: from xgboost import XGBClassifier model = XGBClassifier() # XGBRegressor para regresión model.fit(X_train, y_train) y_pred = model.predict(X_test)  "},{"title":"Cuando usar Arboles de decisión y métodos de ensamblaje​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#cuando-usar-arboles-de-decisión-y-métodos-de-ensamblaje","content":"Trabajan bien con datos tabulares (estructurados)No se recomienda para datos no estructurados (imágenes, texto, audio, etc)Es muy rápido en entrenamiento y predicciónPequeños arboles de decisión son fáciles de interpretar (visualizar) "},{"title":"Cuando usar neural networks​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/article/arboles-decision-ensamble#cuando-usar-neural-networks","content":"Trabaja bien con todo tipo de datos tabulares &quot;estructurados&quot; y &quot;no estructurados&quot;Puede ser lento en entrenamiento y predicciónTrabaja con transfer learningCuando trabajamos con multiples modelos juntos, puede ser mas sencillo encadenarlos con una red neuronal "},{"title":"Arbol de Desición, conceptos básicos","type":0,"sectionRef":"#","url":"/article/arbol-decision-conceptos-basicos","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Modelo de arbol de desición​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#modelo-de-arbol-de-desición","content":"Un árbol de decisión es un modelo de predicción utilizado en el ámbito de la inteligencia artificial, que utiliza un árbol de estructura similar a los diagramas de flujo en donde cada nodo representa una característica (o atributo), cada rama representa una regla de decisión y cada hoja representa el resultado de una decisión. Los árboles de decisión son utilizados comúnmente en minería de datos con el fin de resolver problemas de clasificación. Ejemplo de un arbol de desición y su estructura:  "},{"title":"Entropía​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#entropía","content":""},{"title":"¿Qué es la entropía?​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#qué-es-la-entropía","content":"La entropía es una medida de incertidumbre. En el contexto de la toma de decisiones, la entropía mide la impureza de un conjunto de ejemplos S. Si S solo contiene ejemplos de una clase, entonces la entropía es 0. Si S contiene una cantidad uniforme de ejemplos de cada clase, entonces la entropía es 1. La entropía de un conjunto S se denota por H (S). "},{"title":"¿Cómo se calcula la entropía?​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#cómo-se-calcula-la-entropía","content":"La entropía de un conjunto S se calcula como: H(S)=−∑i=1cpilog2piH(S) = -\\sum_{i=1}^{c} p_i log_2 p_iH(S)=−∑i=1c​pi​log2​pi​ Donde: ccc es el número de clasespip_ipi​ es la proporción de ejemplos de clase iii en SSSlog2log_2log2​ es el logaritmo en base 2 "},{"title":"Ejemplo de cálculo de entropía​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#ejemplo-de-cálculo-de-entropía","content":"Supongamos que tenemos un conjunto de ejemplos SSS con 14 ejemplos de clase 1 y 6 ejemplos de clase 2. La entropía de SSS es: P1=14/20P_1 = 14/20P1​=14/20 yP2=6/20P_2 = 6/20P2​=6/20 Entonces, la entropía de SSS sería: H(S)=−(1420log⁡21420+620log⁡2620)≈0.88H(S) = - \\left(\\frac{14}{20} \\log_2 \\frac{14}{20} + \\frac{6}{20} \\log_2 \\frac{6}{20}\\right) \\approx 0.88H(S)=−(2014​log2​2014​+206​log2​206​)≈0.88. "},{"title":"Ganancia de información​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#ganancia-de-información","content":"La ganancia de información(IG) se utiliza para decidir qué atributo se utilizará para dividir el conjunto de datos en subconjuntos homogéneos. La ganancia de información se define como la diferencia entre la entropía antes de la división y la entropía después de la división por un atributo. La ganancia de información se denota por IG (S, A) y se calcula como: IG(S,A)=H(S)−∑v∈Values(A)∣Sv∣∣S∣H(Sv)IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)IG(S,A)=H(S)−∑v∈Values(A)​∣S∣∣Sv​∣​H(Sv​) Donde: SSS es el conjunto de ejemplosAAA es el atributo utilizado para dividir SSS en subconjuntosValues(A)Values(A)Values(A) es el conjunto de valores que puede tomar el atributo AAASvS_vSv​ es el subconjunto de SSS en el que el atributo AAA tiene el valor vvv "},{"title":"Indice Gini​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#indice-gini","content":"El índice de Gini es una medida de impureza utilizada en los árboles de decisión para decidir qué atributo dividir un nodo en dos o más subnodos. El índice de Gini se define como: Gini(S)=1−∑i=1cpi2Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2Gini(S)=1−∑i=1c​pi2​ Donde: ccc es el número de clasespip_ipi​ es la proporción de ejemplos de clase iii en SSS "},{"title":"Pros y contras de los árboles de decisión​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#pros-y-contras-de-los-árboles-de-decisión","content":""},{"title":"Pros​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#pros","content":"Fácil de entender e interpretar. Los árboles se pueden visualizar.Puede ser muy util para solucionar problemas relacionados con decisiones.Hay menos requisitos de limpieza de datos "},{"title":"Contras​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/article/arbol-decision-conceptos-basicos#contras","content":"Los árboles de decisión pueden ser poco precisos. Pueden ser muy sensibles a pequeños cambios en los datos. "},{"title":"Comando mágico timeit - jupyter","type":0,"sectionRef":"#","url":"/article/comando-magico-timeit-jupyter","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Uso básico de %timeit​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/article/comando-magico-timeit-jupyter#uso-básico-de-timeit","content":"Para utilizar %timeit, simplemente coloca el comando mágico antes de la expresión o función que deseas medir. Por ejemplo, para medir el tiempo de ejecución de la expresión '1 + 1', puedes usar el siguiente código en una celda de Jupyter Lab: %timeit 1 + 1  10.1 ns ± 0.491 ns per loop (mean ± std. dev. of 7 runs, 100,000,000 loops each)  Después de ejecutar la celda, %timeit ejecutará la expresión '1 + 1'varias veces y mostrará el tiempo promedio de ejecución. En este caso, el tiempo promedio de ejecución en unidades de tiempo "},{"title":"2. Tabla de tiempos​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/article/comando-magico-timeit-jupyter#tabla-de-tiempos","content":"Abreviatura\tUnidad de tiempons\tnanosegundos us\tmicrosegundos ms\tmilisegundos s\tsegundos m\tminutos h\thoras "},{"title":"3. Especificar el número de ejecuciones y repeticiones​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/article/comando-magico-timeit-jupyter#especificar-el-número-de-ejecuciones-y-repeticiones","content":"Por defecto, %timeit ejecuta la expresión o función 100.000 veces y repite la operación tres veces. Puedes especificar el número de ejecuciones y repeticiones utilizando la sintaxis%timeit -r &lt;repeticiones&gt; -n &lt;ejecuciones&gt;. Por ejemplo, para ejecutar la expresión '1 + 1' 10.000 veces y repetir la operación cinco veces, puedes usar el siguiente código: %timeit -r5 -n50 1 + 1  25.6 ns ± 5.28 ns per loop (mean ± std. dev. of 5 runs, 50 loops each)  En el comando anterior espesificamps que se ejecute 50 veces en 5 repeticiones "},{"title":"5. Medir el tiempo de ejecución de una función​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/article/comando-magico-timeit-jupyter#medir-el-tiempo-de-ejecución-de-una-función","content":"También puedes utilizar %timeit para medir el tiempo de ejecución de una función. Por ejemplo, para medir el tiempo de ejecución de la función sum() de Python, puedes usar el siguiente código: def mi_funcion(): # puedes colocar cualquier código aquí return 1 + 1  Jupyter Lab ejecutara el código y te devolvera el tiempo de ejecución de la función "},{"title":"6. Medir el tiempo de ejecución de una celda​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/article/comando-magico-timeit-jupyter#medir-el-tiempo-de-ejecución-de-una-celda","content":"También puedes utilizar %timeit para medir el tiempo de ejecución de una celda completa. Por ejemplo, para medir el tiempo de ejecución de la siguiente celda, puedes usar el siguiente código: %%timeit x = 1 x += 1  36.7 ns ± 1.13 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)  "},{"title":"7. Obtener el tiempo de ejecución como variable​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/article/comando-magico-timeit-jupyter#obtener-el-tiempo-de-ejecución-como-variable","content":"En caso de que desees obtener información más detallada sobre el tiempo de ejecución, podrias asignar el resultado de %timeit a una variable, para esto utilizaremos las opciones -o para almacenar el resultado y -q para silenciar la salida de la celda. Por ejemplo, para obtener el tiempo de ejecución de la expresión '1 + 1' como una variable, puedes usar el siguiente código: resultado = %timeit -o -q 1 + 1 print(f'El mejor tiempo fue {resultado.best}') print(f'El peor tiempo fue {resultado.worst}')  El mejor tiempo fue 9.775258000008763e-09 El peor tiempo fue 1.1235137999756262e-08  Hemos visto de forma muy rapida como usar el comando magico %timeit en Jupyter Lab, con expresiones muy sencillas, pero en la practica se utiliza para medir el tiempo de ejecución de funciones y celdas completas, lo cual es muy util para comparar diferentes enfoques de implementación. "},{"title":"Brocasting con Numpy","type":0,"sectionRef":"#","url":"/article/brocasting-con-numpy","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Ejemplos de broadcasting​","type":1,"pageTitle":"Brocasting con Numpy","url":"/article/brocasting-con-numpy#ejemplos-de-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t1 x 5\t10 x 5 10 x 5\t5\t10 x 5 10 x 5\t10 x 1\t10 x 5 10 x 5\tscalar\t10 x 5 import numpy as np from rich import print # matris de 10x5 m10_5 = np.random.randint(0, 10, (10, 5)) m1_5 = np.random.randint(0, 10, (1, 5)) m5 = np.random.randint(0, 10, (5)) m10_1 = np.random.randint(0, 10, (10, 1)) scalar = 5 print('Broadcasting de 10x5 + 1x5') print(m10_5 + m1_5) print('Broadcasting de 10x5 + 5') print(m10_5 + m5) print('Broadcasting de 10x5 + 10x1') print(m10_5 + m10_1) print(m10_5 + scalar)  Broadcasting de 10x5 + 1x5  [[ 9 2 6 5 1] [ 5 9 10 10 8] [ 6 9 3 10 3] [ 8 8 7 4 8] [ 5 2 6 10 5] [ 7 9 9 8 3] [ 7 5 3 5 7] [ 4 7 7 3 1] [ 2 8 6 12 7] [ 9 9 12 10 10]]  Broadcasting de 10x5 + 5  [[ 7 11 9 2 5] [ 3 18 13 7 12] [ 4 18 6 7 7] [ 6 17 10 1 12] [ 3 11 9 7 9] [ 5 18 12 5 7] [ 5 14 6 2 11] [ 2 16 10 0 5] [ 0 17 9 9 11] [ 7 18 15 7 14]]  Broadcasting de 10x5 + 10x1  [[14 9 10 9 7] [ 5 11 9 9 9] [ 5 10 1 8 3] [11 13 9 6 12] [ 9 8 9 13 10] [11 15 12 11 8] [ 6 6 1 3 7] [ 3 8 5 1 1] [ 8 16 11 17 14] [ 7 9 9 7 9]]  [[12 7 8 7 5] [ 8 14 12 12 12] [ 9 14 5 12 7] [11 13 9 6 12] [ 8 7 8 12 9] [10 14 11 10 7] [10 10 5 7 11] [ 7 12 9 5 5] [ 5 13 8 14 11] [12 14 14 12 14]]  "},{"title":"Ejemplos de no broadcasting​","type":1,"pageTitle":"Brocasting con Numpy","url":"/article/brocasting-con-numpy#ejemplos-de-no-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t5 x 10\tError 10 x 5\t10\tError m10_5 = np.random.randint(0, 10, (10, 5)) m5_10 = np.random.randint(0, 10, (5, 10)) m10 = np.random.randint(0, 10, (10)) print('Broadcasting de 10x5 + 5x10') print(m10_5 + m5_10)  Broadcasting de 10x5 + 5x10  ValueError: operands could not be broadcast together with shapes (10,5) (5,10)  print('Broadcasting de 10x5 + 10') print(m10_5 + m10)  Broadcasting de 10x5 + 10  ValueError: operands could not be broadcast together with shapes (10,5) (10,)  "},{"title":"Expresiones regulares en Python","type":0,"sectionRef":"#","url":"/article/expresiones-regulares-python","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Caracteres especiales​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/article/expresiones-regulares-python#caracteres-especiales","content":"Los caracteres especiales son aquellos que tienen un significado especial para las expresiones regulares. Por ejemplo, el punto y coma (;) es un caracter especial que se utiliza para separar instrucciones en Python. Sin embargo, en las expresiones regulares, el punto y coma (;) es un caracter especial que se utiliza para indicar que el patrón de búsqueda debe coincidir con cualquier caracter. A continuación se muestra una lista de los caracteres especiales más utilizados en las expresiones regulares: Caracter\tDescripción.\tCoincide con cualquier caracter ^\tCoincide con el inicio de una cadena $\tCoincide con el final de una cadena *\tCoincide con 0 o más ocurrencias del caracter anterior +\tCoincide con 1 o más ocurrencias del caracter anterior ?\tCoincide con 0 o 1 ocurrencia del caracter anterior {n}\tCoincide con n ocurrencias del caracter anterior {n,}\tCoincide con n o más ocurrencias del caracter anterior {n,m}\tCoincide con un rango de ocurrencias del caracter anterior […]\tCoincide con cualquier caracter dentro de los corchetes [^...]\tCoincide con cualquier caracter que no esté dentro de los corchetes (…)\tAgrupa una serie de patrones Coincide con un espacio en blanco Coincide con cualquier caracter que no sea un espacio en blanco Coincide con cualquier caracter alfanumérico Coincide con cualquier caracter que no sea alfanumérico Coincide con cualquier caracter numérico Coincide con cualquier caracter que no sea numérico "},{"title":"Trabajando en python​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/article/expresiones-regulares-python#trabajando-en-python","content":"para trabajar con expresiones regulares en python, se debe importar el módulo re. A continuación se muestra un ejemplo de como utilizar el módulo re para buscar un patrón en una cadena de caracteres: import re  "},{"title":"Encontrar todas las coincidencias​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/article/expresiones-regulares-python#encontrar-todas-las-coincidencias","content":"text = &quot;Hola, mi nombre es Juan y mi número de teléfono es 123456789&quot; pattern = r&quot;mi&quot; print(re.findall(pattern, text)) pattern = r&quot;\\d+&quot; print(re.findall(pattern, text))  ['mi', 'mi'] ['123456789']  "},{"title":"Sustituir un patrón en una cadena de caracteres​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/article/expresiones-regulares-python#sustituir-un-patrón-en-una-cadena-de-caracteres","content":"text = &quot;Hol, mi nombre es Juan y mi nUmero de teléfono es 123456789&quot; text = re.sub(r&quot;Hol&quot;, &quot;Hola&quot;, text) print(text) text = re.sub(r&quot;U&quot;, &quot;ú&quot;, text) print(text)  Hola, mi nombre es Juan y mi nUmero de teléfono es 123456789 Hola, mi nombre es Juan y mi número de teléfono es 123456789  "},{"title":"Dividir una cadena de caracteres​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/article/expresiones-regulares-python#dividir-una-cadena-de-caracteres","content":"text = &quot;Hola, mi nombre es Juan y mi número de teléfono es 123456789&quot; text_split = re.split(r&quot;y&quot;, text) text_split  ['Hola, mi nombre es Juan ', ' mi número de teléfono es 123456789']  Python tambien tiene integrado funciones de expresiones regulares en el módulo string. text = &quot;Hola, mi nombre es Juan y mi número de teléfono es 123456789&quot; print(text.replace(&quot;Juan&quot;, &quot;Darvin&quot;)) print(text.split(','))  Hola, mi nombre es Darvin y mi número de teléfono es 123456789 ['Hola', ' mi nombre es Juan y mi número de teléfono es 123456789']  Estos son solo alguno de todos los metodos que tiene python para trabajar con expresiones regulares. Para más información, puede consultar la documentación oficial de python en el siguiente enlace:https://docs.python.org/3/library/re.html "},{"title":"Comparar series de tiempo","type":0,"sectionRef":"#","url":"/article/Compare-time-series-growth-rates","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Importar librerías​","type":1,"pageTitle":"Comparar series de tiempo","url":"/article/Compare-time-series-growth-rates#importar-librerías","content":"Para este ejercicio, se necesitara de las siguientes librerías: pip install pandas pip install matplotlib pip install yfinance  Usaremos la librería yfinance para obtener los datos de las acciones de las empresas tecnológicas, hay otras librearías que también pueden ayudar con esta tarea como pandas_datareader o quandl. import pandas as pd import matplotlib.pyplot as plt import yfinance as yf  "},{"title":"2. Obtener datos​","type":1,"pageTitle":"Comparar series de tiempo","url":"/article/Compare-time-series-growth-rates#obtener-datos","content":"Para este ejemplo, se obtendrán los datos de las acciones de las empresas tecnológicas desde el 2015 de google, amazon, facebook, apple y microsoft. tickets = ['GOOG', 'AMZN', 'META', 'AAPL', 'MSFT'] start_date = '2015-01-01' end_date = '2023-01-01' df = yf.download(tickets, start=start_date, end=end_date)['Adj Close'] df.head()  [*********************100%***********************] 5 of 5 completed  \tAAPL\tAMZN\tGOOG\tMETA\tMSFTDate 2015-01-02\t24.531763\t15.4260\t26.168653\t78.449997\t40.620667 2015-01-05\t23.840666\t15.1095\t25.623152\t77.190002\t40.247116 2015-01-06\t23.842913\t14.7645\t25.029282\t76.150002\t39.656406 2015-01-07\t24.177238\t14.9210\t24.986401\t76.150002\t40.160259 2015-01-08\t25.106184\t15.0230\t25.065184\t78.180000\t41.341694 "},{"title":"3. Normalizar datos​","type":1,"pageTitle":"Comparar series de tiempo","url":"/article/Compare-time-series-growth-rates#normalizar-datos","content":"Para poder comparar los datos vamos a normalizarlos, para esto se usará la siguiente fórmula: $$ \\frac{P_t}{P_0} * 100 $$ Donde PtP_tPt​ es el precio en el tiempo ttt yP0P_0P0​ es el precio inicial. normalized_df = df / df.iloc[0] * 100 normalized_df.head()  \tAAPL\tAMZN\tGOOG\tMETA\tMSFTDate 2015-01-02\t100.000000\t100.000000\t100.000000\t100.000000\t100.000000 2015-01-05\t97.182847\t97.948271\t97.915438\t98.393888\t99.080393 2015-01-06\t97.192006\t95.711785\t95.646043\t97.068202\t97.626183 2015-01-07\t98.554834\t96.726305\t95.482179\t97.068202\t98.866569 2015-01-08\t102.341540\t97.387528\t95.783238\t99.655836\t101.775026 "},{"title":"4. Graficar datos​","type":1,"pageTitle":"Comparar series de tiempo","url":"/article/Compare-time-series-growth-rates#graficar-datos","content":"Por ultimo grafiaremos los datos para poder compararlos y ver como se han comportado en el tiempo. normalized_df.plot(figsize=(15, 10)) plt.show()   "},{"title":"5. Conclusiones​","type":1,"pageTitle":"Comparar series de tiempo","url":"/article/Compare-time-series-growth-rates#conclusiones","content":"Como hemos podido ver hacer una comparación de series de tiempo es muy sencillo con pandas, y nos permite ver como se han comportado las acciones de las empresas tecnológicas en los últimos años, las concluciones respecto al comportamiento de las acciones de las empresas tecnológicas se las dejo a ustedes. "},{"title":"¿Como ver los días de la semana usando pandas?","type":0,"sectionRef":"#","url":"/article/how-to-see-the-days-of-the-week-with-pandas","content":"¿Como ver los días de la semana usando pandas?` Con pandas podemos ver de forma muy sencilla los días de la semana de una fecha en específico, para esto usaremos la función weekday_name y con dayofweek podemos ver el número del día de la semana. import pandas day = pandas.to_datetime('2023-07-10') print(day.dayofweek, day.day_name()) 0 Monday # todos los días de la semana week = pandas.date_range(start='2023-07-10', periods=7, freq='D') for day in week: print(day.dayofweek, day.day_name()) 0 Monday 1 Tuesday 2 Wednesday 3 Thursday 4 Friday 5 Saturday 6 Sunday # df con los dias de la semana df = pandas.DataFrame(week, columns=['date']) df['dayofweek'] = df['date'].dt.dayofweek df['dayname'] = df['date'].dt.day_name() df.set_index('date', inplace=True) df dayofweek\tdaynamedate 2023-07-10\t0\tMonday 2023-07-11\t1\tTuesday 2023-07-12\t2\tWednesday 2023-07-13\t3\tThursday 2023-07-14\t4\tFriday 2023-07-15\t5\tSaturday 2023-07-16\t6\tSunday","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Articulos","type":0,"sectionRef":"#","url":"/article/index","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Clustering","type":0,"sectionRef":"#","url":"/article/introduccion-a-los-algoritmos-de-clustering","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"¿Que es clustering?​","type":1,"pageTitle":"Clustering","url":"/article/introduccion-a-los-algoritmos-de-clustering#que-es-clustering","content":"Clustering es un método de aprendizaje no supervisado, que consiste en agrupar un conjunto de objetos de tal manera que los objetos del mismo grupo (o cluster) sean más similares (en algún sentido o en algún aspecto) entre sí que los de otros grupos. "},{"title":"k-means​","type":1,"pageTitle":"Clustering","url":"/article/introduccion-a-los-algoritmos-de-clustering#k-means","content":"k-means es un algoritmo de clustering que consiste en agrupar un conjunto de objetos de tal manera que los objetos del mismo grupo (o cluster) sean más similares (en algún sentido o en algún aspecto) entre sí que los de otros grupos. "},{"title":"Algoritmo de k-means​","type":1,"pageTitle":"Clustering","url":"/article/introduccion-a-los-algoritmos-de-clustering#algoritmo--de-k-means","content":"Inicializar los centroides de los clusters aleatoriamente (k puntos): μ1,μ2,...,μk\\mu_1, \\mu_2, ..., \\mu_kμ1​,μ2​,...,μk​ repeat{#Asignar puntos a los centroides del clusterfor i=1 to mc(i):=ıˊndice (de 1 a k) del centroide maˊs cercano a x(i)# Mover los centroides de los clustersfor k=1 to kuk:=promedio de los puntos asignados al cluster k}\\begin{align} repeat \\{ \\\\ &amp; \\# Asignar \\ puntos \\ a \\ los \\ centroides \\ del \\ cluster \\\\ &amp; \\text{for } i = 1 \\text{ to } m \\\\ &amp; \\quad c^{(i)} := \\text{índice (de 1 a k) del centroide más cercano a } x^{(i)} \\\\ &amp; \\# \\ Mover \\ los \\ centroides \\ de \\ los \\ clusters \\\\ &amp; \\text{for } k = 1 \\text{ to } k \\\\ &amp; \\quad u_k := \\text{promedio de los puntos asignados al cluster } k \\\\ \\} \\\\ \\end{align}repeat{}​#Asignar puntos a los centroides del clusterfor i=1 to mc(i):=ıˊndice (de 1 a k) del centroide maˊs cercano a x(i)# Mover los centroides de los clustersfor k=1 to kuk​:=promedio de los puntos asignados al cluster k​​  "},{"title":"k-means optimización objetivo​","type":1,"pageTitle":"Clustering","url":"/article/introduccion-a-los-algoritmos-de-clustering#k-means-optimización-objetivo","content":"c(i)c^{(i)}c(i) = índice del cluster (1, 2, ..., k) al que se asigna el ejemplo x(i)x^{(i)}x(i)uku_kuk​ = vector de parámetros del centroide del cluster kkk μc(i)\\mu_{c^{(i)}}μc(i)​ = vector de parámetros del centroide del cluster al que se asigna el ejemplo x(i)x^{(i)}x(i) Función de costo J(c(1),...,c(m),μ1,...,μk)=1m∑i=1m∣∣x(i)−μc(i)∣∣2J(c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_k) = \\frac{1}{m} \\sum_{i=1}^{m} ||x^{(i)} - \\mu_{c^{(i)}}||^2J(c(1),...,c(m),μ1​,...,μk​)=m1​∑i=1m​∣∣x(i)−μc(i)​∣∣2 Objetivo: Encontrar c(1),...,c(m),μ1,...,μkc^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_kc(1),...,c(m),μ1​,...,μk​ que minimicen JJJ. minc(1),...,c(m),μ1,...,μkJ(c(1),...,c(m),μ1,...,μk)min_{c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_k} J(c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_k)minc(1),...,c(m),μ1​,...,μk​​J(c(1),...,c(m),μ1​,...,μk​) "},{"title":"Inicializando k-means​","type":1,"pageTitle":"Clustering","url":"/article/introduccion-a-los-algoritmos-de-clustering#inicializando-k-means","content":"Seleccionar aleatoriamente kkk ejemplos de entrenamiento x(1),...,x(k)x^{(1)}, ..., x^{(k)}x(1),...,x(k) que servirán como los centroides iniciales: μ1,...,μk\\mu_1, ..., \\mu_kμ1​,...,μk​. "},{"title":"Elección del número de clusters​","type":1,"pageTitle":"Clustering","url":"/article/introduccion-a-los-algoritmos-de-clustering#elección-del-número-de-clusters","content":"¿Cual es el número de clusters óptimo? Para elegir el número de clusters óptimo se puede utilizar los siguientes 2 métodos: Método del codo: el metodo del codo consiste en graficar el valor de la función de costo JJJ en función del número de clusters kkk. El número de clusters óptimo será el valor de kkk en el que la función de costo JJJ se &quot;quiebre&quot; o tenga un cambio de pendiente más pronunciado.  No es una buena métrica para elegir el número de clusters óptimo, ya que no siempre se puede identificar un cambio de pendiente claro en la gráfica, no hay un codo claro. la elección del número de clusters es subjetiva, depende de la aplicación y del contexto. "},{"title":"Modelo con mutiples salidas","type":0,"sectionRef":"#","url":"/article/modelo-con-mutiples-salidas","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Importar modulos​","type":1,"pageTitle":"Modelo con mutiples salidas","url":"/article/modelo-con-mutiples-salidas#importar-modulos","content":"::: {.cell _cell_guid=‘bc1c482a-3443-4deb-bfbe-35775986216c’_uuid=‘3869d47c-4312-4dae-8587-4c76fe28cb21’ execution=‘{“iopub.execute_input”:“2023-08-11T20:34:57.287840Z”,“iopub.status.busy”:“2023-08-11T20:34:57.286929Z”,“iopub.status.idle”:“2023-08-11T20:34:57.295622Z”,“shell.execute_reply”:“2023-08-11T20:34:57.293802Z”,“shell.execute_reply.started”:“2023-08-11T20:34:57.287795Z”}’ jupyter=‘{“outputs_hidden”:false}’ trusted=‘true’ execution_count=1} import tensorflow as tf import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn import preprocessing import requests from io import BytesIO import zipfile  ::: "},{"title":"Preparar los datos​","type":1,"pageTitle":"Modelo con mutiples salidas","url":"/article/modelo-con-mutiples-salidas#preparar-los-datos","content":"Descargamos el dataset y preparamos nuestros datos de entrenamiento y pruebas # download file url = 'https://archive.ics.uci.edu/static/public/242/energy+efficiency.zip' r = requests.get(url) zip_data = BytesIO(r.content) # unzip with zipfile.ZipFile(zip_data, 'r') as zip_file: with zip_file.open('ENB2012_data.xlsx') as excel_file: df = pd.read_excel(excel_file) # random sort df = df.sample(frac=1) df.head()  \tX1\tX2\tX3\tX4\tX5\tX6\tX7\tX8\tY1\tY2470\t0.66\t759.5\t318.5\t220.5\t3.5\t4\t0.25\t4\t12.86\t16.17 379\t0.64\t784.0\t343.0\t220.5\t3.5\t5\t0.25\t2\t17.11\t20.43 31\t0.71\t710.5\t269.5\t220.5\t3.5\t5\t0.00\t0\t6.40\t11.67 747\t0.74\t686.0\t245.0\t220.5\t3.5\t5\t0.40\t5\t14.39\t16.70 737\t0.79\t637.0\t343.0\t147.0\t7.0\t3\t0.40\t5\t41.96\t37.70 ::: {.cell _cell_guid=‘966384d2-52f6-43fe-a9a7-30a34005525c’_uuid=‘246e7b03-c479-425e-b12e-0d31b6caa74c’ execution=‘{“iopub.execute_input”:“2023-08-11T20:35:01.995196Z”,“iopub.status.busy”:“2023-08-11T20:35:01.994656Z”,“iopub.status.idle”:“2023-08-11T20:35:02.010268Z”,“shell.execute_reply”:“2023-08-11T20:35:02.008886Z”,“shell.execute_reply.started”:“2023-08-11T20:35:01.995155Z”}’ jupyter=‘{“outputs_hidden”:false}’ trusted=‘true’ execution_count=3} # split data train, test = train_test_split(df, test_size=0.2) def format_data(df: pd.DataFrame): y1 = df['Y1'].values y2 = df['Y2'].values X = df.drop(['Y1', 'Y2'], axis=1) return X, (y1, y2) X_train, Y_train = format_data(train) X_test, Y_test = format_data(test)  ::: ::: {.cell _cell_guid=‘f867a2c3-849d-476b-8068-deea62ff3ea8’_uuid=‘3717aebb-1951-47bc-aeb9-ef1ee7af3ced’ execution=‘{“iopub.execute_input”:“2023-08-11T20:35:06.917556Z”,“iopub.status.busy”:“2023-08-11T20:35:06.917099Z”,“iopub.status.idle”:“2023-08-11T20:35:06.925773Z”,“shell.execute_reply”:“2023-08-11T20:35:06.924386Z”,“shell.execute_reply.started”:“2023-08-11T20:35:06.917524Z”}’ jupyter=‘{“outputs_hidden”:false}’ trusted=‘true’ execution_count=4} # normalize data scaler = preprocessing.StandardScaler() X_train_norm = scaler.fit_transform(X_train) X_test_norm = scaler.transform(X_test)  ::: "},{"title":"El modelo​","type":1,"pageTitle":"Modelo con mutiples salidas","url":"/article/modelo-con-mutiples-salidas#el-modelo","content":"Ahora construiremos nuestro modelo teniendo en cuenta que va a tener dos salidas def build_model(): input_layer = tf.keras.layers.Input(shape=(8,)) hidden_layer = tf.keras.layers.Dense(128, activation='relu')(input_layer) hidden_layer = tf.keras.layers.Dense(128, activation='relu')(hidden_layer) # ouput 1 y1 = tf.keras.layers.Dense(1, name='y1')(hidden_layer) # output 2 hidden_layer = tf.keras.layers.Dense(64, activation='relu')(hidden_layer) y2 = tf.keras.layers.Dense(1, name='y2')(hidden_layer) return tf.keras.models.Model(inputs=input_layer, outputs=[y1, y2])  "},{"title":"Graficar nuestro modelo​","type":1,"pageTitle":"Modelo con mutiples salidas","url":"/article/modelo-con-mutiples-salidas#graficar-nuestro-modelo","content":"Vamos a graficar nuestro modelo donde podemos observar de forma mas simple nuestras dos salidas y1 y y2, tambien podemos ver que y2tiene una capa extra, esta es la detf.keras.layers.Dense(units=1, name='y2', activation='linear')(X) model = build_model() model.summary()  Model: &quot;model&quot; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 8)] 0 [] dense (Dense) (None, 128) 1152 ['input_1[0][0]'] dense_1 (Dense) (None, 128) 16512 ['dense[0][0]'] dense_2 (Dense) (None, 64) 8256 ['dense_1[0][0]'] y1 (Dense) (None, 1) 129 ['dense_1[0][0]'] y2 (Dense) (None, 1) 65 ['dense_2[0][0]'] ================================================================================================== Total params: 26,114 Trainable params: 26,114 Non-trainable params: 0 __________________________________________________________________________________________________  "},{"title":"Configurar parametros​","type":1,"pageTitle":"Modelo con mutiples salidas","url":"/article/modelo-con-mutiples-salidas#configurar-parametros","content":"model.compile( optimizer=tf.optimizers.SGD(learning_rate=0.001), loss={ 'y1': tf.keras.losses.MeanSquaredError(), 'y2': tf.keras.losses.MeanSquaredError(), }, metrics={ 'y1': tf.keras.metrics.RootMeanSquaredError(), 'y2': tf.keras.metrics.RootMeanSquaredError(), } )  Entrenar el modelo model.fit( X_train_norm, Y_train, epochs=500, batch_size=10, validation_data=(X_test_norm, Y_test), verbose=0 )  &lt;keras.callbacks.History at 0x1e161d1ed10&gt;  "},{"title":"Evaluar el modelo​","type":1,"pageTitle":"Modelo con mutiples salidas","url":"/article/modelo-con-mutiples-salidas#evaluar-el-modelo","content":"Ahora vamos a evaluar el modelo y graficar los datos para tener una mejor idea de lo que esta pasando model.evaluate(X_test_norm, Y_test)  5/5 [==============================] - 0s 2ms/step - loss: 0.9150 - y1_loss: 0.2067 - y2_loss: 0.7082 - y1_root_mean_squared_error: 0.4547 - y2_root_mean_squared_error: 0.8416  [0.9149883389472961, 0.2067471146583557, 0.7082412838935852, 0.454694539308548, 0.8415707349777222]  "},{"title":"Comparar datos reales y predichos​","type":1,"pageTitle":"Modelo con mutiples salidas","url":"/article/modelo-con-mutiples-salidas#comparar-datos-reales-y-predichos","content":"pred = model.predict(X_test_norm) print(pred[0][:5]) Y_test[0][:5]  5/5 [==============================] - 0s 1ms/step [[33.359097] [37.002422] [23.915794] [35.689697] [24.020735]]  array([33.16, 37.26, 24.03, 35.94, 24.24])  def plot_values(ax, y_true, y_pred, title): ax.scatter(y_true, y_pred, alpha=0.5) ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--') ax.set_xlabel('True value') ax.set_ylabel('Predicted value') ax.set_title(title) # Y1 vs pred fig, ax = plt.subplots(1, 2, figsize=(15, 5)) plot_values(ax[0], Y_test[0], pred[0], 'Y1') plot_values(ax[1], Y_test[1], pred[1], 'Y2')   "},{"title":"Modelo funcional usando el API de Keras","type":0,"sectionRef":"#","url":"/article/modelo-funcional-usando-el-API-de-Keras","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Importar modulo​","type":1,"pageTitle":"Modelo funcional usando el API de Keras","url":"/article/modelo-funcional-usando-el-API-de-Keras#importar-modulo","content":"import tensorflow as tf  "},{"title":"2. Modelo Sequencial​","type":1,"pageTitle":"Modelo funcional usando el API de Keras","url":"/article/modelo-funcional-usando-el-API-de-Keras#modelo-sequencial","content":"Crearemos un modelo secuencial, que es una pila lineal de capas. Para ello, usaremos la función Sequential(), en el que nos basaremos para crear nuestro modelo funcional. def sequential_model(): model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dense(10, activation=tf.nn.softmax) ]) return model  "},{"title":"Modelo con el API funcional​","type":1,"pageTitle":"Modelo funcional usando el API de Keras","url":"/article/modelo-funcional-usando-el-API-de-Keras#modelo-con-el-api-funcional","content":"Ahora crearemos un modelo usando el api funcional de Keras. Este modelo es un poco más flexible que el modelo secuencial, ya que nos permite crear modelos con múltiples entradas y salidas, y también nos permite crear modelos con capas compartidas. def functional_model(): # creamos nuestra entrada input = tf.keras.Input(shape=(28, 28)) # creamos nuestra capas x = tf.keras.layers.Flatten()(input) x = tf.keras.layers.Dense(128, activation='relu')(x) x = tf.keras.layers.Dense(64, activation='softmax')(x) # definimos nuestro modelo model = tf.keras.Model(inputs=input, outputs=x) return model model_func = functional_model()  "},{"title":"3. Entrenar nuestro modelo​","type":1,"pageTitle":"Modelo funcional usando el API de Keras","url":"/article/modelo-funcional-usando-el-API-de-Keras#entrenar-nuestro-modelo","content":"Ahora vamos a entrenar nuestro modelo usando los datos defashion_mnist que es uno de los datasets de ejemplo que vienen con tensorflow. fashion_mnist = tf.keras.datasets.fashion_mnist (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data() # normalizar los datos X_train = X_train / 255.0 X_test = X_test / 255.0 # configurar y conpilar nuestro modelo model_func.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # entrenar el modelo model_func.fit(X_train, y_train, epochs=5)  Epoch 1/5 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2289 - accuracy: 0.9147 Epoch 2/5 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2180 - accuracy: 0.9187 Epoch 3/5 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2137 - accuracy: 0.9207 Epoch 4/5 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2063 - accuracy: 0.9226 Epoch 5/5 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2003 - accuracy: 0.9247  &lt;keras.callbacks.History at 0x212d77f6d50&gt;  "},{"title":"4. Evalular el modelo​","type":1,"pageTitle":"Modelo funcional usando el API de Keras","url":"/article/modelo-funcional-usando-el-API-de-Keras#evalular-el-modelo","content":"Por ultimo vamos a evaluar el modelo con el conjunto de test. Para ello vamos a utilizar la función evaluate del modelo. Esta función nos devuelve el valor de la función de perdida y el valor de la métrica que hemos definido. model_func.evaluate(X_test, y_test)  313/313 [==============================] - 1s 2ms/step - loss: 0.3422 - accuracy: 0.8854  [0.34223565459251404, 0.8853999972343445]  Como hemos visto crear un modelo usando el API funcional de keras, no es complicado, pero si es un poco más complejo que usando el API secuencial. Sin embargo, debemos tener en cuenta que el API funcional nos permite crear modelos más complejos, con más de una entrada y más de una salida, lo cual no es posible con el API secuencial. "},{"title":"Detección de anomalías","type":0,"sectionRef":"#","url":"/article/ml-deteccion-de-anomalias","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Estimación de densidad​","type":1,"pageTitle":"Detección de anomalías","url":"/article/ml-deteccion-de-anomalias#estimación-de-densidad","content":"La detección de anomalías se puede realizar utilizando un modelo de estimación de densidad. La idea es que los datos normales se distribuirán de manera diferente a los datos anormales. Por lo tanto, podemos estimar la densidad de los datos normales y luego identificar los puntos de datos que tienen una densidad significativamente menor como anomalías. Dado el conjunto de datos de entrenamiento{x(1),x(2),…,x(m)}\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)} \\}{x(1),x(2),…,x(m)}, donde cada ejemplo tiene nnncaracterísticas, podemos estimar la densidad de los datos como: p(x)=p(x1;μ1,σ12)×p(x2;μ2,σ22)×…×p(xn;μn,σn2)p(x) = p(x_1; \\mu_1, \\sigma_1^2) \\times p(x_2; \\mu_2, \\sigma_2^2) \\times \\ldots \\times p(x_n; \\mu_n, \\sigma_n^2)p(x)=p(x1​;μ1​,σ12​)×p(x2​;μ2​,σ22​)×…×p(xn​;μn​,σn2​)=∏j=1np(xj;μj,σj2)= \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2)=∏j=1n​p(xj​;μj​,σj2​) "},{"title":"Algoritmo de detección de anomalías​","type":1,"pageTitle":"Detección de anomalías","url":"/article/ml-deteccion-de-anomalias#algoritmo-de-detección-de-anomalías","content":"Elija las características xix_ixi​ que crea que pueden indicar anomalías. Ajuste los parámetrosμ1,…,μn,σ12,…,σn2\\mu_1, \\ldots, \\mu_n, \\sigma_1^2, \\ldots, \\sigma_n^2μ1​,…,μn​,σ12​,…,σn2​ en el conjunto de entrenamiento {x(1),x(2),…,x(m)}\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)} \\}{x(1),x(2),…,x(m)}.μ⃗=1m∑i=1mx(i)⃗\\vec{\\mu} = \\frac{1}{m} \\sum_{i=1}^m \\vec{x^{(i)}}μ​=m1​∑i=1m​x(i)σ2⃗=1m∑i=1m(x(i)⃗−μ⃗)2\\vec{\\sigma^2} = \\frac{1}{m} \\sum_{i=1}^m (\\vec{x^{(i)}} - \\vec{\\mu})^2σ2=m1​∑i=1m​(x(i)−μ​)2 Dado un nuevo ejemplo xxx, compute p(x)p(x)p(x): p(x)=∏j=1np(xj;μj,σj2)=∏j=1n12πσjexp⁡(−(xj−μj)22σj2)p(x) = \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2) = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma_j} \\exp \\left( - \\frac{(x_j - \\mu_j)^2}{2\\sigma_j^2} \\right)p(x)=∏j=1n​p(xj​;μj​,σj2​)=∏j=1n​2π​σj​1​exp(−2σj2​(xj​−μj​)2​) Si p(x)&lt;ϵp(x) &lt; \\epsilonp(x)&lt;ϵ, marque un ejemplo de anomalía.  "},{"title":"Escoger que caracteristicas usar​","type":1,"pageTitle":"Detección de anomalías","url":"/article/ml-deteccion-de-anomalias#escoger-que-caracteristicas-usar","content":"En Deteción de Anomalías, se debe escoger que caracteristicas usar, ya que si se usan todas las caracteristicas, el algoritmo no funcionará correctamentem. "},{"title":"Caracteristicas no gaussianas​","type":1,"pageTitle":"Detección de anomalías","url":"/article/ml-deteccion-de-anomalias#caracteristicas-no-gaussianas","content":"Cuando encontramos caracteristicas que no son gaussianas, se debe aplicar una transformación a los datos para que se vuelvan gaussianos. por ejemplo: x1=log⁡(x1)x_1 = \\log(x_1)x1​=log(x1​)x2=log⁡(x2+c)x_2 = \\log(x_2 + c)x2​=log(x2​+c)x3=x3x_3 = \\sqrt{x_3}x3​=x3​​x4=x41/3x_4 = x_4^{1/3}x4​=x41/3​ En python from scipy.stats import skewnorm import matplotlib.pyplot as plt numValues = 1000 maxValue = 100 skewness = 20 randomValues = skewnorm.rvs(a=skewness, loc=maxValue, size=numValues) randomValues = randomValues - min(randomValues) # cambia el conjunto de datos para que comience en 0 randomValues = randomValues / max(randomValues) # cambia el conjunto de datos para que termine en 1 randomValues = randomValues * maxValue # cambia el conjunto de datos para que termine en maxValue x = randomValues fig, ax = plt.subplots(1, 3, figsize=(15, 5)) ax[0].hist(x, bins=50) ax[0].set_title('X') # x**2 ax[1].hist(x**2, bins=50) ax[1].set_title('X^2') # x**0.4 ax[2].hist(x**0.4, bins=50) ax[2].set_title('X^0.4') plt.show()   "},{"title":"Error en el analisis para detección de anomalías​","type":1,"pageTitle":"Detección de anomalías","url":"/article/ml-deteccion-de-anomalias#error-en-el-analisis-para-detección-de-anomalías","content":"El problema más común en la detección de anomalías es que el conjunto de datos de entrenamiento contiene muy pocos ejemplos de anomalías. Por lo tanto, el algoritmo de detección de anomalías no puede aprender lo suficiente sobre los ejemplos de anomalías para identificarlos correctamente en el conjunto de prueba. "},{"title":"¿Como perfilar código con line_profiler?","type":0,"sectionRef":"#","url":"/article/perfilar-codigo-con-line-profiler","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Instalación​","type":1,"pageTitle":"¿Como perfilar código con line_profiler?","url":"/article/perfilar-codigo-con-line-profiler#instalación","content":"Como line_profiler no viene instalado por defecto en Anaconda, lo instalaremos con conda: En la terminal: pip install line_profiler  En el notebook: ! pip install line_profiler  "},{"title":"¿Cómo funciona en Jupyter?​","type":1,"pageTitle":"¿Como perfilar código con line_profiler?","url":"/article/perfilar-codigo-con-line-profiler#cómo-funciona-en-jupyter","content":"line_profiler es una herramienta que permite perfilar el código de un programa. Esto significa que nos permite ver cuánto tiempo se tarda en ejecutar cada línea de código. Para ello, line_profiler nos permite usar el comando %lprun en Jupyter. Este comando nos permite perfilar una función. Para ello, debemos añadir el decorador @profile a la función que queremos perfilar. cargar el módulo line_profiler en el notebook: %load_ext line_profiler  The line_profiler extension is already loaded. To reload it, use: %reload_ext line_profiler  "},{"title":"Perfilando una función​","type":1,"pageTitle":"¿Como perfilar código con line_profiler?","url":"/article/perfilar-codigo-con-line-profiler#perfilando-una-función","content":"Perfilar una funcion en en jupyter lab ees muy sencillo con el comando%lprun. Para ello vamos a crear una funcion de prueba que calcule el doble de una lista de números:  def funcion_prueba(): data = [1, 2, 3, 4, 5, 6, 7, 8, 9] doble = [] for item in data: doble.append(item * 2) return doble  %lprun -f funcion_prueba funcion_prueba()  Timer unit: 1e-07 s Total time: 8e-06 s Could not find file C:\\Users\\WillyCotrina\\AppData\\Local\\Temp\\ipykernel_14792\\1026023441.py Are you sure you are running this program from the same directory that you ran the profiler from? Continuing without the function's contents. Line # Hits Time Per Hit % Time Line Contents ============================================================== 1 2 1 7.0 7.0 8.8 3 1 3.0 3.0 3.8 4 9 24.0 2.7 30.0 5 9 43.0 4.8 53.8 6 7 1 3.0 3.0 3.8  Como pudimos notar pefilar una funcion es muy sencillo y extremaente util para optimizar el codigo de un programa. "},{"title":"Scraping web con BeautifulSoup","type":0,"sectionRef":"#","url":"/article/scraping-with-BeautifulSoup","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Requerimientos​","type":1,"pageTitle":"Scraping web con BeautifulSoup","url":"/article/scraping-with-BeautifulSoup#requerimientos","content":"Antes de empezar, debemos instalar las siguientes librerías: pip install requests pip install beautifulsoup4  "},{"title":"2. Importar librerías​","type":1,"pageTitle":"Scraping web con BeautifulSoup","url":"/article/scraping-with-BeautifulSoup#importar-librerías","content":"Como en todo codigo de Python, lo primero que debemos hacer es importar las librerías que vamos a usar, en este caso from bs4 import BeautifulSoup import requests  "},{"title":"3. Obtener el contenido de una página web​","type":1,"pageTitle":"Scraping web con BeautifulSoup","url":"/article/scraping-with-BeautifulSoup#obtener-el-contenido-de-una-página-web","content":"BeautifulSoup no puede optener el contenido directamente desde una url, por lo que nos vamos ayudar en el módulo requests url = 'https://peps.python.org/pep-0020/' r = requests.get(url) html = r.text  "},{"title":"4. Ver el contenido de la página​","type":1,"pageTitle":"Scraping web con BeautifulSoup","url":"/article/scraping-with-BeautifulSoup#ver-el-contenido-de-la-página","content":"BeautifulSoup nos permite ver el contenido de la página de una forma más amigable, para ello usamos el método prettify() soup = BeautifulSoup(html, 'html') print(soup.prettify())  &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;/&gt; &lt;meta content=&quot;width=device-width, initial-scale=1.0&quot; name=&quot;viewport&quot;/&gt; &lt;meta content=&quot;light dark&quot; name=&quot;color-scheme&quot;/&gt; &lt;title&gt; PEP 20 – The Zen of Python | peps.python.org &lt;/title&gt; &lt;link href=&quot;../_static/py.png&quot; rel=&quot;shortcut icon&quot;/&gt; &lt;link href=&quot;https://peps.python.org/pep-0020/&quot; rel=&quot;canonical&quot;/&gt; &lt;link href=&quot;../_static/style.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt; &lt;link href=&quot;../_static/mq.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt; &lt;link href=&quot;../_static/pygments.css&quot; id=&quot;pyg-light&quot; media=&quot;(prefers-color-scheme: light)&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt; &lt;link href=&quot;../_static/pygments_dark.css&quot; id=&quot;pyg-dark&quot; media=&quot;(prefers-color-scheme: dark)&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt; &lt;link href=&quot;https://peps.python.org/peps.rss&quot; rel=&quot;alternate&quot; title=&quot;Latest PEPs&quot; type=&quot;application/rss+xml&quot;/&gt; &lt;meta content=&quot;Python Enhancement Proposals (PEPs)&quot; name=&quot;description&quot;/&gt; &lt;/head&gt; &lt;body&gt; &lt;svg style=&quot;display: none;&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt; &lt;symbol id=&quot;svg-sun-half&quot; pointer-events=&quot;all&quot; viewbox=&quot;0 0 24 24&quot;&gt; &lt;title&gt; Following system colour scheme &lt;/title&gt; &lt;svg fill=&quot;none&quot; stroke=&quot;currentColor&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot; stroke-width=&quot;2&quot; viewbox=&quot;0 0 24 24&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt; &lt;circle cx=&quot;12&quot; cy=&quot;12&quot; r=&quot;9&quot;&gt; &lt;/circle&gt; &lt;path d=&quot;M12 3v18m0-12l4.65-4.65M12 14.3l7.37-7.37M12 19.6l8.85-8.85&quot;&gt; &lt;/path&gt; &lt;/svg&gt; &lt;/symbol&gt; &lt;symbol id=&quot;svg-moon&quot; pointer-events=&quot;all&quot; viewbox=&quot;0 0 24 24&quot;&gt; &lt;title&gt; Selected dark colour scheme &lt;/title&gt; &lt;svg fill=&quot;none&quot; stroke=&quot;currentColor&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot; stroke-width=&quot;2&quot; viewbox=&quot;0 0 24 24&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt; &lt;path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot; stroke=&quot;none&quot;&gt; &lt;/path&gt; &lt;path d=&quot;M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z&quot;&gt; &lt;/path&gt; &lt;/svg&gt; &lt;/symbol&gt; &lt;symbol id=&quot;svg-sun&quot; pointer-events=&quot;all&quot; viewbox=&quot;0 0 24 24&quot;&gt; &lt;title&gt; Selected light colour scheme &lt;/title&gt; &lt;svg fill=&quot;none&quot; stroke=&quot;currentColor&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot; stroke-width=&quot;2&quot; viewbox=&quot;0 0 24 24&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt; &lt;circle cx=&quot;12&quot; cy=&quot;12&quot; r=&quot;5&quot;&gt; &lt;/circle&gt; &lt;line x1=&quot;12&quot; x2=&quot;12&quot; y1=&quot;1&quot; y2=&quot;3&quot;&gt; &lt;/line&gt; &lt;line x1=&quot;12&quot; x2=&quot;12&quot; y1=&quot;21&quot; y2=&quot;23&quot;&gt; &lt;/line&gt; &lt;line x1=&quot;4.22&quot; x2=&quot;5.64&quot; y1=&quot;4.22&quot; y2=&quot;5.64&quot;&gt; &lt;/line&gt; &lt;line x1=&quot;18.36&quot; x2=&quot;19.78&quot; y1=&quot;18.36&quot; y2=&quot;19.78&quot;&gt; &lt;/line&gt; &lt;line x1=&quot;1&quot; x2=&quot;3&quot; y1=&quot;12&quot; y2=&quot;12&quot;&gt; &lt;/line&gt; &lt;line x1=&quot;21&quot; x2=&quot;23&quot; y1=&quot;12&quot; y2=&quot;12&quot;&gt; &lt;/line&gt; &lt;line x1=&quot;4.22&quot; x2=&quot;5.64&quot; y1=&quot;19.78&quot; y2=&quot;18.36&quot;&gt; &lt;/line&gt; &lt;line x1=&quot;18.36&quot; x2=&quot;19.78&quot; y1=&quot;5.64&quot; y2=&quot;4.22&quot;&gt; &lt;/line&gt; &lt;/svg&gt; &lt;/symbol&gt; &lt;/svg&gt; &lt;script&gt; document.documentElement.dataset.colour_scheme = localStorage.getItem(&quot;colour_scheme&quot;) || &quot;auto&quot; &lt;/script&gt; &lt;section id=&quot;pep-page-section&quot;&gt; &lt;header&gt; &lt;h1&gt; Python Enhancement Proposals &lt;/h1&gt; &lt;ul class=&quot;breadcrumbs&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://www.python.org/&quot; title=&quot;The Python Programming Language&quot;&gt; Python &lt;/a&gt; » &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;../pep-0000/&quot;&gt; PEP Index &lt;/a&gt; » &lt;/li&gt; &lt;li&gt; PEP 20 &lt;/li&gt; &lt;/ul&gt; &lt;button id=&quot;colour-scheme-cycler&quot; onclick=&quot;setColourScheme(nextColourScheme())&quot;&gt; &lt;svg aria-hidden=&quot;true&quot; class=&quot;colour-scheme-icon-when-auto&quot;&gt; &lt;use href=&quot;#svg-sun-half&quot;&gt; &lt;/use&gt; &lt;/svg&gt; &lt;svg aria-hidden=&quot;true&quot; class=&quot;colour-scheme-icon-when-dark&quot;&gt; &lt;use href=&quot;#svg-moon&quot;&gt; &lt;/use&gt; &lt;/svg&gt; &lt;svg aria-hidden=&quot;true&quot; class=&quot;colour-scheme-icon-when-light&quot;&gt; &lt;use href=&quot;#svg-sun&quot;&gt; &lt;/use&gt; &lt;/svg&gt; &lt;span class=&quot;visually-hidden&quot;&gt; Toggle light / dark / auto colour theme &lt;/span&gt; &lt;/button&gt; &lt;/header&gt; &lt;article&gt; &lt;section id=&quot;pep-content&quot;&gt; &lt;h1 class=&quot;page-title&quot;&gt; PEP 20 – The Zen of Python &lt;/h1&gt; &lt;dl class=&quot;rfc2822 field-list simple&quot;&gt; &lt;dt class=&quot;field-odd&quot;&gt; Author &lt;span class=&quot;colon&quot;&gt; : &lt;/span&gt; &lt;/dt&gt; &lt;dd class=&quot;field-odd&quot;&gt; Tim Peters &amp;lt;tim.peters at gmail.com&amp;gt; &lt;/dd&gt; &lt;dt class=&quot;field-even&quot;&gt; Status &lt;span class=&quot;colon&quot;&gt; : &lt;/span&gt; &lt;/dt&gt; &lt;dd class=&quot;field-even&quot;&gt; &lt;abbr title=&quot;Currently valid informational guidance, or an in-use process&quot;&gt; Active &lt;/abbr&gt; &lt;/dd&gt; &lt;dt class=&quot;field-odd&quot;&gt; Type &lt;span class=&quot;colon&quot;&gt; : &lt;/span&gt; &lt;/dt&gt; &lt;dd class=&quot;field-odd&quot;&gt; &lt;abbr title=&quot;Non-normative PEP containing background, guidelines or other information relevant to the Python ecosystem&quot;&gt; Informational &lt;/abbr&gt; &lt;/dd&gt; &lt;dt class=&quot;field-even&quot;&gt; Created &lt;span class=&quot;colon&quot;&gt; : &lt;/span&gt; &lt;/dt&gt; &lt;dd class=&quot;field-even&quot;&gt; 19-Aug-2004 &lt;/dd&gt; &lt;dt class=&quot;field-odd&quot;&gt; Post-History &lt;span class=&quot;colon&quot;&gt; : &lt;/span&gt; &lt;/dt&gt; &lt;dd class=&quot;field-odd&quot;&gt; 22-Aug-2004 &lt;/dd&gt; &lt;/dl&gt; &lt;hr class=&quot;docutils&quot;/&gt; &lt;section id=&quot;contents&quot;&gt; &lt;details&gt; &lt;summary&gt; Table of Contents &lt;/summary&gt; &lt;ul class=&quot;simple&quot;&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#abstract&quot;&gt; Abstract &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#the-zen-of-python&quot;&gt; The Zen of Python &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#easter-egg&quot;&gt; Easter Egg &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#references&quot;&gt; References &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#copyright&quot;&gt; Copyright &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/details&gt; &lt;/section&gt; &lt;section id=&quot;abstract&quot;&gt; &lt;h2&gt; &lt;a class=&quot;toc-backref&quot; href=&quot;#abstract&quot; role=&quot;doc-backlink&quot;&gt; Abstract &lt;/a&gt; &lt;/h2&gt; &lt;p&gt; Long time Pythoneer Tim Peters succinctly channels the BDFL’s guiding principles for Python’s design into 20 aphorisms, only 19 of which have been written down. &lt;/p&gt; &lt;/section&gt; &lt;section id=&quot;the-zen-of-python&quot;&gt; &lt;h2&gt; &lt;a class=&quot;toc-backref&quot; href=&quot;#the-zen-of-python&quot; role=&quot;doc-backlink&quot;&gt; The Zen of Python &lt;/a&gt; &lt;/h2&gt; &lt;div class=&quot;highlight-text notranslate&quot;&gt; &lt;div class=&quot;highlight&quot;&gt; &lt;pre&gt;&lt;span&gt;&lt;/span&gt;Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! &lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/section&gt; &lt;section id=&quot;easter-egg&quot;&gt; &lt;h2&gt; &lt;a class=&quot;toc-backref&quot; href=&quot;#easter-egg&quot; role=&quot;doc-backlink&quot;&gt; Easter Egg &lt;/a&gt; &lt;/h2&gt; &lt;div class=&quot;highlight-pycon notranslate&quot;&gt; &lt;div class=&quot;highlight&quot;&gt; &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;this&lt;/span&gt; &lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/section&gt; &lt;section id=&quot;references&quot;&gt; &lt;h2&gt; &lt;a class=&quot;toc-backref&quot; href=&quot;#references&quot; role=&quot;doc-backlink&quot;&gt; References &lt;/a&gt; &lt;/h2&gt; &lt;p&gt; Originally posted to &lt;a class=&quot;reference external&quot; href=&quot;mailto:comp.lang.python/python-list%40python.org&quot;&gt; comp &lt;span&gt; . &lt;/span&gt; lang &lt;span&gt; . &lt;/span&gt; python/python-list &lt;span&gt; @ &lt;/span&gt; python &lt;span&gt; . &lt;/span&gt; org &lt;/a&gt; under a thread called &lt;a class=&quot;reference external&quot; href=&quot;https://groups.google.com/d/msg/comp.lang.python/B_VxeTBClM0/L8W9KlsiriUJ&quot;&gt; “The Way of Python” &lt;/a&gt; &lt;/p&gt; &lt;/section&gt; &lt;section id=&quot;copyright&quot;&gt; &lt;h2&gt; &lt;a class=&quot;toc-backref&quot; href=&quot;#copyright&quot; role=&quot;doc-backlink&quot;&gt; Copyright &lt;/a&gt; &lt;/h2&gt; &lt;p&gt; This document has been placed in the public domain. &lt;/p&gt; &lt;/section&gt; &lt;/section&gt; &lt;hr class=&quot;docutils&quot;/&gt; &lt;p&gt; Source: &lt;a class=&quot;reference external&quot; href=&quot;https://github.com/python/peps/blob/main/pep-0020.txt&quot;&gt; https://github.com/python/peps/blob/main/pep-0020.txt &lt;/a&gt; &lt;/p&gt; &lt;p&gt; Last modified: &lt;a class=&quot;reference external&quot; href=&quot;https://github.com/python/peps/commits/main/pep-0020.txt&quot;&gt; 2022-03-15 17:40:34+00:00 GMT &lt;/a&gt; &lt;/p&gt; &lt;/article&gt; &lt;nav id=&quot;pep-sidebar&quot;&gt; &lt;h2&gt; Contents &lt;/h2&gt; &lt;ul&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#abstract&quot;&gt; Abstract &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#the-zen-of-python&quot;&gt; The Zen of Python &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#easter-egg&quot;&gt; Easter Egg &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#references&quot;&gt; References &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;reference internal&quot; href=&quot;#copyright&quot;&gt; Copyright &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; &lt;a href=&quot;https://github.com/python/peps/blob/main/pep-0020.txt&quot; id=&quot;source&quot;&gt; Page Source (GitHub) &lt;/a&gt; &lt;/nav&gt; &lt;/section&gt; &lt;script src=&quot;../_static/colour_scheme.js&quot;&gt; &lt;/script&gt; &lt;script src=&quot;../_static/wrap_tables.js&quot;&gt; &lt;/script&gt; &lt;script src=&quot;../_static/sticky_banner.js&quot;&gt; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt;  "},{"title":"5. Navegar por el contenido​","type":1,"pageTitle":"Scraping web con BeautifulSoup","url":"/article/scraping-with-BeautifulSoup#navegar-por-el-contenido","content":"Una vez que tenemos el contenido de la página, podemos navegar por el contenido usando los métodos find() y find_all(). a = soup.find_all('a') for link in a: print(link.get('href'))  https://www.python.org/ ../pep-0000/ #abstract #the-zen-of-python #easter-egg #references #copyright #abstract #the-zen-of-python #easter-egg #references mailto:comp.lang.python/python-list%40python.org https://groups.google.com/d/msg/comp.lang.python/B_VxeTBClM0/L8W9KlsiriUJ #copyright https://github.com/python/peps/blob/main/pep-0020.txt https://github.com/python/peps/commits/main/pep-0020.txt #abstract #the-zen-of-python #easter-egg #references #copyright https://github.com/python/peps/blob/main/pep-0020.txt  Para navegar por el contenido del documento, BeautifulSoup tiene varios métodos que nos permiten navegar por el contenido del documento, los más usados son: find(): Nos permite encontrar el primer elemento que cumpla con las condiciones especificadas.find_all(): Nos permite encontrar todos los elementos que cumplan con las condiciones especificadas.select(): Nos permite encontrar todos los elementos que cumplan con las condiciones especificadas usando selectores CSS.select_one(): Nos permite encontrar el primer elemento que cumpla con las condiciones especificadas usando selectores CSS.find_parent(): Nos permite encontrar el elemento padre que cumpla con las condiciones especificadas.find_parents(): Nos permite encontrar todos los elementos padres que cumplan con las condiciones especificadas. Entre otros, para mas información ir a ladocumentación # buscar elementos a por class elements = soup.find_all('a', {'class': 'reference internal'}) for element in elements: print(element.get('href'))  #abstract #the-zen-of-python #easter-egg #references #copyright #abstract #the-zen-of-python #easter-egg #references #copyright  # buscar elementos por id elements = soup.find_all(id = 'copyright') print(elements)  [&lt;section id=&quot;copyright&quot;&gt; &lt;h2&gt;&lt;a class=&quot;toc-backref&quot; href=&quot;#copyright&quot; role=&quot;doc-backlink&quot;&gt;Copyright&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;This document has been placed in the public domain.&lt;/p&gt; &lt;/section&gt;]  # buscar elementos por texto elements = soup.find_all(string='PEP 20') elements  []  # buscar elementos por texto con expresiones regulares import re elements = soup.find_all(re.compile('^sy')) elements  [&lt;symbol id=&quot;svg-sun-half&quot; pointer-events=&quot;all&quot; viewbox=&quot;0 0 24 24&quot;&gt; &lt;title&gt;Following system colour scheme&lt;/title&gt; &lt;svg fill=&quot;none&quot; stroke=&quot;currentColor&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot; stroke-width=&quot;2&quot; viewbox=&quot;0 0 24 24&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt; &lt;circle cx=&quot;12&quot; cy=&quot;12&quot; r=&quot;9&quot;&gt;&lt;/circle&gt; &lt;path d=&quot;M12 3v18m0-12l4.65-4.65M12 14.3l7.37-7.37M12 19.6l8.85-8.85&quot;&gt;&lt;/path&gt; &lt;/svg&gt; &lt;/symbol&gt;, &lt;symbol id=&quot;svg-moon&quot; pointer-events=&quot;all&quot; viewbox=&quot;0 0 24 24&quot;&gt; &lt;title&gt;Selected dark colour scheme&lt;/title&gt; &lt;svg fill=&quot;none&quot; stroke=&quot;currentColor&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot; stroke-width=&quot;2&quot; viewbox=&quot;0 0 24 24&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt; &lt;path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot; stroke=&quot;none&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z&quot;&gt;&lt;/path&gt; &lt;/svg&gt; &lt;/symbol&gt;, &lt;symbol id=&quot;svg-sun&quot; pointer-events=&quot;all&quot; viewbox=&quot;0 0 24 24&quot;&gt; &lt;title&gt;Selected light colour scheme&lt;/title&gt; &lt;svg fill=&quot;none&quot; stroke=&quot;currentColor&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot; stroke-width=&quot;2&quot; viewbox=&quot;0 0 24 24&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt; &lt;circle cx=&quot;12&quot; cy=&quot;12&quot; r=&quot;5&quot;&gt;&lt;/circle&gt; &lt;line x1=&quot;12&quot; x2=&quot;12&quot; y1=&quot;1&quot; y2=&quot;3&quot;&gt;&lt;/line&gt; &lt;line x1=&quot;12&quot; x2=&quot;12&quot; y1=&quot;21&quot; y2=&quot;23&quot;&gt;&lt;/line&gt; &lt;line x1=&quot;4.22&quot; x2=&quot;5.64&quot; y1=&quot;4.22&quot; y2=&quot;5.64&quot;&gt;&lt;/line&gt; &lt;line x1=&quot;18.36&quot; x2=&quot;19.78&quot; y1=&quot;18.36&quot; y2=&quot;19.78&quot;&gt;&lt;/line&gt; &lt;line x1=&quot;1&quot; x2=&quot;3&quot; y1=&quot;12&quot; y2=&quot;12&quot;&gt;&lt;/line&gt; &lt;line x1=&quot;21&quot; x2=&quot;23&quot; y1=&quot;12&quot; y2=&quot;12&quot;&gt;&lt;/line&gt; &lt;line x1=&quot;4.22&quot; x2=&quot;5.64&quot; y1=&quot;19.78&quot; y2=&quot;18.36&quot;&gt;&lt;/line&gt; &lt;line x1=&quot;18.36&quot; x2=&quot;19.78&quot; y1=&quot;5.64&quot; y2=&quot;4.22&quot;&gt;&lt;/line&gt; &lt;/svg&gt; &lt;/symbol&gt;]  "},{"title":"6. Conclusiones​","type":1,"pageTitle":"Scraping web con BeautifulSoup","url":"/article/scraping-with-BeautifulSoup#conclusiones","content":"Como hemos visto, BeautifulSoup es una librería muy poderosa que nos permite extraer información de páginas web de una forma rapida y sencilla, pero tiene sus limitaciones y no es la mejor opción para extraer información de páginas web complejas, para ello existen otras librerías como Scrapy que nos permiten extraer información de páginas web de una forma más eficiente. Cuando hagamos scraping de páginas web, debes de tener en cuenta que tienes que respetar las políticas de privacidad de la página web, para ello debes de leer los términos y condiciones de la página web, asi como el acceso a los robots de búsqueda, del archivo robots.txt de la página web. "},{"title":"Broadcasting con NumPy","type":0,"sectionRef":"#","url":"/article/tags/broadcasting","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Ejemplos de broadcasting​","type":1,"pageTitle":"Broadcasting con NumPy","url":"/article/tags/broadcasting#ejemplos-de-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t1 x 5\t10 x 5 10 x 5\t5\t10 x 5 10 x 5\t10 x 1\t10 x 5 10 x 5\tscalar\t10 x 5 import numpy as np from rich import print # matris de 10x5 m10_5 = np.random.randint(0, 10, (10, 5)) m1_5 = np.random.randint(0, 10, (1, 5)) m5 = np.random.randint(0, 10, (5)) m10_1 = np.random.randint(0, 10, (10, 1)) scalar = 5 print('Broadcasting de 10x5 + 1x5') print(m10_5 + m1_5) print('Broadcasting de 10x5 + 5') print(m10_5 + m5) print('Broadcasting de 10x5 + 10x1') print(m10_5 + m10_1) print(m10_5 + scalar)  Broadcasting de 10x5 + 1x5  [[ 9 2 6 5 1] [ 5 9 10 10 8] [ 6 9 3 10 3] [ 8 8 7 4 8] [ 5 2 6 10 5] [ 7 9 9 8 3] [ 7 5 3 5 7] [ 4 7 7 3 1] [ 2 8 6 12 7] [ 9 9 12 10 10]]  Broadcasting de 10x5 + 5  [[ 7 11 9 2 5] [ 3 18 13 7 12] [ 4 18 6 7 7] [ 6 17 10 1 12] [ 3 11 9 7 9] [ 5 18 12 5 7] [ 5 14 6 2 11] [ 2 16 10 0 5] [ 0 17 9 9 11] [ 7 18 15 7 14]]  Broadcasting de 10x5 + 10x1  [[14 9 10 9 7] [ 5 11 9 9 9] [ 5 10 1 8 3] [11 13 9 6 12] [ 9 8 9 13 10] [11 15 12 11 8] [ 6 6 1 3 7] [ 3 8 5 1 1] [ 8 16 11 17 14] [ 7 9 9 7 9]]  [[12 7 8 7 5] [ 8 14 12 12 12] [ 9 14 5 12 7] [11 13 9 6 12] [ 8 7 8 12 9] [10 14 11 10 7] [10 10 5 7 11] [ 7 12 9 5 5] [ 5 13 8 14 11] [12 14 14 12 14]]  "},{"title":"Ejemplos de no broadcasting​","type":1,"pageTitle":"Broadcasting con NumPy","url":"/article/tags/broadcasting#ejemplos-de-no-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t5 x 10\tError 10 x 5\t10\tError m10_5 = np.random.randint(0, 10, (10, 5)) m5_10 = np.random.randint(0, 10, (5, 10)) m10 = np.random.randint(0, 10, (10)) print('Broadcasting de 10x5 + 5x10') print(m10_5 + m5_10)  Broadcasting de 10x5 + 5x10  ValueError: operands could not be broadcast together with shapes (10,5) (5,10)  print('Broadcasting de 10x5 + 10') print(m10_5 + m10)  Broadcasting de 10x5 + 10  ValueError: operands could not be broadcast together with shapes (10,5) (10,)  "},{"title":"¿Como ver los días de la semana usando pandas?`","type":0,"sectionRef":"#","url":"/article/tags/datetime","content":"¿Como ver los días de la semana usando pandas?` Con pandas podemos ver de forma muy sencilla los días de la semana de una fecha en específico, para esto usaremos la función weekday_name y con dayofweek podemos ver el número del día de la semana. import pandas day = pandas.to_datetime('2023-07-10') print(day.dayofweek, day.day_name()) 0 Monday # todos los días de la semana week = pandas.date_range(start='2023-07-10', periods=7, freq='D') for day in week: print(day.dayofweek, day.day_name()) 0 Monday 1 Tuesday 2 Wednesday 3 Thursday 4 Friday 5 Saturday 6 Sunday # df con los dias de la semana df = pandas.DataFrame(week, columns=['date']) df['dayofweek'] = df['date'].dt.dayofweek df['dayname'] = df['date'].dt.day_name() df.set_index('date', inplace=True) df dayofweek\tdaynamedate 2023-07-10\t0\tMonday 2023-07-11\t1\tTuesday 2023-07-12\t2\tWednesday 2023-07-13\t3\tThursday 2023-07-14\t4\tFriday 2023-07-15\t5\tSaturday 2023-07-16\t6\tSunday","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Comparar series de tiempo con pandas","type":0,"sectionRef":"#","url":"/article/tags/finanzas","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Importar librerías​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/finanzas#importar-librerías","content":"Para este ejercicio, se necesitara de las siguientes librerías: pip install pandas pip install matplotlib pip install yfinance  Usaremos la librería yfinance para obtener los datos de las acciones de las empresas tecnológicas, hay otras librearías que también pueden ayudar con esta tarea como pandas_datareader o quandl. import pandas as pd import matplotlib.pyplot as plt import yfinance as yf  "},{"title":"2. Obtener datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/finanzas#obtener-datos","content":"Para este ejemplo, se obtendrán los datos de las acciones de las empresas tecnológicas desde el 2015 de google, amazon, facebook, apple y microsoft. tickets = ['GOOG', 'AMZN', 'META', 'AAPL', 'MSFT'] start_date = '2015-01-01' end_date = '2023-01-01' df = yf.download(tickets, start=start_date, end=end_date)['Adj Close'] df.head()  [*********************100%***********************] 5 of 5 completed  \tAAPL\tAMZN\tGOOG\tMETA\tMSFTDate 2015-01-02\t24.531763\t15.4260\t26.168653\t78.449997\t40.620667 2015-01-05\t23.840666\t15.1095\t25.623152\t77.190002\t40.247116 2015-01-06\t23.842913\t14.7645\t25.029282\t76.150002\t39.656406 2015-01-07\t24.177238\t14.9210\t24.986401\t76.150002\t40.160259 2015-01-08\t25.106184\t15.0230\t25.065184\t78.180000\t41.341694 "},{"title":"3. Normalizar datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/finanzas#normalizar-datos","content":"Para poder comparar los datos vamos a normalizarlos, para esto se usará la siguiente fórmula: $$ \\frac{P_t}{P_0} * 100 $$ Donde PtP_tPt​ es el precio en el tiempo ttt yP0P_0P0​ es el precio inicial. normalized_df = df / df.iloc[0] * 100 normalized_df.head()  \tAAPL\tAMZN\tGOOG\tMETA\tMSFTDate 2015-01-02\t100.000000\t100.000000\t100.000000\t100.000000\t100.000000 2015-01-05\t97.182847\t97.948271\t97.915438\t98.393888\t99.080393 2015-01-06\t97.192006\t95.711785\t95.646043\t97.068202\t97.626183 2015-01-07\t98.554834\t96.726305\t95.482179\t97.068202\t98.866569 2015-01-08\t102.341540\t97.387528\t95.783238\t99.655836\t101.775026 "},{"title":"4. Graficar datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/finanzas#graficar-datos","content":"Por ultimo grafiaremos los datos para poder compararlos y ver como se han comportado en el tiempo. normalized_df.plot(figsize=(15, 10)) plt.show()   "},{"title":"5. Conclusiones​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/finanzas#conclusiones","content":"Como hemos podido ver hacer una comparación de series de tiempo es muy sencillo con pandas, y nos permite ver como se han comportado las acciones de las empresas tecnológicas en los últimos años, las concluciones respecto al comportamiento de las acciones de las empresas tecnológicas se las dejo a ustedes. "},{"title":"Comparar series de tiempo con pandas","type":0,"sectionRef":"#","url":"/article/tags/matplotlib","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Importar librerías​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/matplotlib#importar-librerías","content":"Para este ejercicio, se necesitara de las siguientes librerías: pip install pandas pip install matplotlib pip install yfinance  Usaremos la librería yfinance para obtener los datos de las acciones de las empresas tecnológicas, hay otras librearías que también pueden ayudar con esta tarea como pandas_datareader o quandl. import pandas as pd import matplotlib.pyplot as plt import yfinance as yf  "},{"title":"2. Obtener datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/matplotlib#obtener-datos","content":"Para este ejemplo, se obtendrán los datos de las acciones de las empresas tecnológicas desde el 2015 de google, amazon, facebook, apple y microsoft. tickets = ['GOOG', 'AMZN', 'META', 'AAPL', 'MSFT'] start_date = '2015-01-01' end_date = '2023-01-01' df = yf.download(tickets, start=start_date, end=end_date)['Adj Close'] df.head()  [*********************100%***********************] 5 of 5 completed  \tAAPL\tAMZN\tGOOG\tMETA\tMSFTDate 2015-01-02\t24.531763\t15.4260\t26.168653\t78.449997\t40.620667 2015-01-05\t23.840666\t15.1095\t25.623152\t77.190002\t40.247116 2015-01-06\t23.842913\t14.7645\t25.029282\t76.150002\t39.656406 2015-01-07\t24.177238\t14.9210\t24.986401\t76.150002\t40.160259 2015-01-08\t25.106184\t15.0230\t25.065184\t78.180000\t41.341694 "},{"title":"3. Normalizar datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/matplotlib#normalizar-datos","content":"Para poder comparar los datos vamos a normalizarlos, para esto se usará la siguiente fórmula: $$ \\frac{P_t}{P_0} * 100 $$ Donde PtP_tPt​ es el precio en el tiempo ttt yP0P_0P0​ es el precio inicial. normalized_df = df / df.iloc[0] * 100 normalized_df.head()  \tAAPL\tAMZN\tGOOG\tMETA\tMSFTDate 2015-01-02\t100.000000\t100.000000\t100.000000\t100.000000\t100.000000 2015-01-05\t97.182847\t97.948271\t97.915438\t98.393888\t99.080393 2015-01-06\t97.192006\t95.711785\t95.646043\t97.068202\t97.626183 2015-01-07\t98.554834\t96.726305\t95.482179\t97.068202\t98.866569 2015-01-08\t102.341540\t97.387528\t95.783238\t99.655836\t101.775026 "},{"title":"4. Graficar datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/matplotlib#graficar-datos","content":"Por ultimo grafiaremos los datos para poder compararlos y ver como se han comportado en el tiempo. normalized_df.plot(figsize=(15, 10)) plt.show()   "},{"title":"5. Conclusiones​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/matplotlib#conclusiones","content":"Como hemos podido ver hacer una comparación de series de tiempo es muy sencillo con pandas, y nos permite ver como se han comportado las acciones de las empresas tecnológicas en los últimos años, las concluciones respecto al comportamiento de las acciones de las empresas tecnológicas se las dejo a ustedes. "},{"title":"Broadcasting con NumPy","type":0,"sectionRef":"#","url":"/article/tags/numpy","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Ejemplos de broadcasting​","type":1,"pageTitle":"Broadcasting con NumPy","url":"/article/tags/numpy#ejemplos-de-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t1 x 5\t10 x 5 10 x 5\t5\t10 x 5 10 x 5\t10 x 1\t10 x 5 10 x 5\tscalar\t10 x 5 import numpy as np from rich import print # matris de 10x5 m10_5 = np.random.randint(0, 10, (10, 5)) m1_5 = np.random.randint(0, 10, (1, 5)) m5 = np.random.randint(0, 10, (5)) m10_1 = np.random.randint(0, 10, (10, 1)) scalar = 5 print('Broadcasting de 10x5 + 1x5') print(m10_5 + m1_5) print('Broadcasting de 10x5 + 5') print(m10_5 + m5) print('Broadcasting de 10x5 + 10x1') print(m10_5 + m10_1) print(m10_5 + scalar)  Broadcasting de 10x5 + 1x5  [[ 9 2 6 5 1] [ 5 9 10 10 8] [ 6 9 3 10 3] [ 8 8 7 4 8] [ 5 2 6 10 5] [ 7 9 9 8 3] [ 7 5 3 5 7] [ 4 7 7 3 1] [ 2 8 6 12 7] [ 9 9 12 10 10]]  Broadcasting de 10x5 + 5  [[ 7 11 9 2 5] [ 3 18 13 7 12] [ 4 18 6 7 7] [ 6 17 10 1 12] [ 3 11 9 7 9] [ 5 18 12 5 7] [ 5 14 6 2 11] [ 2 16 10 0 5] [ 0 17 9 9 11] [ 7 18 15 7 14]]  Broadcasting de 10x5 + 10x1  [[14 9 10 9 7] [ 5 11 9 9 9] [ 5 10 1 8 3] [11 13 9 6 12] [ 9 8 9 13 10] [11 15 12 11 8] [ 6 6 1 3 7] [ 3 8 5 1 1] [ 8 16 11 17 14] [ 7 9 9 7 9]]  [[12 7 8 7 5] [ 8 14 12 12 12] [ 9 14 5 12 7] [11 13 9 6 12] [ 8 7 8 12 9] [10 14 11 10 7] [10 10 5 7 11] [ 7 12 9 5 5] [ 5 13 8 14 11] [12 14 14 12 14]]  "},{"title":"Ejemplos de no broadcasting​","type":1,"pageTitle":"Broadcasting con NumPy","url":"/article/tags/numpy#ejemplos-de-no-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t5 x 10\tError 10 x 5\t10\tError m10_5 = np.random.randint(0, 10, (10, 5)) m5_10 = np.random.randint(0, 10, (5, 10)) m10 = np.random.randint(0, 10, (10)) print('Broadcasting de 10x5 + 5x10') print(m10_5 + m5_10)  Broadcasting de 10x5 + 5x10  ValueError: operands could not be broadcast together with shapes (10,5) (5,10)  print('Broadcasting de 10x5 + 10') print(m10_5 + m10)  Broadcasting de 10x5 + 10  ValueError: operands could not be broadcast together with shapes (10,5) (10,)  "},{"title":"Comparar series de tiempo con pandas","type":0,"sectionRef":"#","url":"/article/tags/series-de-tiempo","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Importar librerías​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/series-de-tiempo#importar-librerías","content":"Para este ejercicio, se necesitara de las siguientes librerías: pip install pandas pip install matplotlib pip install yfinance  Usaremos la librería yfinance para obtener los datos de las acciones de las empresas tecnológicas, hay otras librearías que también pueden ayudar con esta tarea como pandas_datareader o quandl. import pandas as pd import matplotlib.pyplot as plt import yfinance as yf  "},{"title":"2. Obtener datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/series-de-tiempo#obtener-datos","content":"Para este ejemplo, se obtendrán los datos de las acciones de las empresas tecnológicas desde el 2015 de google, amazon, facebook, apple y microsoft. tickets = ['GOOG', 'AMZN', 'META', 'AAPL', 'MSFT'] start_date = '2015-01-01' end_date = '2023-01-01' df = yf.download(tickets, start=start_date, end=end_date)['Adj Close'] df.head()  [*********************100%***********************] 5 of 5 completed  \tAAPL\tAMZN\tGOOG\tMETA\tMSFTDate 2015-01-02\t24.531763\t15.4260\t26.168653\t78.449997\t40.620667 2015-01-05\t23.840666\t15.1095\t25.623152\t77.190002\t40.247116 2015-01-06\t23.842913\t14.7645\t25.029282\t76.150002\t39.656406 2015-01-07\t24.177238\t14.9210\t24.986401\t76.150002\t40.160259 2015-01-08\t25.106184\t15.0230\t25.065184\t78.180000\t41.341694 "},{"title":"3. Normalizar datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/series-de-tiempo#normalizar-datos","content":"Para poder comparar los datos vamos a normalizarlos, para esto se usará la siguiente fórmula: $$ \\frac{P_t}{P_0} * 100 $$ Donde PtP_tPt​ es el precio en el tiempo ttt yP0P_0P0​ es el precio inicial. normalized_df = df / df.iloc[0] * 100 normalized_df.head()  \tAAPL\tAMZN\tGOOG\tMETA\tMSFTDate 2015-01-02\t100.000000\t100.000000\t100.000000\t100.000000\t100.000000 2015-01-05\t97.182847\t97.948271\t97.915438\t98.393888\t99.080393 2015-01-06\t97.192006\t95.711785\t95.646043\t97.068202\t97.626183 2015-01-07\t98.554834\t96.726305\t95.482179\t97.068202\t98.866569 2015-01-08\t102.341540\t97.387528\t95.783238\t99.655836\t101.775026 "},{"title":"4. Graficar datos​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/series-de-tiempo#graficar-datos","content":"Por ultimo grafiaremos los datos para poder compararlos y ver como se han comportado en el tiempo. normalized_df.plot(figsize=(15, 10)) plt.show()   "},{"title":"5. Conclusiones​","type":1,"pageTitle":"Comparar series de tiempo con pandas","url":"/article/tags/series-de-tiempo#conclusiones","content":"Como hemos podido ver hacer una comparación de series de tiempo es muy sencillo con pandas, y nos permite ver como se han comportado las acciones de las empresas tecnológicas en los últimos años, las concluciones respecto al comportamiento de las acciones de las empresas tecnológicas se las dejo a ustedes. "},{"title":"Resampling de series temporales con Pandas","type":0,"sectionRef":"#","url":"/article/time-series-resampling-with-pandas","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Frecuencias de series de tiempo​","type":1,"pageTitle":"Resampling de series temporales con Pandas","url":"/article/time-series-resampling-with-pandas#frecuencias-de-series-de-tiempo","content":"Las series de tiempo pueden tener diferentes frecuencias, ahora vamos a ver las frecuencias que podemos encontrar en pandas. Código\tDescripciónB\tFrecuencia de negocios C\tFrecuencia personalizada D\tFrecuencia diaria W\tFrecuencia semanal M\tFrecuencia mensual Q\tFrecuencia trimestral A\tFrecuencia anual H\tFrecuencia horaria T\tFrecuencia minutal S\tFrecuencia segundal "},{"title":"2. Importar librerías​","type":1,"pageTitle":"Resampling de series temporales con Pandas","url":"/article/time-series-resampling-with-pandas#importar-librerías","content":"Primero empesaremos importando las librerías que vamos a utilizar. import pandas as pd import numpy as np import matplotlib.pyplot as plt  "},{"title":"3. Cagar y preparar los datos​","type":1,"pageTitle":"Resampling de series temporales con Pandas","url":"/article/time-series-resampling-with-pandas#cagar-y-preparar-los-datos","content":"Ahora vamos a crear un DataFrame con datos usando una frecuencia diaria. date_range = pd.date_range('01/01/2020', periods=365, freq='D') # crearemos datos para la demostración data = np.random.randn(len(date_range)) df = pd.DataFrame(data, index=date_range, columns=['Value']) # graficamos los datos df.plot() plt.show()   "},{"title":"4. Resampling a una frecuencia mas baja (downsampling)​","type":1,"pageTitle":"Resampling de series temporales con Pandas","url":"/article/time-series-resampling-with-pandas#resampling-a-una-frecuencia-mas-baja-downsampling","content":"Ahora vamos a convertir la serie de tiempo con frecuencia diaria a una serie de tiempo con frecuencia mensual. Para hacer esto vamos a usar el método resample(), al tener una frecuencia mas baja tenemos que especificar como queremos que se agreguen los datos, vamos a tener que ingresar una función de agregación. En este caso vamos a usar la funciónmean() para calcular el promedio de los datos, peru tu puedes usar cualquier función de agregación que necesites. df_monthly_mean = df.resample('M').mean() df_monthly_mean.head(5) # graficamos los datos df_monthly_mean.plot() plt.show()   "},{"title":"5. Resampling a una frecuencia mas alta (upsampling)​","type":1,"pageTitle":"Resampling de series temporales con Pandas","url":"/article/time-series-resampling-with-pandas#resampling-a-una-frecuencia-mas-alta-upsampling","content":"Ya vimos como disminuir la frecuencia, ahora veremos como aumentar la frecuencia. Para hacer esto vamos a usar el método resample() y especificar la frecuencia que queremos. En este caso vamos a aumentar la frecuencia de diaria a horaria, para esto vamos a usar el código H que significa frecuencia horaria. df_hourly = df.resample('H').ffill() # otros metodos: bfill, interpolate  Para poder ver con mas detalle los datos haremos un acercamiento a un periodo de tiempo especifico. df_hourly.loc['2020-01-01':'2020-01-30'].plot() plt.show()   "},{"title":"6. Manejar los datos faltantes​","type":1,"pageTitle":"Resampling de series temporales con Pandas","url":"/article/time-series-resampling-with-pandas#manejar-los-datos-faltantes","content":"Como pudimos ver en el ejemplo anterior cuando aumentamos la frecuencía de diaria a horaria, muchos valores se convirtieron en NaN. Para solucionar esto vamos a usar el método interpolate() para interpolar los valores faltantes. df_hourly = df.resample('H').interpolate() df_hourly.loc['2020-01-01':'2020-01-30'].plot() plt.show()   "},{"title":"7. Resampling con multiples metodos de agregación​","type":1,"pageTitle":"Resampling de series temporales con Pandas","url":"/article/time-series-resampling-with-pandas#resampling-con-multiples-metodos-de-agregación","content":"Tambien podemos hacer un resampling con multiples métodos de agregación. Para hacer esto vamos a usar el método agg() y especificar los métodos de agregación que queremos usar. df_weekly = df.resample('W').agg(['mean', 'std', 'min', 'max']) df_weekly.plot() plt.show()   Eso es todo por ahora, espero que este articulo te haya sido de ayuda "},{"title":"Vectorize de numpy vs apply de pandas","type":0,"sectionRef":"#","url":"/article/vectorize-numpy-vs-apply-pandas","content":"Tanto numpy como pandas tienen funciones que permiten aplicar una funcion a un array o dataframe, respectivamente, de forma vectorizada. Esto significa que la funcion se aplica a todos los elementos del array o dataframe, sin necesidad de iterar sobre ellos. Esto es mucho mas eficiente que iterar sobre los elementos, ya que no se necesita hacer un loop en python, sino que la funcion se aplica en C. import numpy as np import pandas as pd # comparación de vectorize de numpy vs apply de pandas # vectorize de numpy def f(x): return x**2 + 1 array = np.arange(100000, dtype=np.int16) %timeit np.vectorize(f)(array) # pandas apply df = pd.DataFrame({'x': array}) %timeit df['x'].apply(f) 24.2 ms ± 1.56 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 40.7 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Esta es una comparación muy simple entre ambas formas de aplicar una funcion, pero nos da una idea bastante clara de la diferencia de performance entre ambas, como podemos ver vectorize fue mucho mas rapido que apply.","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Teorema de limite central","type":0,"sectionRef":"#","url":"/article/teorema-de-limite-central","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"En python​","type":1,"pageTitle":"Teorema de limite central","url":"/article/teorema-de-limite-central#en-python","content":"Para graficar el TLC en python, usaremos un ejemplo de tirar dados. import numpy as np import matplotlib.pyplot as plt # Tiraremos 10 veces los dados y calcularemos la media dados = list(range(1,7)) muestra_10 = np.random.choice(dados, size=10, replace=True) media = np.mean(muestra_10) print(&quot;La media de la muestra es: &quot;, media)  La media de la muestra es: 3.0  Como podemos ver la Media de esta muestra no es 3.5, hora veamos que pasa si hacemos este mismo experimento pero 10 veces. exp_10 = [np.mean(muestra) for muestra in np.random.choice(dados, size=(10, 10), replace=True)] # Graficamos el histograma de las medias plt.hist(exp_10, bins=10, density=True, alpha=0.5) plt.vlines(3.5, 0, 1, color='red', label='Media teórica') plt.vlines(np.mean(exp_10), 0, 1, color='green', label='Media muestral') plt.show()   Ahora veamos que pasa si hacemos este mismo experimento pero 1000 veces. exp_1000 = [np.mean(muestra) for muestra in np.random.choice(dados, size=(1000, 10), replace=True)] # Graficamos plt.hist(exp_1000, bins=10, density=True, alpha=0.5) plt.vlines(3.5, 0, 1, color='red', label='Media teórica') plt.vlines(np.mean(exp_1000), 0, 1, color='green', label='Media muestral') plt.show()   Como podemos ver, a medida que aumentamos el número de experimentos, la distribución de las medias muestrales se aproxima a una distribución normal. caution Si ejecutas este código en tu computadora, es posible que no obtengas los mismos resultados que yo, ya que los números aleatorios son generados de forma aleatoria. "},{"title":"Importar datos de diferentes fuentes con Python","type":0,"sectionRef":"#","url":"/cheat-sheets/importing-data-python","content":"Flat files CSV​ import pandas as pd # puede ser un file o una url pd.read_csv('file.csv') pd.read_csv('file.txt', sep='\\t') Excel import pandas as pd # puede ser un file o una url # en caso de que sheetname, no este asignado, se lee la primera hoja xls = pd.ExcelFile('file.xlsx', sheetname=None) # Leer todas las hojas xls.keys() # Nombre de las hojas xls['sheet'] # Leer hoja SAS from sas7bdat import SAS7BDAT with SAS7BDAT('file.sas7bdat') as file: df_sas = file.to_data_frame() Stata import pandas as pd # puede ser un file o una url df = pd.read_stata('file.dta') HDF5 Los archivos HDF5 son una buena opción para guardar grandes cantidades de datos. Se pueden leer con la librería h5py import h5py data = h5py.File('file.hdf5', 'r') data.keys() # Nombre de los grupos group = data['group'] # Leer grupo group.keys() # Nombre de los datasets dataset = group['dataset'] # Leer dataset dataset.shape # Dimensiones dataset.value # Valores Matlab import scipy.io mat = scipy.io.loadmat('file.mat') Pickled files Los archivos pickled son archivos binarios de Python. Se pueden leer con la librería pickle import pickle with open('file.pkl', 'rb') as file: data = pickle.load(file) SQL from sqlalchemy import create_engine engine = create_engine('sqlite:///file.sqlite') table_names = engine.table_names() # Nombre de las tablas with engine.connect() as con: rs = con.execute('SELECT * FROM table') df = pd.DataFrame(rs.fetchall()) df.columns = rs.keys() Con Pandas​ import pandas as pd from sqlalchemy import create_engine engine = create_engine('sqlite:///file.sqlite') df = pd.read_sql_query('SELECT * FROM table', engine) ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Cheat Sheets","type":0,"sectionRef":"#","url":"/cheat-sheets/index","content":"Probando el componente CheatSheet con diferentes lenguajes de programación y latex. Titulo Contenido de la cheat sheet import CheatSheet from '@site/src/components/CheatSheet'; &lt;CheatSheet header='Titulo'&gt; Contenido de la cheat sheet &lt;/CheatSheet&gt; Contenido de latext Formula en latex∫abx2dx\\int_{a}^{b} x^2 dx∫ab​x2dx $$\\int_{a}^{b} x^2 dx$$ Multiples lenguajes Contenido de la cheat sheet def foo(): print('Hello world!') foo &lt;- function() { print('Hello world!') } Fomulas y codigo Formula en latex∫abx2dx\\int_{a}^{b} x^2 dx∫ab​x2dx def foo(): print('Hello world!') Imagenes Sin titulo &lt;CheatSheet&gt; Sin titulo &lt;/CheatSheet&gt; Header Texto en latex \\int_{a}^{b} x^2 dx ∫abx2dx\\int_{a}^{b} x^2 dx∫ab​x2dx Header Código python def foo(): print('Hello world!') ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Metricas para evaluar un modelo en machine learning","type":0,"sectionRef":"#","url":"/cheat-sheets/metrics-to-evaluate-a-machine-learning-model","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Classification Metrics​","type":1,"pageTitle":"Metricas para evaluar un modelo en machine learning","url":"/cheat-sheets/metrics-to-evaluate-a-machine-learning-model#classification-metrics","content":"Confusion Matrix Predicted Actual class\tPositive\tNegativePositive\tTrue Positive (TP)\tFalse Negative (FN) Negative\tFalse Positive (FP)\tTrue Negative (TN) from sklearn.metrics import confusion_matrix confusion_matrix(y_true, y_pred)  Accuaracy Usar Accuaracy cuando quieres medir la performance de un modelo de clasificacion. Es la proporcion de predicciones correctas sobre el total de predicciones realizadas. Accuracy = TP+TNTP+TN+FP+FN\\frac{TP + TN}{TP + TN + FP + FN}TP+TN+FP+FNTP+TN​ from sklearn.metrics import accuracy_score accuracy_score(y_true, y_pred)  Precision Usar Precision cuanto quieres minimizar los falsos positivos (Errores de tipo I). Es la proporcion de predicciones correctas sobre el total de predicciones realizadas. Precision = TPTP+FP\\frac{TP}{TP + FP}TP+FPTP​ from sklearn.metrics import precision_score precision_score(y_true, y_pred)  Recall Usar Recall cuando quieres minimizar los falsos negativos (Errores de tipo II). Es la proporcion de predicciones correctas sobre el total de predicciones realizadas. Recall = TPTP+FN\\frac{TP}{TP + FN}TP+FNTP​ from sklearn.metrics import recall_score recall_score(y_true, y_pred)  F1 Score Usar F1 Score cuando quieres minimizar los falsos negativos y falsos positivos. Es la media armonica entre Precision y Recall. F1 Score = 2∗Precision∗RecallPrecision+Recall\\frac{2 * Precision * Recall}{Precision + Recall}Precision+Recall2∗Precision∗Recall​ from sklearn.metrics import f1_score f1_score(y_true, y_pred)  ROC Curve Usar ROC Curve cuando quieres evaluar el rendimiento de un modelo de clasificacion binaria. Es una grafica de la tasa de verdaderos positivos (TPR) frente a la tasa de falsos positivos (FPR) para diferentes umbrales de probabilidad de clasificacion. TPR = TPTP+FN\\frac{TP}{TP + FN}TP+FNTP​FPR = FPFP+TN\\frac{FP}{FP + TN}FP+TNFP​ from sklearn.metrics import roc_curve, roc_auc_score import matplotlib.pyplot as plt fpr, tpr, thresholds = roc_curve(y_true, y_pred) auc = roc_auc_score(y_true, y_pred) plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc) plt.plot([0, 1], [0, 1], 'k--') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC Curve') plt.show()  Classification Report Usar Classification Report cuando quieres evaluar el rendimiento de un modelo de clasificacion. Es un resumen de las metricas de clasificacion para cada clase del problema. from sklearn.metrics import classification_report print(classification_report(y_true, y_pred))  "},{"title":"Regression Metrics​","type":1,"pageTitle":"Metricas para evaluar un modelo en machine learning","url":"/cheat-sheets/metrics-to-evaluate-a-machine-learning-model#regression-metrics","content":"Mean Absolute Error (MAE) Usar MAE cuando quieres medir el error medio de un modelo de regresion. Es la media de la diferencia absoluta entre las predicciones y los valores reales. MAE = 1n∑i=1n∣yi−y^i∣\\frac{1}{n} \\sum_{i=1}^{n} |y_{i} - \\hat{y}_{i}|n1​∑i=1n​∣yi​−y^​i​∣ from sklearn.metrics import mean_absolute_error mean_absolute_error(y_true, y_pred)  Mean Squared Error (MSE) Usar MSE cuando quieres penalizar los errores mas grandes. Es la mejor metrica cuando le preocupa las grandes desviaciones en los errores. MSE = 1n∑i=1n(yi−y^i)2\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}n1​∑i=1n​(yi​−y^​i​)2 from sklearn.metrics import mean_squared_error mean_squared_error(y_true, y_pred)  Root Mean Squared Error (RMSE) Usar RMSE cuando quieres penalizar los errores mas grandes. Es la mejor metrica cuando desea una medida que sea menos sensible a los valores atipicos. RMSE = 1n∑i=1n(yi−y^i)2\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}}n1​∑i=1n​(yi​−y^​i​)2​ from sklearn.metrics import mean_squared_error mean_squared_error(y_true, y_pred, squared=False) # or import numpy as np np.sqrt(mean_squared_error(y_true, y_pred))  R-Squared (R2) Usar R2 cuando quieres medir la varianza de los errores. Es la proporcion de la varianza de los errores y la varianza de los valores reales R2 = 1−∑i=1n(yi−y^i)2∑i=1n(yi−yˉi)21 - \\frac{\\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}}{\\sum_{i=1}^{n} (y_{i} - \\bar{y}_{i})^{2}}1−∑i=1n​(yi​−yˉ​i​)2∑i=1n​(yi​−y^​i​)2​ from sklearn.metrics import r2_score r2_score(y_true, y_pred)  "},{"title":"Preprocesamiento de datos para machine learning","type":0,"sectionRef":"#","url":"/cheat-sheets/preprocessing-data","content":"Missing data Es importante tener en cuenta que los modelos de machine learning no pueden trabajar con valores nulos, por lo que es necesario reemplazarlos por algún valor. Eliminar​ Si hay muchos valores nulos, se puede eliminar la columna o fila, tener en cuenta que se puede perder información importante. df.dropna() Imputar​ from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy='mean') imputer.fit_transform(X) crear columna indicadora from sklearn.impute import MissingIndicator indicator = MissingIndicator() indicator.fit_transform(X) Encoder data Dummy​ Variable\tDummy color\tcolor_rojo\tcolor_verde\tcolor_azulrojo\t1\t0\t0 verde\t0\t1\t0 azul\t0\t0\t1 from sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder() encoder.fit_transform(X) import pandas as pd pd.get_dummies(X) Label​ Variable\tLabelrojo\t0 verde\t1 azul\t2```python from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() encoder.fit_transform(X) ``` import pandas as pd df = pd.DataFrame({'color': ['rojo', 'verde', 'azul']}) df['color'].astype('category').cat.codes Scaling and Centering Data StandardScaler​ x−μσ\\frac{x - \\mu}{\\sigma}σx−μ​ from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit_transform(X) MinMaxScaler​ x−minmax−min\\frac{x - min}{max - min}max−minx−min​ from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaler.fit_transform(X) RobustScaler​ x−Q1Q3−Q1\\frac{x - Q_1}{Q_3 - Q_1}Q3​−Q1​x−Q1​​ from sklearn.preprocessing import RobustScaler scaler = RobustScaler() scaler.fit_transform(X) Normalizer​ L1: x∑i=1n∣xi∣\\frac{x}{\\sum_{i=1}^n |x_i|}∑i=1n​∣xi​∣x​L2: x∑i=1nxi2\\frac{x}{\\sqrt{\\sum_{i=1}^n x_i^2}}∑i=1n​xi2​​x​max: xmax(x)\\frac{x}{max(x)}max(x)x​ from sklearn.preprocessing import Normalizer # L1, L2, max scaler = Normalizer(norm='l2') scaler.fit_transform(X) Feature engineering PolynomialFeatures​ x1,x2→x12,x1x2,x22x_1, x_2 \\rightarrow x_1^2, x_1x_2, x_2^2x1​,x2​→x12​,x1​x2​,x22​ from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) poly.fit_transform(X) Binning​ Este proceso se utiliza para discretizar variables continuas, es decir, convertir variables continuas en variables categóricas, agrupando los valores en intervalos. x→{0,1,2,...,n}x \\rightarrow \\{0, 1, 2,..., n\\}x→{0,1,2,...,n} from sklearn.preprocessing import KBinsDiscretizer discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform') discretizer.fit_transform(X) Feature selection VarianceThreshold​ from sklearn.feature_selection import VarianceThreshold selector = VarianceThreshold(threshold=0.1) selector.fit_transform(X) SelectKBest​ from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 selector = SelectKBest(chi2, k=2) selector.fit_transform(X, y) SelectFromModel​ from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression selector = SelectFromModel(estimator=LogisticRegression()) selector.fit_transform(X, y) RFE​ from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression selector = RFE(estimator=LogisticRegression(), n_features_to_select=2) selector.fit_transform(X, y) ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Cheat Sheets de Pandas reshaping","type":0,"sectionRef":"#","url":"/cheat-sheets/python-pandas-reshape","content":"pivot df.pivot(index='foo', columns='bar', values='baz') df.pivot_table(index='foo', columns='bar', values='baz', aggfunc='sum') melt df3.melt(id_vars=['first', 'last'], var_name='variable', value_name='value') df3.melt(id_vars=['first', 'last'], var_name='variable', value_name='value', value_vars=['height', 'weight']) Wide to long pd.wide_to_long(df, stubnames=['age', 'weight'], i=['name'], j='year') # format age_2019 pd.wide_to_long(df, stubnames=['age', 'weight'], i=['name'], j='year', sep='_', suffix='\\w+') ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Muestreo con python","type":0,"sectionRef":"#","url":"/cheat-sheets/sampling-python","content":"import numpy as np import pandas as pd from scipy import stats Muestreo aleatorio np.random.random(10) np.random.randint(0, 100, 10) colors = ['red', 'blue', 'green'] np.random.choice(colors, 10) Con reemplazo​ np.random.choice(colors, 10, replace=True) Distribucion normal parametros: μ\\muμ media, σ\\sigmaσ desviacion estandar y nnn tamaño de la muestra np.random.normal(0, 1, 10) stats.norm.rvs(0, 1, 10) Distribucion uniforme parametros: aaa minimo, bbb maximo y nnn tamaño de la muestra np.random.uniform(0, 1, 10) stats.uniform.rvs(0, 1, 10) Distribucion binomial parametros: nnn numero de ensayos, ppp probabilidad de exito y nnn tamaño de la muestra np.random.binomial(10, 0.5, 10) stats.binom.rvs(10, 0.5, 10) Distribucion poisson parametros: λ\\lambdaλ tasa de ocurrencia y nnn tamaño de la muestra np.random.poisson(10, 10) stats.poisson.rvs(10, 10) Distribucion exponencial parametros: λ\\lambdaλ tasa de ocurrencia y nnn tamaño de la muestra np.random.exponential(10, 10) stats.expon.rvs(10, 10) Muestreo estratificado df = pd.DataFrame({ 'sexo': np.random.choice(['M', 'F'], 100), 'edad': np.random.randint(18, 65, 100), 'estado_civil': np.random.choice(['S', 'C', 'D', 'V'], 100), 'ingreso': np.random.randint(1000, 10000, 100) }) df.groupby('sexo').apply(lambda x: x.sample(10)) Muestreo sistematico df = pd.DataFrame({ 'sexo': np.random.choice(['M', 'F'], 100), 'edad': np.random.randint(18, 65, 100), 'estado_civil': np.random.choice(['S', 'C', 'D', 'V'], 100), 'ingreso': np.random.randint(1000, 10000, 100) }) df.iloc[::10] # selecciona cada 10 filas ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Linear models in scikit-learn","type":0,"sectionRef":"#","url":"/cheat-sheets/sklearn-linear-model","content":"Linear Model The following linear models are available in scikit-learn for regression and classification tasks, if yyy is the target variable, xxx is the feature vector, and www is the weight vector y=w0+w1x1+w2x2+...+wpxpy = w_0 + w_1x_1 + w_2x_2 + ... + w_px_py=w0​+w1​x1​+w2​x2​+...+wp​xp​ w0w_0w0​ is the intercept_w1,w2,...,wpw_1, w_2, ..., w_pw1​,w2​,...,wp​ are the coef_ Linear Regression Fits a linear model with coefficients w=(w1,…,wp)w = (w1, …, wp)w=(w1,…,wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. min⁡w∣∣Xw−y∣∣22\\min_{w} || X w - y||_2^2minw​∣∣Xw−y∣∣22​ from sklearn.linear_model import LinearRegression lr = LinearRegression() Ridge Regression Applies L2 regularization to reduce the complexity of the model and prevent overfitting. min⁡w∣∣Xw−y∣∣22+α∣∣w∣∣22\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2minw​∣∣Xw−y∣∣22​+α∣∣w∣∣22​Hyperparameter α\\alphaα if α=0\\alpha = 0α=0, then the model is the same as Linear Regression from sklearn.linear_model import Ridge ridge = Ridge(alpha=1.0) from sklearn.linear_model import RidgeCV Lasso Regression Applies L1 regularization to reduce the complexity of the model and prevent overfitting. min⁡w∣∣Xw−y∣∣22+α∣∣w∣∣1\\min_{w} || X w - y||_2^2 + \\alpha ||w||_1minw​∣∣Xw−y∣∣22​+α∣∣w∣∣1​Hyperparameter α\\alphaα if α=0\\alpha = 0α=0, then the model is the same as Linear Regression from sklearn.linear_model import Lasso lasso = Lasso(alpha=1.0) Elastic Net Regression Applies both L1 and L2 regularization to reduce the complexity of the model and prevent overfitting. min⁡w∣∣Xw−y∣∣22+αρ∣∣w∣∣1+α(1−ρ)2∣∣w∣∣22\\min_{w} || X w - y||_2^2 + \\alpha \\rho ||w||_1 + \\frac{\\alpha(1-\\rho)}{2} ||w||_2^2minw​∣∣Xw−y∣∣22​+αρ∣∣w∣∣1​+2α(1−ρ)​∣∣w∣∣22​Hyperparameter α\\alphaα and l1_ratiol1\\_ratiol1_ratio if α=0\\alpha = 0α=0, and l1_ratio=0l1\\_ratio = 0l1_ratio=0, then the model is the same as Linear Regression from sklearn.linear_model import ElasticNet elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5) Polynomial Regression Generates polynomial features and fits a linear model to the transformed data. y=w0+w1x1+w2x2+w3x12+w4x1x2+w5x22+...y = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_1x_2 + w_5x_2^2 + ...y=w0​+w1​x1​+w2​x2​+w3​x12​+w4​x1​x2​+w5​x22​+...Hyperparameter degree from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline poly = PolynomialFeatures(degree=2) poly_reg = make_pipeline(poly, LinearRegression()) Logistic Regression Use when you want to predict a binary outcome (0 or 1, yes or no, true or false) given a set of independent variables. y=11+e−(w0+w1x1+w2x2+...+wpxp)y = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + ... + w_px_p)}}y=1+e−(w0​+w1​x1​+w2​x2​+...+wp​xp​)1​ from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression() Stocastic Gradient Descent Use when you want to train large datasets. wt+1=wt−η∇Qi(wt)w_{t+1} = w_t - \\eta \\nabla Q_i(w_t)wt+1​=wt​−η∇Qi​(wt​)Hyperparameter eta0 is the learning rate from sklearn.linear_model import SGDClassifier, SGDRegressor sgd_clf = SGDClassifier() sgd_reg = SGDRegressor() Bayesian Ridge Regression Bayesian Ridge Regression is similar to Ridge Regression, but it introduces a prior on the weights www. Original Algorithm is detailed in the book Bayesian learning for neural networksHyperparameter alpha_1, alpha_2, lambda_1, lambda_2 from sklearn.linear_model import BayesianRidge bayesian_ridge = BayesianRidge() Passive Aggressive Passive Aggressive algorithms are a family of algorithms for large-scale learning from sklearn.linear_model import PassiveAggressiveClassifier, PassiveAggressiveRegressor passive_aggressive_clf = PassiveAggressiveClassifier() passive_aggressive_reg = PassiveAggressiveRegressor() RANSAC Regression RANSAC (RANdom SAmple Consensus) is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. from sklearn.linear_model import RANSACRegressor ransac_reg = RANSACRegressor() ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"crear columna indicadora","type":0,"sectionRef":"#","url":"/cheat-sheets/tags/machine-learning","content":"Missing data Es importante tener en cuenta que los modelos de machine learning no pueden trabajar con valores nulos, por lo que es necesario reemplazarlos por algún valor. Eliminar​ Si hay muchos valores nulos, se puede eliminar la columna o fila, tener en cuenta que se puede perder información importante. df.dropna() Imputar​ from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy='mean') imputer.fit_transform(X) crear columna indicadora from sklearn.impute import MissingIndicator indicator = MissingIndicator() indicator.fit_transform(X) Encoder data Dummy​ Variable\tDummy color\tcolor_rojo\tcolor_verde\tcolor_azulrojo\t1\t0\t0 verde\t0\t1\t0 azul\t0\t0\t1 from sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder() encoder.fit_transform(X) import pandas as pd pd.get_dummies(X) Label​ Variable\tLabelrojo\t0 verde\t1 azul\t2```python from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() encoder.fit_transform(X) ``` import pandas as pd df = pd.DataFrame({'color': ['rojo', 'verde', 'azul']}) df['color'].astype('category').cat.codes Scaling and Centering Data StandardScaler​ x−μσ\\frac{x - \\mu}{\\sigma}σx−μ​ from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit_transform(X) MinMaxScaler​ x−minmax−min\\frac{x - min}{max - min}max−minx−min​ from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaler.fit_transform(X) RobustScaler​ x−Q1Q3−Q1\\frac{x - Q_1}{Q_3 - Q_1}Q3​−Q1​x−Q1​​ from sklearn.preprocessing import RobustScaler scaler = RobustScaler() scaler.fit_transform(X) Normalizer​ L1: x∑i=1n∣xi∣\\frac{x}{\\sum_{i=1}^n |x_i|}∑i=1n​∣xi​∣x​L2: x∑i=1nxi2\\frac{x}{\\sqrt{\\sum_{i=1}^n x_i^2}}∑i=1n​xi2​​x​max: xmax(x)\\frac{x}{max(x)}max(x)x​ from sklearn.preprocessing import Normalizer # L1, L2, max scaler = Normalizer(norm='l2') scaler.fit_transform(X) Feature engineering PolynomialFeatures​ x1,x2→x12,x1x2,x22x_1, x_2 \\rightarrow x_1^2, x_1x_2, x_2^2x1​,x2​→x12​,x1​x2​,x22​ from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) poly.fit_transform(X) Binning​ Este proceso se utiliza para discretizar variables continuas, es decir, convertir variables continuas en variables categóricas, agrupando los valores en intervalos. x→{0,1,2,...,n}x \\rightarrow \\{0, 1, 2,..., n\\}x→{0,1,2,...,n} from sklearn.preprocessing import KBinsDiscretizer discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform') discretizer.fit_transform(X) Feature selection VarianceThreshold​ from sklearn.feature_selection import VarianceThreshold selector = VarianceThreshold(threshold=0.1) selector.fit_transform(X) SelectKBest​ from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 selector = SelectKBest(chi2, k=2) selector.fit_transform(X, y) SelectFromModel​ from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression selector = SelectFromModel(estimator=LogisticRegression()) selector.fit_transform(X, y) RFE​ from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression selector = RFE(estimator=LogisticRegression(), n_features_to_select=2) selector.fit_transform(X, y) ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"crear columna indicadora","type":0,"sectionRef":"#","url":"/cheat-sheets/tags/preprocessing","content":"Missing data Es importante tener en cuenta que los modelos de machine learning no pueden trabajar con valores nulos, por lo que es necesario reemplazarlos por algún valor. Eliminar​ Si hay muchos valores nulos, se puede eliminar la columna o fila, tener en cuenta que se puede perder información importante. df.dropna() Imputar​ from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy='mean') imputer.fit_transform(X) crear columna indicadora from sklearn.impute import MissingIndicator indicator = MissingIndicator() indicator.fit_transform(X) Encoder data Dummy​ Variable\tDummy color\tcolor_rojo\tcolor_verde\tcolor_azulrojo\t1\t0\t0 verde\t0\t1\t0 azul\t0\t0\t1 from sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder() encoder.fit_transform(X) import pandas as pd pd.get_dummies(X) Label​ Variable\tLabelrojo\t0 verde\t1 azul\t2```python from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() encoder.fit_transform(X) ``` import pandas as pd df = pd.DataFrame({'color': ['rojo', 'verde', 'azul']}) df['color'].astype('category').cat.codes Scaling and Centering Data StandardScaler​ x−μσ\\frac{x - \\mu}{\\sigma}σx−μ​ from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit_transform(X) MinMaxScaler​ x−minmax−min\\frac{x - min}{max - min}max−minx−min​ from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaler.fit_transform(X) RobustScaler​ x−Q1Q3−Q1\\frac{x - Q_1}{Q_3 - Q_1}Q3​−Q1​x−Q1​​ from sklearn.preprocessing import RobustScaler scaler = RobustScaler() scaler.fit_transform(X) Normalizer​ L1: x∑i=1n∣xi∣\\frac{x}{\\sum_{i=1}^n |x_i|}∑i=1n​∣xi​∣x​L2: x∑i=1nxi2\\frac{x}{\\sqrt{\\sum_{i=1}^n x_i^2}}∑i=1n​xi2​​x​max: xmax(x)\\frac{x}{max(x)}max(x)x​ from sklearn.preprocessing import Normalizer # L1, L2, max scaler = Normalizer(norm='l2') scaler.fit_transform(X) Feature engineering PolynomialFeatures​ x1,x2→x12,x1x2,x22x_1, x_2 \\rightarrow x_1^2, x_1x_2, x_2^2x1​,x2​→x12​,x1​x2​,x22​ from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) poly.fit_transform(X) Binning​ Este proceso se utiliza para discretizar variables continuas, es decir, convertir variables continuas en variables categóricas, agrupando los valores en intervalos. x→{0,1,2,...,n}x \\rightarrow \\{0, 1, 2,..., n\\}x→{0,1,2,...,n} from sklearn.preprocessing import KBinsDiscretizer discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform') discretizer.fit_transform(X) Feature selection VarianceThreshold​ from sklearn.feature_selection import VarianceThreshold selector = VarianceThreshold(threshold=0.1) selector.fit_transform(X) SelectKBest​ from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 selector = SelectKBest(chi2, k=2) selector.fit_transform(X, y) SelectFromModel​ from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression selector = SelectFromModel(estimator=LogisticRegression()) selector.fit_transform(X, y) RFE​ from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression selector = RFE(estimator=LogisticRegression(), n_features_to_select=2) selector.fit_transform(X, y) ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"index","type":0,"sectionRef":"#","url":"/tutorial","content":"index","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"}]