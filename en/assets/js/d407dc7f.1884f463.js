"use strict";(self.webpackChunkdocsweb=self.webpackChunkdocsweb||[]).push([[6287],{6628:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/python-pandas-reshape","metadata":{"permalink":"/en/cheat-sheets/python-pandas-reshape","source":"@site/cheat-sheets/python-pandas-reshape.md","title":"Cheat Sheets de Pandas reshaping","description":"Cheat Sheets de Pandas reshaping","date":"2023-07-03T21:59:15.000Z","formattedDate":"July 3, 2023","tags":[{"label":"pandas","permalink":"/en/cheat-sheets/tags/pandas"},{"label":"python","permalink":"/en/cheat-sheets/tags/python"}],"readingTime":0.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Cheat Sheets de Pandas reshaping","description":"Cheat Sheets de Pandas reshaping","tags":["pandas","python"]},"nextItem":{"title":"Cheat Sheets","permalink":"/en/cheat-sheets/index"}},"content":"import CheatSheet from \'@site/src/components/CheatSheet\';\\n\\n\\n<CheatSheet header=\\"pivot\\">\\n\\n![Alt text](images/pd-pivot.png)\\n\\n```python\\ndf.pivot(index=\'foo\', columns=\'bar\', values=\'baz\')\\n```\\n\\n```python\\ndf.pivot_table(index=\'foo\', columns=\'bar\', values=\'baz\', aggfunc=\'sum\')\\n```\\n</CheatSheet>\\n\\n<CheatSheet header=\\"melt\\">\\n\\n![Alt text](images/pd-melt.png)\\n\\n```python\\ndf3.melt(id_vars=[\'first\', \'last\'], var_name=\'variable\', \\n        value_name=\'value\')\\ndf3.melt(id_vars=[\'first\', \'last\'], var_name=\'variable\', \\n        value_name=\'value\', value_vars=[\'height\', \'weight\'])\\n```\\n</CheatSheet>\\n\\n<CheatSheet header=\\"Wide to long\\">\\n\\n![Alt text](images/pd-wide-to-long.png)\\n\\n```python\\npd.wide_to_long(df, stubnames=[\'age\', \'weight\'], i=[\'name\'], \\n                j=\'year\')\\n# format age_2019\\npd.wide_to_long(df, stubnames=[\'age\', \'weight\'], i=[\'name\'], \\n                j=\'year\', sep=\'_\', suffix=\'\\\\w+\')\\n```\\n</CheatSheet>"},{"id":"/index","metadata":{"permalink":"/en/cheat-sheets/index","source":"@site/cheat-sheets/index.mdx","title":"Cheat Sheets","description":"Probando el componente CheatSheet con diferentes lenguajes de programaci\xf3n y latex.","date":"2023-07-03T21:59:15.000Z","formattedDate":"July 3, 2023","tags":[],"readingTime":0.67,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"Cheat Sheets de Pandas reshaping","permalink":"/en/cheat-sheets/python-pandas-reshape"},"nextItem":{"title":"Linear models in scikit-learn","permalink":"/en/cheat-sheets/sklearn-linear-model"}},"content":"import CheatSheet from \'@site/src/components/CheatSheet\';\\n\\n\\nProbando el componente CheatSheet con diferentes lenguajes de programaci\xf3n y latex.\\n\\n<CheatSheet header=\'Titulo\'>\\n\\nContenido de la cheat sheet\\n\\n```tsx\\nimport CheatSheet from \'@site/src/components/CheatSheet\';\\n<CheatSheet header=\'Titulo\'>\\n    Contenido de la cheat sheet\\n</CheatSheet>\\n```\\n</CheatSheet>\\n\\n\\n<CheatSheet header=\'Contenido de latext\'>\\n\\nFormula en latex\\n$$\\\\int_{a}^{b} x^2 dx$$\\n\\n```latex\\n$$\\\\int_{a}^{b} x^2 dx$$\\n```\\n</CheatSheet>\\n\\n\\n<CheatSheet header=\'Multiples lenguajes\'>\\n\\nContenido de la cheat sheet\\n\\n```python\\ndef foo():\\n    print(\'Hello world!\')\\n```\\n\\n```r\\nfoo <- function() {\\n    print(\'Hello world!\')\\n}\\n```\\n</CheatSheet>\\n\\n<CheatSheet header=\'Fomulas y codigo\'>\\n\\nFormula en latex\\n$$\\\\int_{a}^{b} x^2 dx$$\\n\\n```python\\ndef foo():\\n    print(\'Hello world!\')\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\'Imagenes\'>\\n\\n![Alt text](images/image.png)\\n\\n</CheatSheet>\\n\\n<CheatSheet>\\n\\nSin titulo \\n\\n```tsx\\n<CheatSheet>\\n    Sin titulo\\n</CheatSheet>\\n```\\n</CheatSheet>\\n\\n<CheatSheet header={\'Header\'}>\\n\\nTexto en latex\\n```latex\\n\\\\int_{a}^{b} x^2 dx\\n```\\n</CheatSheet>\\n\\n$$\\\\int_{a}^{b} x^2 dx$$\\n\\n<CheatSheet header={\'Header\'}>\\n\\nC\xf3digo python\\n\\n```python\\ndef foo():\\n    print(\'Hello world!\')\\n```\\n</CheatSheet>"},{"id":"/sklearn-linear-model","metadata":{"permalink":"/en/cheat-sheets/sklearn-linear-model","source":"@site/cheat-sheets/sklearn-linear-model.mdx","title":"Linear models in scikit-learn","description":"cheat sheet for linear models in scikit-learn","date":"2023-07-03T21:59:15.000Z","formattedDate":"July 3, 2023","tags":[{"label":"scikit-learn","permalink":"/en/cheat-sheets/tags/scikit-learn"},{"label":"linear-models","permalink":"/en/cheat-sheets/tags/linear-models"}],"readingTime":1.035,"hasTruncateMarker":false,"authors":[{"name":"Darvin Cotrina","title":"Creador de entredata.org","url":"https://github.com/ccdarvin","imageURL":"https://github.com/ccdarvin.png","key":"ccdarvin"}],"frontMatter":{"title":"Linear models in scikit-learn","description":"cheat sheet for linear models in scikit-learn","authors":["ccdarvin"],"data":"2023-06-08T00:00:00.000Z","tags":["scikit-learn","linear-models"]},"prevItem":{"title":"Cheat Sheets","permalink":"/en/cheat-sheets/index"},"nextItem":{"title":"Metricas para evaluar un modelo en machine learning","permalink":"/en/cheat-sheets/metrics-to-evaluate-a-machine-learning-model"}},"content":"import CheatSheet from \'@site/src/components/CheatSheet\'\\n\\n\\n\\n<CheatSheet header=\\"Linear Regression\\">\\n\\n> Fits a linear model with coefficients $w = (w1, \u2026, wp)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\\n> - model equation: $y = b_0 + b_1x_1 + b_2x_2 + ... + b_px_p$\\n\\n```python\\nfrom sklearn.linear_model import LinearRegression\\nlr = LinearRegression()\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"Ridge Regression\\">\\n\\n> Applies L2 regularization to reduce the complexity of the model and prevent overfitting.\\n> - model equation: $y = b_0 + b_1x_1 + b_2x_2 + ... + b_px_p - \\\\alpha \\\\sum_{j=1}^{p} b_j^2$\\n\\n* if $\\\\alpha = 0$, then the model is the same as Linear Regression\\n* if $\\\\alpha = \\\\infty$, then all coefficients are zero, and the result is a constant mean\\n\\n```python\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=1.0)\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"Lasso Regression\\">\\n\\n> Applies L1 regularization to reduce the complexity of the model and prevent overfitting.\\n> - model equation: $y = b_0 + b_1x_1 + b_2x_2 + ... + b_px_p - \\\\alpha \\\\sum_{j=1}^{p} |b_j|$\\n> *In this case \\n\\n```python\\nfrom sklearn.linear_model import Lasso\\nlasso = Lasso(alpha=1.0)\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet>\\n</CheatSheet>\\n\\n<CheatSheet>\\n</CheatSheet>\\n\\n<CheatSheet>\\n</CheatSheet>\\n\\n<CheatSheet>\\n</CheatSheet>\\n\\n<CheatSheet>\\n</CheatSheet>\\n\\n<CheatSheet>\\n</CheatSheet>\\n\\n<CheatSheet>\\n</CheatSheet>"},{"id":"/metrics-to-evaluate-a-machine-learning-model","metadata":{"permalink":"/en/cheat-sheets/metrics-to-evaluate-a-machine-learning-model","source":"@site/cheat-sheets/metrics-to-evaluate-a-machine-learning-model.mdx","title":"Metricas para evaluar un modelo en machine learning","description":"Cheat Sheet de Metricas para evaluar un modelo en machine learning","date":"2023-07-03T00:00:00.000Z","formattedDate":"July 3, 2023","tags":[],"readingTime":2.835,"hasTruncateMarker":false,"authors":[{"name":"Darvin Cotrina","title":"Creador de entredata.org","url":"https://github.com/ccdarvin","imageURL":"https://github.com/ccdarvin.png","key":"ccdarvin"}],"frontMatter":{"title":"Metricas para evaluar un modelo en machine learning","description":"Cheat Sheet de Metricas para evaluar un modelo en machine learning","authors":"ccdarvin","date":"2023-07-03T00:00:00.000Z"},"prevItem":{"title":"Linear models in scikit-learn","permalink":"/en/cheat-sheets/sklearn-linear-model"}},"content":"import CheatSheet from \'@site/src/components/CheatSheet\';\\n\\n\\n\\n## Classification Metrics\\n\\n<CheatSheet header=\\"Confusion Matrix\\">\\n<table width=\\"100%\\">\\n    <thead>\\n        <tr>\\n            <th></th>\\n            <th colspan=\\"2\\">Predicted</th>\\n        </tr>\\n        <tr>\\n            <th>Actual class</th>\\n            <th>Positive</th>\\n            <th>Negative</th>\\n        </tr>\\n    </thead>\\n    <tbody>\\n        <tr>\\n            <th>Positive</th>\\n            <td>True Positive (TP)</td>\\n            <td>False Negative (FN)</td>\\n        </tr>\\n        <tr>\\n            <th>Negative</th>\\n            <td>False Positive (FP)</td>\\n            <td>True Negative (TN)</td>\\n        </tr>\\n    </tbody>\\n</table>\\n\\n```python\\t\\nfrom sklearn.metrics import confusion_matrix\\nconfusion_matrix(y_true, y_pred)\\n```\\n</CheatSheet>\\n\\n<CheatSheet header=\\"Accuaracy\\">\\n\\nUsar Accuaracy cuando quieres medir la performance de un modelo de clasificacion. Es la proporcion de predicciones correctas sobre el total de predicciones realizadas.\\n\\n* Accuracy = $\\\\frac{TP + TN}{TP + TN + FP + FN}$\\n\\n```python\\t\\nfrom sklearn.metrics import accuracy_score\\naccuracy_score(y_true, y_pred)\\n```\\n\\n</CheatSheet>\\n<CheatSheet header=\\"Precision\\">\\n\\nUsar Precision cuanto quieres minimizar los falsos positivos (Errores de tipo I). Es la proporcion de predicciones correctas sobre el total de predicciones realizadas.\\n* Precision = $\\\\frac{TP}{TP + FP}$\\n\\n```python\\nfrom sklearn.metrics import precision_score\\nprecision_score(y_true, y_pred)\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"Recall\\">\\n\\nUsar Recall cuando quieres minimizar los falsos negativos (Errores de tipo II). Es la proporcion de predicciones correctas sobre el total de predicciones realizadas.\\n\\n* Recall = $\\\\frac{TP}{TP + FN}$\\n\\n```python\\nfrom sklearn.metrics import recall_score\\nrecall_score(y_true, y_pred)\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"F1 Score\\">\\n\\nUsar F1 Score cuando quieres minimizar los falsos negativos y falsos positivos. Es la media armonica entre Precision y Recall.\\n\\n* F1 Score = $\\\\frac{2 * Precision * Recall}{Precision + Recall}$\\n\\n```python\\nfrom sklearn.metrics import f1_score\\nf1_score(y_true, y_pred)\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"ROC Curve\\">\\n\\nUsar ROC Curve cuando quieres evaluar el rendimiento de un modelo de clasificacion binaria. Es una grafica de la tasa de verdaderos positivos (TPR) frente a la tasa de falsos positivos (FPR) para diferentes umbrales de probabilidad de clasificacion.\\n\\n* TPR = $\\\\frac{TP}{TP + FN}$\\n* FPR = $\\\\frac{FP}{FP + TN}$\\n\\n```python\\nfrom sklearn.metrics import roc_curve, roc_auc_score\\nimport matplotlib.pyplot as plt\\n\\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\\nauc = roc_auc_score(y_true, y_pred)\\n\\nplt.plot(fpr, tpr, label=\'ROC curve (area = %0.2f)\' % auc)\\nplt.plot([0, 1], [0, 1], \'k--\')\\nplt.xlabel(\'False Positive Rate\')\\nplt.ylabel(\'True Positive Rate\')\\nplt.title(\'ROC Curve\')\\nplt.show()\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"Classification Report\\">\\n\\nUsar Classification Report cuando quieres evaluar el rendimiento de un modelo de clasificacion. Es un resumen de las metricas de clasificacion para cada clase del problema.\\n\\n```python\\nfrom sklearn.metrics import classification_report\\nprint(classification_report(y_true, y_pred))\\n```\\n\\n</CheatSheet>\\n\\n## Regression Metrics\\n\\n<CheatSheet header=\\"Mean Absolute Error (MAE)\\">\\n\\nUsar MAE cuando quieres medir el error medio de un modelo de regresion. Es la media de la diferencia absoluta entre las predicciones y los valores reales.\\n\\n* MAE = $\\\\frac{1}{n} \\\\sum_{i=1}^{n} |y_{i} - \\\\hat{y}_{i}|$\\n\\n```python\\nfrom sklearn.metrics import mean_absolute_error\\nmean_absolute_error(y_true, y_pred)\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"Mean Squared Error (MSE)\\">\\n\\nUsar MSE cuando quieres penalizar los errores mas grandes. Es la mejor metrica cuando le preocupa las grandes desviaciones en los errores.\\n\\n* MSE = $\\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_{i} - \\\\hat{y}_{i})^{2}$\\n\\n```python\\nfrom sklearn.metrics import mean_squared_error\\nmean_squared_error(y_true, y_pred)\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"Root Mean Squared Error (RMSE)\\">\\n\\nUsar RMSE cuando quieres penalizar los errores mas grandes. Es la mejor metrica cuando desea una medida que sea menos sensible a los valores atipicos.\\n\\n* RMSE = $\\\\sqrt{\\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_{i} - \\\\hat{y}_{i})^{2}}$\\n\\n```python\\nfrom sklearn.metrics import mean_squared_error\\nmean_squared_error(y_true, y_pred, squared=False)\\n# or\\nimport numpy as np\\nnp.sqrt(mean_squared_error(y_true, y_pred))\\n```\\n\\n</CheatSheet>\\n\\n<CheatSheet header=\\"R-Squared (R2)\\">\\nUsar R2 cuando quieres medir la varianza de los errores. Es la proporcion de la varianza de los errores y la varianza de los valores reales\\n\\n* R2 = $1 - \\\\frac{\\\\sum_{i=1}^{n} (y_{i} - \\\\hat{y}_{i})^{2}}{\\\\sum_{i=1}^{n} (y_{i} - \\\\bar{y}_{i})^{2}}$\\n\\n```python\\nfrom sklearn.metrics import r2_score\\nr2_score(y_true, y_pred)\\n```\\n\\n</CheatSheet>"}]}')}}]);