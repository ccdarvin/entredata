[{"title":"Arbol de Desición, conceptos básicos","type":0,"sectionRef":"#","url":"/en/article/arbol-decision-conceptos-basicos","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Modelo de arbol de desición​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#modelo-de-arbol-de-desición","content":"Un árbol de decisión es un modelo de predicción utilizado en el ámbito de la inteligencia artificial, que utiliza un árbol de estructura similar a los diagramas de flujo en donde cada nodo representa una característica (o atributo), cada rama representa una regla de decisión y cada hoja representa el resultado de una decisión. Los árboles de decisión son utilizados comúnmente en minería de datos con el fin de resolver problemas de clasificación. Ejemplo de un arbol de desición y su estructura:  "},{"title":"Entropía​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#entropía","content":""},{"title":"¿Qué es la entropía?​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#qué-es-la-entropía","content":"La entropía es una medida de incertidumbre. En el contexto de la toma de decisiones, la entropía mide la impureza de un conjunto de ejemplos S. Si S solo contiene ejemplos de una clase, entonces la entropía es 0. Si S contiene una cantidad uniforme de ejemplos de cada clase, entonces la entropía es 1. La entropía de un conjunto S se denota por H (S). "},{"title":"¿Cómo se calcula la entropía?​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#cómo-se-calcula-la-entropía","content":"La entropía de un conjunto S se calcula como: H(S)=−∑i=1cpilog2piH(S) = -\\sum_{i=1}^{c} p_i log_2 p_iH(S)=−∑i=1c​pi​log2​pi​ Donde: ccc es el número de clasespip_ipi​ es la proporción de ejemplos de clase iii en SSSlog2log_2log2​ es el logaritmo en base 2 "},{"title":"Ejemplo de cálculo de entropía​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#ejemplo-de-cálculo-de-entropía","content":"Supongamos que tenemos un conjunto de ejemplos SSS con 14 ejemplos de clase 1 y 6 ejemplos de clase 2. La entropía de SSS es: P1=14/20P_1 = 14/20P1​=14/20 yP2=6/20P_2 = 6/20P2​=6/20 Entonces, la entropía de SSS sería: H(S)=−(1420log⁡21420+620log⁡2620)≈0.88H(S) = - \\left(\\frac{14}{20} \\log_2 \\frac{14}{20} + \\frac{6}{20} \\log_2 \\frac{6}{20}\\right) \\approx 0.88H(S)=−(2014​log2​2014​+206​log2​206​)≈0.88. "},{"title":"Ganancia de información​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#ganancia-de-información","content":"La ganancia de información(IG) se utiliza para decidir qué atributo se utilizará para dividir el conjunto de datos en subconjuntos homogéneos. La ganancia de información se define como la diferencia entre la entropía antes de la división y la entropía después de la división por un atributo. La ganancia de información se denota por IG (S, A) y se calcula como: IG(S,A)=H(S)−∑v∈Values(A)∣Sv∣∣S∣H(Sv)IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)IG(S,A)=H(S)−∑v∈Values(A)​∣S∣∣Sv​∣​H(Sv​) Donde: SSS es el conjunto de ejemplosAAA es el atributo utilizado para dividir SSS en subconjuntosValues(A)Values(A)Values(A) es el conjunto de valores que puede tomar el atributo AAASvS_vSv​ es el subconjunto de SSS en el que el atributo AAA tiene el valor vvv "},{"title":"Indice Gini​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#indice-gini","content":"El índice de Gini es una medida de impureza utilizada en los árboles de decisión para decidir qué atributo dividir un nodo en dos o más subnodos. El índice de Gini se define como: Gini(S)=1−∑i=1cpi2Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2Gini(S)=1−∑i=1c​pi2​ Donde: ccc es el número de clasespip_ipi​ es la proporción de ejemplos de clase iii en SSS "},{"title":"Pros y contras de los árboles de decisión​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#pros-y-contras-de-los-árboles-de-decisión","content":""},{"title":"Pros​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#pros","content":"Fácil de entender e interpretar. Los árboles se pueden visualizar.Puede ser muy util para solucionar problemas relacionados con decisiones.Hay menos requisitos de limpieza de datos "},{"title":"Contras​","type":1,"pageTitle":"Arbol de Desición, conceptos básicos","url":"/en/article/arbol-decision-conceptos-basicos#contras","content":"Los árboles de decisión pueden ser poco precisos. Pueden ser muy sensibles a pequeños cambios en los datos. "},{"title":"Arboles de decisión y métodos de ensamble","type":0,"sectionRef":"#","url":"/en/article/arboles-decision-ensamble","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"¿Que es un árbol de decisión?​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#que-es-un-árbol-de-decisión","content":"Un árbol de decisión es un modelo de predicción utilizado en el ámbito de la inteligencia artificial. Dada una base de datos se construye un árbol de decisión para poder llegar a la conclusión deseada. Es una herramienta de apoyo para la toma de decisiones. "},{"title":"¿Que es un método de ensamblaje?​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#que-es-un-método-de-ensamblaje","content":"Los métodos de ensamblaje son métodos que combinan varios algoritmos de aprendizaje automático para obtener un mejor rendimiento predictivo que un solo algoritmo de aprendizaje automático. Los métodos de ensamblaje funcionan mejor cuando los predictores individuales están correlacionados entre sí. "},{"title":"Muestras con reemplazo​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#muestras-con-reemplazo","content":"En estadística, el muestreo con reemplazo es un método de muestreo en el que, para cada extracción, el elemento elegido se devuelve a la población y se mezcla con el resto de elementos. El muestreo con reemplazo es un método de muestreo no exhaustivo. P(xi)=1NP(x_i) = \\frac{1}{N}P(xi​)=N1​ En arboles de decisión se utiliza el muestreo con reemplazo para generar los árboles de decisión que se utilizaran para el ensamblaje, es decir, se generan varios árboles de decisión con muestras de la base de datos original, y se combinan para generar un modelo más robusto. "},{"title":"Random Forest​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#random-forest","content":"Random Forest es un método de ensamblaje que combina varios árboles de decisión, cada uno de los cuales se genera con una muestra de la base de datos original, y se combinan para generar un modelo más robusto. esteme metodo usa el muestreo con reemplazo para generar los árboles de decisión. Tenemos un datos de entrenamiento de tamaño mmm para b = 1 hasta B: Utilizamos el muestreo con reemplazo para generar una muestra de tamaño mmm de la base de datos original. Entrenamos un árbol de decisión TbT_bTb​ con la muestra generada.Se obtiene el modelo final combinando los BBB árboles de decisión generados. Cuando usamos este algorithmo, muchas veces tenemos la misma división en el nodo raíz, por lo que podemos modificar un poco el algorithmo para que esto no suceda, y así obtener un mejor modelo. "},{"title":"Elección de características aleatorias​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#elección-de-características-aleatorias","content":"En cada nodo, se elige un subconjunto aleatorio de kkk características de todo el conjunto de características. si nnn es el número total de características, se recomienda k=nk = \\sqrt{n}k=n​ para la regresión y k=n3k = \\frac{n}{3}k=3n​ para la clasificación, debe de tener en cuenta que esto es recomendado para un gran número de características "},{"title":"XGBoost ( ExTreme Gradient Boosting)​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#xgboost--extreme-gradient-boosting","content":"XGBoost es un método de ensamblaje que combina varios árboles de decisión, cada uno de los cuales se genera con una muestra de la base de datos original, y se combinan para generar un modelo más robusto. este metodo usa el muestreo con reemplazo para generar los árboles de decisión. Pero a diferencia de Random Forest, XGBoost utiliza un algorithmo de optimización para generar los árboles de decisión, En vez de utilizar el muestreo con reemplazo para generar los árboles de decisión con una probabilidad uniforme 1/m1/m1/m, XGBoost utiliza un algorithmo de optimización para generar los árboles de decisión con una probabilidad pip_ipi​ que depende de la pérdida de la iteración anterior. pi=e−ΔLiλ∑i=1me−ΔLiλp_i = \\frac{e^{\\frac{-\\Delta L_i}{\\lambda}}}{\\sum_{i=1}^{m} e^{\\frac{-\\Delta L_i}{\\lambda}}}pi​=∑i=1m​eλ−ΔLi​​eλ−ΔLi​​​ Donde: ΔLi\\Delta L_iΔLi​ es la pérdida de la iteración iiiλ\\lambdaλ es un parámetro de regularización La idea de esto es que el algorithmo de optimización se enfoque en las muestras que tienen una pérdida mayor, y así generar un mejor modelo. "},{"title":"Ventajas de XGBoost​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#ventajas-de-xgboost","content":"Implementaciónes open source en varios lenguajes de programaciónRapidez en el entrenamientoBuena elección de divisiónes criticas por defecto y criterio para cuando parar de dividirRegularización para evitar el sobreajuste "},{"title":"Implementación en Python​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#implementación-en-python","content":"Para la implementación en Python, se utilizara la librería XGBoost, la cual se puede instalar con el comando: from xgboost import XGBClassifier model = XGBClassifier() # XGBRegressor para regresión model.fit(X_train, y_train) y_pred = model.predict(X_test)  "},{"title":"Cuando usar Arboles de decisión y métodos de ensamblaje​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#cuando-usar-arboles-de-decisión-y-métodos-de-ensamblaje","content":"Trabajan bien con datos tabulares (estructurados)No se recomienda para datos no estructurados (imágenes, texto, audio, etc)Es muy rápido en entrenamiento y predicciónPequeños arboles de decisión son fáciles de interpretar (visualizar) "},{"title":"Cuando usar neural networks​","type":1,"pageTitle":"Arboles de decisión y métodos de ensamble","url":"/en/article/arboles-decision-ensamble#cuando-usar-neural-networks","content":"Trabaja bien con todo tipo de datos tabulares &quot;estructurados&quot; y &quot;no estructurados&quot;Puede ser lento en entrenamiento y predicciónTrabaja con transfer learningCuando trabajamos con multiples modelos juntos, puede ser mas sencillo encadenarlos con una red neuronal "},{"title":"Brocasting con Numpy","type":0,"sectionRef":"#","url":"/en/article/brocasting-con-numpy","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Ejemplos de broadcasting​","type":1,"pageTitle":"Brocasting con Numpy","url":"/en/article/brocasting-con-numpy#ejemplos-de-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t1 x 5\t10 x 5 10 x 5\t5\t10 x 5 10 x 5\t10 x 1\t10 x 5 10 x 5\tscalar\t10 x 5 import numpy as np from rich import print # matris de 10x5 m10_5 = np.random.randint(0, 10, (10, 5)) m1_5 = np.random.randint(0, 10, (1, 5)) m5 = np.random.randint(0, 10, (5)) m10_1 = np.random.randint(0, 10, (10, 1)) scalar = 5 print('Broadcasting de 10x5 + 1x5') print(m10_5 + m1_5) print('Broadcasting de 10x5 + 5') print(m10_5 + m5) print('Broadcasting de 10x5 + 10x1') print(m10_5 + m10_1) print(m10_5 + scalar)  Broadcasting de 10x5 + 1x5  [[ 9 2 6 5 1] [ 5 9 10 10 8] [ 6 9 3 10 3] [ 8 8 7 4 8] [ 5 2 6 10 5] [ 7 9 9 8 3] [ 7 5 3 5 7] [ 4 7 7 3 1] [ 2 8 6 12 7] [ 9 9 12 10 10]]  Broadcasting de 10x5 + 5  [[ 7 11 9 2 5] [ 3 18 13 7 12] [ 4 18 6 7 7] [ 6 17 10 1 12] [ 3 11 9 7 9] [ 5 18 12 5 7] [ 5 14 6 2 11] [ 2 16 10 0 5] [ 0 17 9 9 11] [ 7 18 15 7 14]]  Broadcasting de 10x5 + 10x1  [[14 9 10 9 7] [ 5 11 9 9 9] [ 5 10 1 8 3] [11 13 9 6 12] [ 9 8 9 13 10] [11 15 12 11 8] [ 6 6 1 3 7] [ 3 8 5 1 1] [ 8 16 11 17 14] [ 7 9 9 7 9]]  [[12 7 8 7 5] [ 8 14 12 12 12] [ 9 14 5 12 7] [11 13 9 6 12] [ 8 7 8 12 9] [10 14 11 10 7] [10 10 5 7 11] [ 7 12 9 5 5] [ 5 13 8 14 11] [12 14 14 12 14]]  "},{"title":"Ejemplos de no broadcasting​","type":1,"pageTitle":"Brocasting con Numpy","url":"/en/article/brocasting-con-numpy#ejemplos-de-no-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t5 x 10\tError 10 x 5\t10\tError m10_5 = np.random.randint(0, 10, (10, 5)) m5_10 = np.random.randint(0, 10, (5, 10)) m10 = np.random.randint(0, 10, (10)) print('Broadcasting de 10x5 + 5x10') print(m10_5 + m5_10)  Broadcasting de 10x5 + 5x10  ValueError: operands could not be broadcast together with shapes (10,5) (5,10)  print('Broadcasting de 10x5 + 10') print(m10_5 + m10)  Broadcasting de 10x5 + 10  ValueError: operands could not be broadcast together with shapes (10,5) (10,)  "},{"title":"Comando mágico timeit - jupyter","type":0,"sectionRef":"#","url":"/en/article/comando-magico-timeit-jupyter","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"1. Uso básico de %timeit​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/en/article/comando-magico-timeit-jupyter#uso-básico-de-timeit","content":"Para utilizar %timeit, simplemente coloca el comando mágico antes de la expresión o función que deseas medir. Por ejemplo, para medir el tiempo de ejecución de la expresión '1 + 1', puedes usar el siguiente código en una celda de Jupyter Lab: %timeit 1 + 1  10.1 ns ± 0.491 ns per loop (mean ± std. dev. of 7 runs, 100,000,000 loops each)  Después de ejecutar la celda, %timeit ejecutará la expresión '1 + 1'varias veces y mostrará el tiempo promedio de ejecución. En este caso, el tiempo promedio de ejecución en unidades de tiempo "},{"title":"2. Tabla de tiempos​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/en/article/comando-magico-timeit-jupyter#tabla-de-tiempos","content":"Abreviatura\tUnidad de tiempons\tnanosegundos us\tmicrosegundos ms\tmilisegundos s\tsegundos m\tminutos h\thoras "},{"title":"3. Especificar el número de ejecuciones y repeticiones​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/en/article/comando-magico-timeit-jupyter#especificar-el-número-de-ejecuciones-y-repeticiones","content":"Por defecto, %timeit ejecuta la expresión o función 100.000 veces y repite la operación tres veces. Puedes especificar el número de ejecuciones y repeticiones utilizando la sintaxis%timeit -r &lt;repeticiones&gt; -n &lt;ejecuciones&gt;. Por ejemplo, para ejecutar la expresión '1 + 1' 10.000 veces y repetir la operación cinco veces, puedes usar el siguiente código: %timeit -r5 -n50 1 + 1  25.6 ns ± 5.28 ns per loop (mean ± std. dev. of 5 runs, 50 loops each)  En el comando anterior espesificamps que se ejecute 50 veces en 5 repeticiones "},{"title":"5. Medir el tiempo de ejecución de una función​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/en/article/comando-magico-timeit-jupyter#medir-el-tiempo-de-ejecución-de-una-función","content":"También puedes utilizar %timeit para medir el tiempo de ejecución de una función. Por ejemplo, para medir el tiempo de ejecución de la función sum() de Python, puedes usar el siguiente código: def mi_funcion(): # puedes colocar cualquier código aquí return 1 + 1  Jupyter Lab ejecutara el código y te devolvera el tiempo de ejecución de la función "},{"title":"6. Medir el tiempo de ejecución de una celda​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/en/article/comando-magico-timeit-jupyter#medir-el-tiempo-de-ejecución-de-una-celda","content":"También puedes utilizar %timeit para medir el tiempo de ejecución de una celda completa. Por ejemplo, para medir el tiempo de ejecución de la siguiente celda, puedes usar el siguiente código: %%timeit x = 1 x += 1  36.7 ns ± 1.13 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)  "},{"title":"7. Obtener el tiempo de ejecución como variable​","type":1,"pageTitle":"Comando mágico timeit - jupyter","url":"/en/article/comando-magico-timeit-jupyter#obtener-el-tiempo-de-ejecución-como-variable","content":"En caso de que desees obtener información más detallada sobre el tiempo de ejecución, podrias asignar el resultado de %timeit a una variable, para esto utilizaremos las opciones -o para almacenar el resultado y -q para silenciar la salida de la celda. Por ejemplo, para obtener el tiempo de ejecución de la expresión '1 + 1' como una variable, puedes usar el siguiente código: resultado = %timeit -o -q 1 + 1 print(f'El mejor tiempo fue {resultado.best}') print(f'El peor tiempo fue {resultado.worst}')  El mejor tiempo fue 9.775258000008763e-09 El peor tiempo fue 1.1235137999756262e-08  Hemos visto de forma muy rapida como usar el comando magico %timeit en Jupyter Lab, con expresiones muy sencillas, pero en la practica se utiliza para medir el tiempo de ejecución de funciones y celdas completas, lo cual es muy util para comparar diferentes enfoques de implementación. "},{"title":"Articulos","type":0,"sectionRef":"#","url":"/en/article/index","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Expresiones regulares en Python","type":0,"sectionRef":"#","url":"/en/article/expresiones-regulares-python","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Caracteres especiales​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/en/article/expresiones-regulares-python#caracteres-especiales","content":"Los caracteres especiales son aquellos que tienen un significado especial para las expresiones regulares. Por ejemplo, el punto y coma (;) es un caracter especial que se utiliza para separar instrucciones en Python. Sin embargo, en las expresiones regulares, el punto y coma (;) es un caracter especial que se utiliza para indicar que el patrón de búsqueda debe coincidir con cualquier caracter. A continuación se muestra una lista de los caracteres especiales más utilizados en las expresiones regulares: Caracter\tDescripción.\tCoincide con cualquier caracter ^\tCoincide con el inicio de una cadena $\tCoincide con el final de una cadena *\tCoincide con 0 o más ocurrencias del caracter anterior +\tCoincide con 1 o más ocurrencias del caracter anterior ?\tCoincide con 0 o 1 ocurrencia del caracter anterior {n}\tCoincide con n ocurrencias del caracter anterior {n,}\tCoincide con n o más ocurrencias del caracter anterior {n,m}\tCoincide con un rango de ocurrencias del caracter anterior […]\tCoincide con cualquier caracter dentro de los corchetes [^...]\tCoincide con cualquier caracter que no esté dentro de los corchetes (…)\tAgrupa una serie de patrones Coincide con un espacio en blanco Coincide con cualquier caracter que no sea un espacio en blanco Coincide con cualquier caracter alfanumérico Coincide con cualquier caracter que no sea alfanumérico Coincide con cualquier caracter numérico Coincide con cualquier caracter que no sea numérico "},{"title":"Trabajando en python​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/en/article/expresiones-regulares-python#trabajando-en-python","content":"para trabajar con expresiones regulares en python, se debe importar el módulo re. A continuación se muestra un ejemplo de como utilizar el módulo re para buscar un patrón en una cadena de caracteres: import re  "},{"title":"Encontrar todas las coincidencias​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/en/article/expresiones-regulares-python#encontrar-todas-las-coincidencias","content":"text = &quot;Hola, mi nombre es Juan y mi número de teléfono es 123456789&quot; pattern = r&quot;mi&quot; print(re.findall(pattern, text)) pattern = r&quot;\\d+&quot; print(re.findall(pattern, text))  ['mi', 'mi'] ['123456789']  "},{"title":"Sustituir un patrón en una cadena de caracteres​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/en/article/expresiones-regulares-python#sustituir-un-patrón-en-una-cadena-de-caracteres","content":"text = &quot;Hol, mi nombre es Juan y mi nUmero de teléfono es 123456789&quot; text = re.sub(r&quot;Hol&quot;, &quot;Hola&quot;, text) print(text) text = re.sub(r&quot;U&quot;, &quot;ú&quot;, text) print(text)  Hola, mi nombre es Juan y mi nUmero de teléfono es 123456789 Hola, mi nombre es Juan y mi número de teléfono es 123456789  "},{"title":"Dividir una cadena de caracteres​","type":1,"pageTitle":"Expresiones regulares en Python","url":"/en/article/expresiones-regulares-python#dividir-una-cadena-de-caracteres","content":"text = &quot;Hola, mi nombre es Juan y mi número de teléfono es 123456789&quot; text_split = re.split(r&quot;y&quot;, text) text_split  ['Hola, mi nombre es Juan ', ' mi número de teléfono es 123456789']  Python tambien tiene integrado funciones de expresiones regulares en el módulo string. text = &quot;Hola, mi nombre es Juan y mi número de teléfono es 123456789&quot; print(text.replace(&quot;Juan&quot;, &quot;Darvin&quot;)) print(text.split(','))  Hola, mi nombre es Darvin y mi número de teléfono es 123456789 ['Hola', ' mi nombre es Juan y mi número de teléfono es 123456789']  Estos son solo alguno de todos los metodos que tiene python para trabajar con expresiones regulares. Para más información, puede consultar la documentación oficial de python en el siguiente enlace:https://docs.python.org/3/library/re.html "},{"title":"Clustering","type":0,"sectionRef":"#","url":"/en/article/introduccion-a-los-algoritmos-de-clustering","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"¿Que es clustering?​","type":1,"pageTitle":"Clustering","url":"/en/article/introduccion-a-los-algoritmos-de-clustering#que-es-clustering","content":"Clustering es un método de aprendizaje no supervisado, que consiste en agrupar un conjunto de objetos de tal manera que los objetos del mismo grupo (o cluster) sean más similares (en algún sentido o en algún aspecto) entre sí que los de otros grupos. "},{"title":"k-means​","type":1,"pageTitle":"Clustering","url":"/en/article/introduccion-a-los-algoritmos-de-clustering#k-means","content":"k-means es un algoritmo de clustering que consiste en agrupar un conjunto de objetos de tal manera que los objetos del mismo grupo (o cluster) sean más similares (en algún sentido o en algún aspecto) entre sí que los de otros grupos. "},{"title":"Algoritmo de k-means​","type":1,"pageTitle":"Clustering","url":"/en/article/introduccion-a-los-algoritmos-de-clustering#algoritmo--de-k-means","content":"Inicializar los centroides de los clusters aleatoriamente (k puntos): μ1,μ2,...,μk\\mu_1, \\mu_2, ..., \\mu_kμ1​,μ2​,...,μk​ repeat{#Asignar puntos a los centroides del clusterfor i=1 to mc(i):=ıˊndice (de 1 a k) del centroide maˊs cercano a x(i)# Mover los centroides de los clustersfor k=1 to kuk:=promedio de los puntos asignados al cluster k}\\begin{align} repeat \\{ \\\\ &amp; \\# Asignar \\ puntos \\ a \\ los \\ centroides \\ del \\ cluster \\\\ &amp; \\text{for } i = 1 \\text{ to } m \\\\ &amp; \\quad c^{(i)} := \\text{índice (de 1 a k) del centroide más cercano a } x^{(i)} \\\\ &amp; \\# \\ Mover \\ los \\ centroides \\ de \\ los \\ clusters \\\\ &amp; \\text{for } k = 1 \\text{ to } k \\\\ &amp; \\quad u_k := \\text{promedio de los puntos asignados al cluster } k \\\\ \\} \\\\ \\end{align}repeat{}​#Asignar puntos a los centroides del clusterfor i=1 to mc(i):=ıˊndice (de 1 a k) del centroide maˊs cercano a x(i)# Mover los centroides de los clustersfor k=1 to kuk​:=promedio de los puntos asignados al cluster k​​  "},{"title":"k-means optimización objetivo​","type":1,"pageTitle":"Clustering","url":"/en/article/introduccion-a-los-algoritmos-de-clustering#k-means-optimización-objetivo","content":"c(i)c^{(i)}c(i) = índice del cluster (1, 2, ..., k) al que se asigna el ejemplo x(i)x^{(i)}x(i)uku_kuk​ = vector de parámetros del centroide del cluster kkk μc(i)\\mu_{c^{(i)}}μc(i)​ = vector de parámetros del centroide del cluster al que se asigna el ejemplo x(i)x^{(i)}x(i) Función de costo J(c(1),...,c(m),μ1,...,μk)=1m∑i=1m∣∣x(i)−μc(i)∣∣2J(c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_k) = \\frac{1}{m} \\sum_{i=1}^{m} ||x^{(i)} - \\mu_{c^{(i)}}||^2J(c(1),...,c(m),μ1​,...,μk​)=m1​∑i=1m​∣∣x(i)−μc(i)​∣∣2 Objetivo: Encontrar c(1),...,c(m),μ1,...,μkc^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_kc(1),...,c(m),μ1​,...,μk​ que minimicen JJJ. minc(1),...,c(m),μ1,...,μkJ(c(1),...,c(m),μ1,...,μk)min_{c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_k} J(c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_k)minc(1),...,c(m),μ1​,...,μk​​J(c(1),...,c(m),μ1​,...,μk​) "},{"title":"Inicializando k-means​","type":1,"pageTitle":"Clustering","url":"/en/article/introduccion-a-los-algoritmos-de-clustering#inicializando-k-means","content":"Seleccionar aleatoriamente kkk ejemplos de entrenamiento x(1),...,x(k)x^{(1)}, ..., x^{(k)}x(1),...,x(k) que servirán como los centroides iniciales: μ1,...,μk\\mu_1, ..., \\mu_kμ1​,...,μk​. "},{"title":"Elección del número de clusters​","type":1,"pageTitle":"Clustering","url":"/en/article/introduccion-a-los-algoritmos-de-clustering#elección-del-número-de-clusters","content":"¿Cual es el número de clusters óptimo? Para elegir el número de clusters óptimo se puede utilizar los siguientes 2 métodos: Método del codo: el metodo del codo consiste en graficar el valor de la función de costo JJJ en función del número de clusters kkk. El número de clusters óptimo será el valor de kkk en el que la función de costo JJJ se &quot;quiebre&quot; o tenga un cambio de pendiente más pronunciado.  No es una buena métrica para elegir el número de clusters óptimo, ya que no siempre se puede identificar un cambio de pendiente claro en la gráfica, no hay un codo claro. la elección del número de clusters es subjetiva, depende de la aplicación y del contexto. "},{"title":"¿Como perfilar código con line_profiler?","type":0,"sectionRef":"#","url":"/en/article/perfilar-codigo-con-line-profiler","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Instalación​","type":1,"pageTitle":"¿Como perfilar código con line_profiler?","url":"/en/article/perfilar-codigo-con-line-profiler#instalación","content":"Como line_profiler no viene instalado por defecto en Anaconda, lo instalaremos con conda: En la terminal: pip install line_profiler  En el notebook: ! pip install line_profiler  "},{"title":"¿Cómo funciona en Jupyter?​","type":1,"pageTitle":"¿Como perfilar código con line_profiler?","url":"/en/article/perfilar-codigo-con-line-profiler#cómo-funciona-en-jupyter","content":"line_profiler es una herramienta que permite perfilar el código de un programa. Esto significa que nos permite ver cuánto tiempo se tarda en ejecutar cada línea de código. Para ello, line_profiler nos permite usar el comando %lprun en Jupyter. Este comando nos permite perfilar una función. Para ello, debemos añadir el decorador @profile a la función que queremos perfilar. cargar el módulo line_profiler en el notebook: %load_ext line_profiler  The line_profiler extension is already loaded. To reload it, use: %reload_ext line_profiler  "},{"title":"Perfilando una función​","type":1,"pageTitle":"¿Como perfilar código con line_profiler?","url":"/en/article/perfilar-codigo-con-line-profiler#perfilando-una-función","content":"Perfilar una funcion en en jupyter lab ees muy sencillo con el comando%lprun. Para ello vamos a crear una funcion de prueba que calcule el doble de una lista de números:  def funcion_prueba(): data = [1, 2, 3, 4, 5, 6, 7, 8, 9] doble = [] for item in data: doble.append(item * 2) return doble  %lprun -f funcion_prueba funcion_prueba()  Timer unit: 1e-07 s Total time: 8e-06 s Could not find file C:\\Users\\WillyCotrina\\AppData\\Local\\Temp\\ipykernel_14792\\1026023441.py Are you sure you are running this program from the same directory that you ran the profiler from? Continuing without the function's contents. Line # Hits Time Per Hit % Time Line Contents ============================================================== 1 2 1 7.0 7.0 8.8 3 1 3.0 3.0 3.8 4 9 24.0 2.7 30.0 5 9 43.0 4.8 53.8 6 7 1 3.0 3.0 3.8  Como pudimos notar pefilar una funcion es muy sencillo y extremaente util para optimizar el codigo de un programa. "},{"title":"Detección de anomalías","type":0,"sectionRef":"#","url":"/en/article/ml-deteccion-de-anomalias","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Estimación de densidad​","type":1,"pageTitle":"Detección de anomalías","url":"/en/article/ml-deteccion-de-anomalias#estimación-de-densidad","content":"La detección de anomalías se puede realizar utilizando un modelo de estimación de densidad. La idea es que los datos normales se distribuirán de manera diferente a los datos anormales. Por lo tanto, podemos estimar la densidad de los datos normales y luego identificar los puntos de datos que tienen una densidad significativamente menor como anomalías. Dado el conjunto de datos de entrenamiento{x(1),x(2),…,x(m)}\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)} \\}{x(1),x(2),…,x(m)}, donde cada ejemplo tiene nnncaracterísticas, podemos estimar la densidad de los datos como: p(x)=p(x1;μ1,σ12)×p(x2;μ2,σ22)×…×p(xn;μn,σn2)p(x) = p(x_1; \\mu_1, \\sigma_1^2) \\times p(x_2; \\mu_2, \\sigma_2^2) \\times \\ldots \\times p(x_n; \\mu_n, \\sigma_n^2)p(x)=p(x1​;μ1​,σ12​)×p(x2​;μ2​,σ22​)×…×p(xn​;μn​,σn2​)=∏j=1np(xj;μj,σj2)= \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2)=∏j=1n​p(xj​;μj​,σj2​) "},{"title":"Algoritmo de detección de anomalías​","type":1,"pageTitle":"Detección de anomalías","url":"/en/article/ml-deteccion-de-anomalias#algoritmo-de-detección-de-anomalías","content":"Elija las características xix_ixi​ que crea que pueden indicar anomalías. Ajuste los parámetrosμ1,…,μn,σ12,…,σn2\\mu_1, \\ldots, \\mu_n, \\sigma_1^2, \\ldots, \\sigma_n^2μ1​,…,μn​,σ12​,…,σn2​ en el conjunto de entrenamiento {x(1),x(2),…,x(m)}\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)} \\}{x(1),x(2),…,x(m)}.μ⃗=1m∑i=1mx(i)⃗\\vec{\\mu} = \\frac{1}{m} \\sum_{i=1}^m \\vec{x^{(i)}}μ​=m1​∑i=1m​x(i)σ2⃗=1m∑i=1m(x(i)⃗−μ⃗)2\\vec{\\sigma^2} = \\frac{1}{m} \\sum_{i=1}^m (\\vec{x^{(i)}} - \\vec{\\mu})^2σ2=m1​∑i=1m​(x(i)−μ​)2 Dado un nuevo ejemplo xxx, compute p(x)p(x)p(x): p(x)=∏j=1np(xj;μj,σj2)=∏j=1n12πσjexp⁡(−(xj−μj)22σj2)p(x) = \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2) = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma_j} \\exp \\left( - \\frac{(x_j - \\mu_j)^2}{2\\sigma_j^2} \\right)p(x)=∏j=1n​p(xj​;μj​,σj2​)=∏j=1n​2π​σj​1​exp(−2σj2​(xj​−μj​)2​) Si p(x)&lt;ϵp(x) &lt; \\epsilonp(x)&lt;ϵ, marque un ejemplo de anomalía.  "},{"title":"Escoger que caracteristicas usar​","type":1,"pageTitle":"Detección de anomalías","url":"/en/article/ml-deteccion-de-anomalias#escoger-que-caracteristicas-usar","content":"En Deteción de Anomalías, se debe escoger que caracteristicas usar, ya que si se usan todas las caracteristicas, el algoritmo no funcionará correctamentem. "},{"title":"Caracteristicas no gaussianas​","type":1,"pageTitle":"Detección de anomalías","url":"/en/article/ml-deteccion-de-anomalias#caracteristicas-no-gaussianas","content":"Cuando encontramos caracteristicas que no son gaussianas, se debe aplicar una transformación a los datos para que se vuelvan gaussianos. por ejemplo: x1=log⁡(x1)x_1 = \\log(x_1)x1​=log(x1​)x2=log⁡(x2+c)x_2 = \\log(x_2 + c)x2​=log(x2​+c)x3=x3x_3 = \\sqrt{x_3}x3​=x3​​x4=x41/3x_4 = x_4^{1/3}x4​=x41/3​ En python from scipy.stats import skewnorm import matplotlib.pyplot as plt numValues = 1000 maxValue = 100 skewness = 20 randomValues = skewnorm.rvs(a=skewness, loc=maxValue, size=numValues) randomValues = randomValues - min(randomValues) # cambia el conjunto de datos para que comience en 0 randomValues = randomValues / max(randomValues) # cambia el conjunto de datos para que termine en 1 randomValues = randomValues * maxValue # cambia el conjunto de datos para que termine en maxValue x = randomValues fig, ax = plt.subplots(1, 3, figsize=(15, 5)) ax[0].hist(x, bins=50) ax[0].set_title('X') # x**2 ax[1].hist(x**2, bins=50) ax[1].set_title('X^2') # x**0.4 ax[2].hist(x**0.4, bins=50) ax[2].set_title('X^0.4') plt.show()   "},{"title":"Error en el analisis para detección de anomalías​","type":1,"pageTitle":"Detección de anomalías","url":"/en/article/ml-deteccion-de-anomalias#error-en-el-analisis-para-detección-de-anomalías","content":"El problema más común en la detección de anomalías es que el conjunto de datos de entrenamiento contiene muy pocos ejemplos de anomalías. Por lo tanto, el algoritmo de detección de anomalías no puede aprender lo suficiente sobre los ejemplos de anomalías para identificarlos correctamente en el conjunto de prueba. "},{"title":"Broadcasting con NumPy","type":0,"sectionRef":"#","url":"/en/article/tags/broadcasting","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Ejemplos de broadcasting​","type":1,"pageTitle":"Broadcasting con NumPy","url":"/en/article/tags/broadcasting#ejemplos-de-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t1 x 5\t10 x 5 10 x 5\t5\t10 x 5 10 x 5\t10 x 1\t10 x 5 10 x 5\tscalar\t10 x 5 import numpy as np from rich import print # matris de 10x5 m10_5 = np.random.randint(0, 10, (10, 5)) m1_5 = np.random.randint(0, 10, (1, 5)) m5 = np.random.randint(0, 10, (5)) m10_1 = np.random.randint(0, 10, (10, 1)) scalar = 5 print('Broadcasting de 10x5 + 1x5') print(m10_5 + m1_5) print('Broadcasting de 10x5 + 5') print(m10_5 + m5) print('Broadcasting de 10x5 + 10x1') print(m10_5 + m10_1) print(m10_5 + scalar)  Broadcasting de 10x5 + 1x5  [[ 9 2 6 5 1] [ 5 9 10 10 8] [ 6 9 3 10 3] [ 8 8 7 4 8] [ 5 2 6 10 5] [ 7 9 9 8 3] [ 7 5 3 5 7] [ 4 7 7 3 1] [ 2 8 6 12 7] [ 9 9 12 10 10]]  Broadcasting de 10x5 + 5  [[ 7 11 9 2 5] [ 3 18 13 7 12] [ 4 18 6 7 7] [ 6 17 10 1 12] [ 3 11 9 7 9] [ 5 18 12 5 7] [ 5 14 6 2 11] [ 2 16 10 0 5] [ 0 17 9 9 11] [ 7 18 15 7 14]]  Broadcasting de 10x5 + 10x1  [[14 9 10 9 7] [ 5 11 9 9 9] [ 5 10 1 8 3] [11 13 9 6 12] [ 9 8 9 13 10] [11 15 12 11 8] [ 6 6 1 3 7] [ 3 8 5 1 1] [ 8 16 11 17 14] [ 7 9 9 7 9]]  [[12 7 8 7 5] [ 8 14 12 12 12] [ 9 14 5 12 7] [11 13 9 6 12] [ 8 7 8 12 9] [10 14 11 10 7] [10 10 5 7 11] [ 7 12 9 5 5] [ 5 13 8 14 11] [12 14 14 12 14]]  "},{"title":"Ejemplos de no broadcasting​","type":1,"pageTitle":"Broadcasting con NumPy","url":"/en/article/tags/broadcasting#ejemplos-de-no-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t5 x 10\tError 10 x 5\t10\tError m10_5 = np.random.randint(0, 10, (10, 5)) m5_10 = np.random.randint(0, 10, (5, 10)) m10 = np.random.randint(0, 10, (10)) print('Broadcasting de 10x5 + 5x10') print(m10_5 + m5_10)  Broadcasting de 10x5 + 5x10  ValueError: operands could not be broadcast together with shapes (10,5) (5,10)  print('Broadcasting de 10x5 + 10') print(m10_5 + m10)  Broadcasting de 10x5 + 10  ValueError: operands could not be broadcast together with shapes (10,5) (10,)  "},{"title":"Broadcasting con NumPy","type":0,"sectionRef":"#","url":"/en/article/tags/numpy","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Ejemplos de broadcasting​","type":1,"pageTitle":"Broadcasting con NumPy","url":"/en/article/tags/numpy#ejemplos-de-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t1 x 5\t10 x 5 10 x 5\t5\t10 x 5 10 x 5\t10 x 1\t10 x 5 10 x 5\tscalar\t10 x 5 import numpy as np from rich import print # matris de 10x5 m10_5 = np.random.randint(0, 10, (10, 5)) m1_5 = np.random.randint(0, 10, (1, 5)) m5 = np.random.randint(0, 10, (5)) m10_1 = np.random.randint(0, 10, (10, 1)) scalar = 5 print('Broadcasting de 10x5 + 1x5') print(m10_5 + m1_5) print('Broadcasting de 10x5 + 5') print(m10_5 + m5) print('Broadcasting de 10x5 + 10x1') print(m10_5 + m10_1) print(m10_5 + scalar)  Broadcasting de 10x5 + 1x5  [[ 9 2 6 5 1] [ 5 9 10 10 8] [ 6 9 3 10 3] [ 8 8 7 4 8] [ 5 2 6 10 5] [ 7 9 9 8 3] [ 7 5 3 5 7] [ 4 7 7 3 1] [ 2 8 6 12 7] [ 9 9 12 10 10]]  Broadcasting de 10x5 + 5  [[ 7 11 9 2 5] [ 3 18 13 7 12] [ 4 18 6 7 7] [ 6 17 10 1 12] [ 3 11 9 7 9] [ 5 18 12 5 7] [ 5 14 6 2 11] [ 2 16 10 0 5] [ 0 17 9 9 11] [ 7 18 15 7 14]]  Broadcasting de 10x5 + 10x1  [[14 9 10 9 7] [ 5 11 9 9 9] [ 5 10 1 8 3] [11 13 9 6 12] [ 9 8 9 13 10] [11 15 12 11 8] [ 6 6 1 3 7] [ 3 8 5 1 1] [ 8 16 11 17 14] [ 7 9 9 7 9]]  [[12 7 8 7 5] [ 8 14 12 12 12] [ 9 14 5 12 7] [11 13 9 6 12] [ 8 7 8 12 9] [10 14 11 10 7] [10 10 5 7 11] [ 7 12 9 5 5] [ 5 13 8 14 11] [12 14 14 12 14]]  "},{"title":"Ejemplos de no broadcasting​","type":1,"pageTitle":"Broadcasting con NumPy","url":"/en/article/tags/numpy#ejemplos-de-no-broadcasting","content":"Array 1\tArray 2\tResultado10 x 5\t5 x 10\tError 10 x 5\t10\tError m10_5 = np.random.randint(0, 10, (10, 5)) m5_10 = np.random.randint(0, 10, (5, 10)) m10 = np.random.randint(0, 10, (10)) print('Broadcasting de 10x5 + 5x10') print(m10_5 + m5_10)  Broadcasting de 10x5 + 5x10  ValueError: operands could not be broadcast together with shapes (10,5) (5,10)  print('Broadcasting de 10x5 + 10') print(m10_5 + m10)  Broadcasting de 10x5 + 10  ValueError: operands could not be broadcast together with shapes (10,5) (10,)  "},{"title":"Teorema de limite central","type":0,"sectionRef":"#","url":"/en/article/teorema-de-limite-central","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"En python​","type":1,"pageTitle":"Teorema de limite central","url":"/en/article/teorema-de-limite-central#en-python","content":"Para graficar el TLC en python, usaremos un ejemplo de tirar dados. import numpy as np import matplotlib.pyplot as plt # Tiraremos 10 veces los dados y calcularemos la media dados = list(range(1,7)) muestra_10 = np.random.choice(dados, size=10, replace=True) media = np.mean(muestra_10) print(&quot;La media de la muestra es: &quot;, media)  La media de la muestra es: 3.0  Como podemos ver la Media de esta muestra no es 3.5, hora veamos que pasa si hacemos este mismo experimento pero 10 veces. exp_10 = [np.mean(muestra) for muestra in np.random.choice(dados, size=(10, 10), replace=True)] # Graficamos el histograma de las medias plt.hist(exp_10, bins=10, density=True, alpha=0.5) plt.vlines(3.5, 0, 1, color='red', label='Media teórica') plt.vlines(np.mean(exp_10), 0, 1, color='green', label='Media muestral') plt.show()   Ahora veamos que pasa si hacemos este mismo experimento pero 1000 veces. exp_1000 = [np.mean(muestra) for muestra in np.random.choice(dados, size=(1000, 10), replace=True)] # Graficamos plt.hist(exp_1000, bins=10, density=True, alpha=0.5) plt.vlines(3.5, 0, 1, color='red', label='Media teórica') plt.vlines(np.mean(exp_1000), 0, 1, color='green', label='Media muestral') plt.show()   Como podemos ver, a medida que aumentamos el número de experimentos, la distribución de las medias muestrales se aproxima a una distribución normal. caution Si ejecutas este código en tu computadora, es posible que no obtengas los mismos resultados que yo, ya que los números aleatorios son generados de forma aleatoria. "},{"title":"Vectorize de numpy vs apply de pandas","type":0,"sectionRef":"#","url":"/en/article/vectorize-numpy-vs-apply-pandas","content":"Tanto numpy como pandas tienen funciones que permiten aplicar una funcion a un array o dataframe, respectivamente, de forma vectorizada. Esto significa que la funcion se aplica a todos los elementos del array o dataframe, sin necesidad de iterar sobre ellos. Esto es mucho mas eficiente que iterar sobre los elementos, ya que no se necesita hacer un loop en python, sino que la funcion se aplica en C. import numpy as np import pandas as pd # comparación de vectorize de numpy vs apply de pandas # vectorize de numpy def f(x): return x**2 + 1 array = np.arange(100000, dtype=np.int16) %timeit np.vectorize(f)(array) # pandas apply df = pd.DataFrame({'x': array}) %timeit df['x'].apply(f) 24.2 ms ± 1.56 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 40.7 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Esta es una comparación muy simple entre ambas formas de aplicar una funcion, pero nos da una idea bastante clara de la diferencia de performance entre ambas, como podemos ver vectorize fue mucho mas rapido que apply.","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Importar datos de diferentes fuentes con Python","type":0,"sectionRef":"#","url":"/en/cheat-sheets/importing-data-python","content":"Flat files CSV​ import pandas as pd pd.read_csv('file.csv') pd.read_csv('file.txt', sep='\\t') Excel import pandas as pd wb = pd.ExcelFile('file.xlsx') wb.sheet_names # Nombre de las hojas df = wb.parse('Sheet1') # Leer hoja SAS from sas7bdat import SAS7BDAT with SAS7BDAT('file.sas7bdat') as file: df_sas = file.to_data_frame() Stata import pandas as pd df = pd.read_stata('file.dta') HDF5 Los archivos HDF5 son una buena opción para guardar grandes cantidades de datos. Se pueden leer con la librería h5py import h5py data = h5py.File('file.hdf5', 'r') data.keys() # Nombre de los grupos group = data['group'] # Leer grupo group.keys() # Nombre de los datasets dataset = group['dataset'] # Leer dataset dataset.shape # Dimensiones dataset.value # Valores Matlab import scipy.io mat = scipy.io.loadmat('file.mat') Pickled files Los archivos pickled son archivos binarios de Python. Se pueden leer con la librería pickle import pickle with open('file.pkl', 'rb') as file: data = pickle.load(file) SQL from sqlalchemy import create_engine engine = create_engine('sqlite:///file.sqlite') table_names = engine.table_names() # Nombre de las tablas with engine.connect() as con: rs = con.execute('SELECT * FROM table') df = pd.DataFrame(rs.fetchall()) df.columns = rs.keys() Con Pandas​ import pandas as pd from sqlalchemy import create_engine engine = create_engine('sqlite:///file.sqlite') df = pd.read_sql_query('SELECT * FROM table', engine) ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Cheat Sheets","type":0,"sectionRef":"#","url":"/en/cheat-sheets/index","content":"Probando el componente CheatSheet con diferentes lenguajes de programación y latex. Titulo Contenido de la cheat sheet import CheatSheet from '@site/src/components/CheatSheet'; &lt;CheatSheet header='Titulo'&gt; Contenido de la cheat sheet &lt;/CheatSheet&gt; Contenido de latext Formula en latex∫abx2dx\\int_{a}^{b} x^2 dx∫ab​x2dx $$\\int_{a}^{b} x^2 dx$$ Multiples lenguajes Contenido de la cheat sheet def foo(): print('Hello world!') foo &lt;- function() { print('Hello world!') } Fomulas y codigo Formula en latex∫abx2dx\\int_{a}^{b} x^2 dx∫ab​x2dx def foo(): print('Hello world!') Imagenes Sin titulo &lt;CheatSheet&gt; Sin titulo &lt;/CheatSheet&gt; Header Texto en latex \\int_{a}^{b} x^2 dx ∫abx2dx\\int_{a}^{b} x^2 dx∫ab​x2dx Header Código python def foo(): print('Hello world!') ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Metricas para evaluar un modelo en machine learning","type":0,"sectionRef":"#","url":"/en/cheat-sheets/metrics-to-evaluate-a-machine-learning-model","content":"","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Classification Metrics​","type":1,"pageTitle":"Metricas para evaluar un modelo en machine learning","url":"/en/cheat-sheets/metrics-to-evaluate-a-machine-learning-model#classification-metrics","content":"Confusion Matrix Predicted Actual class\tPositive\tNegativePositive\tTrue Positive (TP)\tFalse Negative (FN) Negative\tFalse Positive (FP)\tTrue Negative (TN) from sklearn.metrics import confusion_matrix confusion_matrix(y_true, y_pred)  Accuaracy Usar Accuaracy cuando quieres medir la performance de un modelo de clasificacion. Es la proporcion de predicciones correctas sobre el total de predicciones realizadas. Accuracy = TP+TNTP+TN+FP+FN\\frac{TP + TN}{TP + TN + FP + FN}TP+TN+FP+FNTP+TN​ from sklearn.metrics import accuracy_score accuracy_score(y_true, y_pred)  Precision Usar Precision cuanto quieres minimizar los falsos positivos (Errores de tipo I). Es la proporcion de predicciones correctas sobre el total de predicciones realizadas. Precision = TPTP+FP\\frac{TP}{TP + FP}TP+FPTP​ from sklearn.metrics import precision_score precision_score(y_true, y_pred)  Recall Usar Recall cuando quieres minimizar los falsos negativos (Errores de tipo II). Es la proporcion de predicciones correctas sobre el total de predicciones realizadas. Recall = TPTP+FN\\frac{TP}{TP + FN}TP+FNTP​ from sklearn.metrics import recall_score recall_score(y_true, y_pred)  F1 Score Usar F1 Score cuando quieres minimizar los falsos negativos y falsos positivos. Es la media armonica entre Precision y Recall. F1 Score = 2∗Precision∗RecallPrecision+Recall\\frac{2 * Precision * Recall}{Precision + Recall}Precision+Recall2∗Precision∗Recall​ from sklearn.metrics import f1_score f1_score(y_true, y_pred)  ROC Curve Usar ROC Curve cuando quieres evaluar el rendimiento de un modelo de clasificacion binaria. Es una grafica de la tasa de verdaderos positivos (TPR) frente a la tasa de falsos positivos (FPR) para diferentes umbrales de probabilidad de clasificacion. TPR = TPTP+FN\\frac{TP}{TP + FN}TP+FNTP​FPR = FPFP+TN\\frac{FP}{FP + TN}FP+TNFP​ from sklearn.metrics import roc_curve, roc_auc_score import matplotlib.pyplot as plt fpr, tpr, thresholds = roc_curve(y_true, y_pred) auc = roc_auc_score(y_true, y_pred) plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc) plt.plot([0, 1], [0, 1], 'k--') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC Curve') plt.show()  Classification Report Usar Classification Report cuando quieres evaluar el rendimiento de un modelo de clasificacion. Es un resumen de las metricas de clasificacion para cada clase del problema. from sklearn.metrics import classification_report print(classification_report(y_true, y_pred))  "},{"title":"Regression Metrics​","type":1,"pageTitle":"Metricas para evaluar un modelo en machine learning","url":"/en/cheat-sheets/metrics-to-evaluate-a-machine-learning-model#regression-metrics","content":"Mean Absolute Error (MAE) Usar MAE cuando quieres medir el error medio de un modelo de regresion. Es la media de la diferencia absoluta entre las predicciones y los valores reales. MAE = 1n∑i=1n∣yi−y^i∣\\frac{1}{n} \\sum_{i=1}^{n} |y_{i} - \\hat{y}_{i}|n1​∑i=1n​∣yi​−y^​i​∣ from sklearn.metrics import mean_absolute_error mean_absolute_error(y_true, y_pred)  Mean Squared Error (MSE) Usar MSE cuando quieres penalizar los errores mas grandes. Es la mejor metrica cuando le preocupa las grandes desviaciones en los errores. MSE = 1n∑i=1n(yi−y^i)2\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}n1​∑i=1n​(yi​−y^​i​)2 from sklearn.metrics import mean_squared_error mean_squared_error(y_true, y_pred)  Root Mean Squared Error (RMSE) Usar RMSE cuando quieres penalizar los errores mas grandes. Es la mejor metrica cuando desea una medida que sea menos sensible a los valores atipicos. RMSE = 1n∑i=1n(yi−y^i)2\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}}n1​∑i=1n​(yi​−y^​i​)2​ from sklearn.metrics import mean_squared_error mean_squared_error(y_true, y_pred, squared=False) # or import numpy as np np.sqrt(mean_squared_error(y_true, y_pred))  R-Squared (R2) Usar R2 cuando quieres medir la varianza de los errores. Es la proporcion de la varianza de los errores y la varianza de los valores reales R2 = 1−∑i=1n(yi−y^i)2∑i=1n(yi−yˉi)21 - \\frac{\\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}}{\\sum_{i=1}^{n} (y_{i} - \\bar{y}_{i})^{2}}1−∑i=1n​(yi​−yˉ​i​)2∑i=1n​(yi​−y^​i​)2​ from sklearn.metrics import r2_score r2_score(y_true, y_pred)  "},{"title":"Cheat Sheets de Pandas reshaping","type":0,"sectionRef":"#","url":"/en/cheat-sheets/python-pandas-reshape","content":"pivot df.pivot(index='foo', columns='bar', values='baz') df.pivot_table(index='foo', columns='bar', values='baz', aggfunc='sum') melt df3.melt(id_vars=['first', 'last'], var_name='variable', value_name='value') df3.melt(id_vars=['first', 'last'], var_name='variable', value_name='value', value_vars=['height', 'weight']) Wide to long pd.wide_to_long(df, stubnames=['age', 'weight'], i=['name'], j='year') # format age_2019 pd.wide_to_long(df, stubnames=['age', 'weight'], i=['name'], j='year', sep='_', suffix='\\w+') ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Preprocesamiento de datos para machine learning","type":0,"sectionRef":"#","url":"/en/cheat-sheets/preprocessing-data","content":"Missing data Es importante tener en cuenta que los modelos de machine learning no pueden trabajar con valores nulos, por lo que es necesario reemplazarlos por algún valor. Eliminar​ Si hay muchos valores nulos, se puede eliminar la columna o fila, tener en cuenta que se puede perder información importante. df.dropna() Imputar​ from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy='mean') imputer.fit_transform(X) crear columna indicadora from sklearn.impute import MissingIndicator indicator = MissingIndicator() indicator.fit_transform(X) Encoder data Dummy​ Variable\tDummy color\tcolor_rojo\tcolor_verde\tcolor_azulrojo\t1\t0\t0 verde\t0\t1\t0 azul\t0\t0\t1 from sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder() encoder.fit_transform(X) import pandas as pd pd.get_dummies(X) Label​ Variable\tLabelrojo\t0 verde\t1 azul\t2```python from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() encoder.fit_transform(X) ``` import pandas as pd df = pd.DataFrame({'color': ['rojo', 'verde', 'azul']}) df['color'].astype('category').cat.codes Scaling and Centering Data StandardScaler​ x−μσ\\frac{x - \\mu}{\\sigma}σx−μ​ from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit_transform(X) MinMaxScaler​ x−minmax−min\\frac{x - min}{max - min}max−minx−min​ from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaler.fit_transform(X) RobustScaler​ x−Q1Q3−Q1\\frac{x - Q_1}{Q_3 - Q_1}Q3​−Q1​x−Q1​​ from sklearn.preprocessing import RobustScaler scaler = RobustScaler() scaler.fit_transform(X) Normalizer​ L1: x∑i=1n∣xi∣\\frac{x}{\\sum_{i=1}^n |x_i|}∑i=1n​∣xi​∣x​L2: x∑i=1nxi2\\frac{x}{\\sqrt{\\sum_{i=1}^n x_i^2}}∑i=1n​xi2​​x​max: xmax(x)\\frac{x}{max(x)}max(x)x​ from sklearn.preprocessing import Normalizer # L1, L2, max scaler = Normalizer(norm='l2') scaler.fit_transform(X) Feature engineering PolynomialFeatures​ x1,x2→x12,x1x2,x22x_1, x_2 \\rightarrow x_1^2, x_1x_2, x_2^2x1​,x2​→x12​,x1​x2​,x22​ from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) poly.fit_transform(X) Binning​ Este proceso se utiliza para discretizar variables continuas, es decir, convertir variables continuas en variables categóricas, agrupando los valores en intervalos. x→{0,1,2,...,n}x \\rightarrow \\{0, 1, 2,..., n\\}x→{0,1,2,...,n} from sklearn.preprocessing import KBinsDiscretizer discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform') discretizer.fit_transform(X) Feature selection VarianceThreshold​ from sklearn.feature_selection import VarianceThreshold selector = VarianceThreshold(threshold=0.1) selector.fit_transform(X) SelectKBest​ from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 selector = SelectKBest(chi2, k=2) selector.fit_transform(X, y) SelectFromModel​ from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression selector = SelectFromModel(estimator=LogisticRegression()) selector.fit_transform(X, y) RFE​ from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression selector = RFE(estimator=LogisticRegression(), n_features_to_select=2) selector.fit_transform(X, y) ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"Linear models in scikit-learn","type":0,"sectionRef":"#","url":"/en/cheat-sheets/sklearn-linear-model","content":"Linear Model The following linear models are available in scikit-learn for regression and classification tasks, if yyy is the target variable, xxx is the feature vector, and www is the weight vector y=w0+w1x1+w2x2+...+wpxpy = w_0 + w_1x_1 + w_2x_2 + ... + w_px_py=w0​+w1​x1​+w2​x2​+...+wp​xp​ w0w_0w0​ is the intercept_w1,w2,...,wpw_1, w_2, ..., w_pw1​,w2​,...,wp​ are the coef_ Linear Regression Fits a linear model with coefficients w=(w1,…,wp)w = (w1, …, wp)w=(w1,…,wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. min⁡w∣∣Xw−y∣∣22\\min_{w} || X w - y||_2^2minw​∣∣Xw−y∣∣22​ from sklearn.linear_model import LinearRegression lr = LinearRegression() Ridge Regression Applies L2 regularization to reduce the complexity of the model and prevent overfitting. min⁡w∣∣Xw−y∣∣22+α∣∣w∣∣22\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2minw​∣∣Xw−y∣∣22​+α∣∣w∣∣22​Hyperparameter α\\alphaα if α=0\\alpha = 0α=0, then the model is the same as Linear Regression from sklearn.linear_model import Ridge ridge = Ridge(alpha=1.0) from sklearn.linear_model import RidgeCV Lasso Regression Applies L1 regularization to reduce the complexity of the model and prevent overfitting. min⁡w∣∣Xw−y∣∣22+α∣∣w∣∣1\\min_{w} || X w - y||_2^2 + \\alpha ||w||_1minw​∣∣Xw−y∣∣22​+α∣∣w∣∣1​Hyperparameter α\\alphaα if α=0\\alpha = 0α=0, then the model is the same as Linear Regression from sklearn.linear_model import Lasso lasso = Lasso(alpha=1.0) Elastic Net Regression Applies both L1 and L2 regularization to reduce the complexity of the model and prevent overfitting. min⁡w∣∣Xw−y∣∣22+αρ∣∣w∣∣1+α(1−ρ)2∣∣w∣∣22\\min_{w} || X w - y||_2^2 + \\alpha \\rho ||w||_1 + \\frac{\\alpha(1-\\rho)}{2} ||w||_2^2minw​∣∣Xw−y∣∣22​+αρ∣∣w∣∣1​+2α(1−ρ)​∣∣w∣∣22​Hyperparameter α\\alphaα and l1_ratiol1\\_ratiol1_ratio if α=0\\alpha = 0α=0, and l1_ratio=0l1\\_ratio = 0l1_ratio=0, then the model is the same as Linear Regression from sklearn.linear_model import ElasticNet elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5) Polynomial Regression Generates polynomial features and fits a linear model to the transformed data. y=w0+w1x1+w2x2+w3x12+w4x1x2+w5x22+...y = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_1x_2 + w_5x_2^2 + ...y=w0​+w1​x1​+w2​x2​+w3​x12​+w4​x1​x2​+w5​x22​+...Hyperparameter degree from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline poly = PolynomialFeatures(degree=2) poly_reg = make_pipeline(poly, LinearRegression()) Logistic Regression Use when you want to predict a binary outcome (0 or 1, yes or no, true or false) given a set of independent variables. y=11+e−(w0+w1x1+w2x2+...+wpxp)y = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + ... + w_px_p)}}y=1+e−(w0​+w1​x1​+w2​x2​+...+wp​xp​)1​ from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression() Stocastic Gradient Descent Use when you want to train large datasets. wt+1=wt−η∇Qi(wt)w_{t+1} = w_t - \\eta \\nabla Q_i(w_t)wt+1​=wt​−η∇Qi​(wt​)Hyperparameter eta0 is the learning rate from sklearn.linear_model import SGDClassifier, SGDRegressor sgd_clf = SGDClassifier() sgd_reg = SGDRegressor() Bayesian Ridge Regression Bayesian Ridge Regression is similar to Ridge Regression, but it introduces a prior on the weights www. Original Algorithm is detailed in the book Bayesian learning for neural networksHyperparameter alpha_1, alpha_2, lambda_1, lambda_2 from sklearn.linear_model import BayesianRidge bayesian_ridge = BayesianRidge() Passive Aggressive Passive Aggressive algorithms are a family of algorithms for large-scale learning from sklearn.linear_model import PassiveAggressiveClassifier, PassiveAggressiveRegressor passive_aggressive_clf = PassiveAggressiveClassifier() passive_aggressive_reg = PassiveAggressiveRegressor() RANSAC Regression RANSAC (RANdom SAmple Consensus) is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. from sklearn.linear_model import RANSACRegressor ransac_reg = RANSACRegressor() ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"crear columna indicadora","type":0,"sectionRef":"#","url":"/en/cheat-sheets/tags/machine-learning","content":"Missing data Es importante tener en cuenta que los modelos de machine learning no pueden trabajar con valores nulos, por lo que es necesario reemplazarlos por algún valor. Eliminar​ Si hay muchos valores nulos, se puede eliminar la columna o fila, tener en cuenta que se puede perder información importante. df.dropna() Imputar​ from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy='mean') imputer.fit_transform(X) crear columna indicadora from sklearn.impute import MissingIndicator indicator = MissingIndicator() indicator.fit_transform(X) Encoder data Dummy​ Variable\tDummy color\tcolor_rojo\tcolor_verde\tcolor_azulrojo\t1\t0\t0 verde\t0\t1\t0 azul\t0\t0\t1 from sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder() encoder.fit_transform(X) import pandas as pd pd.get_dummies(X) Label​ Variable\tLabelrojo\t0 verde\t1 azul\t2```python from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() encoder.fit_transform(X) ``` import pandas as pd df = pd.DataFrame({'color': ['rojo', 'verde', 'azul']}) df['color'].astype('category').cat.codes Scaling and Centering Data StandardScaler​ x−μσ\\frac{x - \\mu}{\\sigma}σx−μ​ from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit_transform(X) MinMaxScaler​ x−minmax−min\\frac{x - min}{max - min}max−minx−min​ from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaler.fit_transform(X) RobustScaler​ x−Q1Q3−Q1\\frac{x - Q_1}{Q_3 - Q_1}Q3​−Q1​x−Q1​​ from sklearn.preprocessing import RobustScaler scaler = RobustScaler() scaler.fit_transform(X) Normalizer​ L1: x∑i=1n∣xi∣\\frac{x}{\\sum_{i=1}^n |x_i|}∑i=1n​∣xi​∣x​L2: x∑i=1nxi2\\frac{x}{\\sqrt{\\sum_{i=1}^n x_i^2}}∑i=1n​xi2​​x​max: xmax(x)\\frac{x}{max(x)}max(x)x​ from sklearn.preprocessing import Normalizer # L1, L2, max scaler = Normalizer(norm='l2') scaler.fit_transform(X) Feature engineering PolynomialFeatures​ x1,x2→x12,x1x2,x22x_1, x_2 \\rightarrow x_1^2, x_1x_2, x_2^2x1​,x2​→x12​,x1​x2​,x22​ from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) poly.fit_transform(X) Binning​ Este proceso se utiliza para discretizar variables continuas, es decir, convertir variables continuas en variables categóricas, agrupando los valores en intervalos. x→{0,1,2,...,n}x \\rightarrow \\{0, 1, 2,..., n\\}x→{0,1,2,...,n} from sklearn.preprocessing import KBinsDiscretizer discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform') discretizer.fit_transform(X) Feature selection VarianceThreshold​ from sklearn.feature_selection import VarianceThreshold selector = VarianceThreshold(threshold=0.1) selector.fit_transform(X) SelectKBest​ from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 selector = SelectKBest(chi2, k=2) selector.fit_transform(X, y) SelectFromModel​ from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression selector = SelectFromModel(estimator=LogisticRegression()) selector.fit_transform(X, y) RFE​ from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression selector = RFE(estimator=LogisticRegression(), n_features_to_select=2) selector.fit_transform(X, y) ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"crear columna indicadora","type":0,"sectionRef":"#","url":"/en/cheat-sheets/tags/preprocessing","content":"Missing data Es importante tener en cuenta que los modelos de machine learning no pueden trabajar con valores nulos, por lo que es necesario reemplazarlos por algún valor. Eliminar​ Si hay muchos valores nulos, se puede eliminar la columna o fila, tener en cuenta que se puede perder información importante. df.dropna() Imputar​ from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy='mean') imputer.fit_transform(X) crear columna indicadora from sklearn.impute import MissingIndicator indicator = MissingIndicator() indicator.fit_transform(X) Encoder data Dummy​ Variable\tDummy color\tcolor_rojo\tcolor_verde\tcolor_azulrojo\t1\t0\t0 verde\t0\t1\t0 azul\t0\t0\t1 from sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder() encoder.fit_transform(X) import pandas as pd pd.get_dummies(X) Label​ Variable\tLabelrojo\t0 verde\t1 azul\t2```python from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() encoder.fit_transform(X) ``` import pandas as pd df = pd.DataFrame({'color': ['rojo', 'verde', 'azul']}) df['color'].astype('category').cat.codes Scaling and Centering Data StandardScaler​ x−μσ\\frac{x - \\mu}{\\sigma}σx−μ​ from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit_transform(X) MinMaxScaler​ x−minmax−min\\frac{x - min}{max - min}max−minx−min​ from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaler.fit_transform(X) RobustScaler​ x−Q1Q3−Q1\\frac{x - Q_1}{Q_3 - Q_1}Q3​−Q1​x−Q1​​ from sklearn.preprocessing import RobustScaler scaler = RobustScaler() scaler.fit_transform(X) Normalizer​ L1: x∑i=1n∣xi∣\\frac{x}{\\sum_{i=1}^n |x_i|}∑i=1n​∣xi​∣x​L2: x∑i=1nxi2\\frac{x}{\\sqrt{\\sum_{i=1}^n x_i^2}}∑i=1n​xi2​​x​max: xmax(x)\\frac{x}{max(x)}max(x)x​ from sklearn.preprocessing import Normalizer # L1, L2, max scaler = Normalizer(norm='l2') scaler.fit_transform(X) Feature engineering PolynomialFeatures​ x1,x2→x12,x1x2,x22x_1, x_2 \\rightarrow x_1^2, x_1x_2, x_2^2x1​,x2​→x12​,x1​x2​,x22​ from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) poly.fit_transform(X) Binning​ Este proceso se utiliza para discretizar variables continuas, es decir, convertir variables continuas en variables categóricas, agrupando los valores en intervalos. x→{0,1,2,...,n}x \\rightarrow \\{0, 1, 2,..., n\\}x→{0,1,2,...,n} from sklearn.preprocessing import KBinsDiscretizer discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform') discretizer.fit_transform(X) Feature selection VarianceThreshold​ from sklearn.feature_selection import VarianceThreshold selector = VarianceThreshold(threshold=0.1) selector.fit_transform(X) SelectKBest​ from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 selector = SelectKBest(chi2, k=2) selector.fit_transform(X, y) SelectFromModel​ from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression selector = SelectFromModel(estimator=LogisticRegression()) selector.fit_transform(X, y) RFE​ from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression selector = RFE(estimator=LogisticRegression(), n_features_to_select=2) selector.fit_transform(X, y) ","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"},{"title":"index","type":0,"sectionRef":"#","url":"/en/tutorial","content":"index","keywords":"entredata  ciencia de datos  machine learning  inteligencia artificial  programación"}]